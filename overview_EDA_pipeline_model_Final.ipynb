{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><strong>Problem Overview:</strong> Spelling error correction is the task of automatically correcting spelling errors in text; e.g. [I followed his advcie -> I followed his advice]. It can be used to not only help language learners improve their writing erros, but also alert native speakers to accidental mistakes or typos.<br>\n",
    "The aim of the task is to produce a correctly spelled sentence given a incorrectly spelled sentence.\n",
    "\n",
    "><strong>Business Problem: </strong> By building the automated spelling error correction. We can create automated tools for writing English scientific texts, Filtering out sentences that need spelling improvements, evaluating articles, etc. These all mentioned tasks are done manually so, by automating this process we can save both time and money for the company.\n",
    "\n",
    "><strong>ML formulation: </strong>Building a model using techniques like Encoder-Decoder architecture, Bidirectional LSTM with attention mechanism, etc. can be used.\n",
    "\n",
    "><strong>Performance metric: </strong>BLEU Score<br>\n",
    "Bilingual Evaluation Understudy(BLEU) is a metric for comparing machine translated sentence to one or more reference setences. The metric ranges on scale of 0 to 1, in an attempt to measure the adequacy and fluency of the MT output. the more overlap there is with their human reference translations and thus, the better the translation.<br>\n",
    "The BLEU is programming task to compare n-grams of the translated sentences with the reference sentence and count the number of matches. The more the matches the better the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import*\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Preprocessing data: </strong>Text files which we need to combine to introduce spelling errors in the dataset. So, that we can train model for spelling error correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nietzsche.txt',\n",
       " 'pride_and_prejudice.txt',\n",
       " 'shakespeare.txt',\n",
       " 'war_and_peace.txt',\n",
       " 'wonderland.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_files = os.listdir('data')\n",
    "text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_CHARS = '[#$%\"\\+@<=>!&,-.?:;()*\\[\\]\\'^_`{|}~/\\d\\t\\r\\x0b\\x0c]'\n",
    "train_text = ''\n",
    "for filename in text_files[:-1]:\n",
    "    file = open('data/'+filename).read().split()\n",
    "    train_text += ' '.join([re.sub(REMOVE_CHARS, '', token) for token in file])+' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of unigram train data: 36821\n",
      "Size of bigram train data: 983848\n",
      "Size of trigram train data: 983847\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "unigram_train_data = set([token for token in train_text.split()])\n",
    "unigram_train_data = list(filter(None, set(unigram_train_data)))\n",
    "\n",
    "bigram_train_data = list(ngrams(train_text.split(), 2))\n",
    "bigram_train_data = [' '.join(x) for x in bigram_train_data]\n",
    "\n",
    "trigram_train_data = list(ngrams(train_text.split(), 3))\n",
    "trigram_train_data = [' '.join(x) for x in trigram_train_data]\n",
    "\n",
    "print('Size of unigram train data:', len(unigram_train_data))\n",
    "print('Size of bigram train data:', len(bigram_train_data))\n",
    "print('Size of trigram train data:', len(trigram_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/'+text_files[-1]).read().split()\n",
    "test_text = ' '.join([re.sub(REMOVE_CHARS, '', token) for token in file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of unigram test data: 3152\n",
      "Size of bigram test data: 26387\n",
      "Size of trigram test data: 26386\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "unigram_test_data = set([token for token in test_text.split()])\n",
    "unigram_test_data = list(filter(None, set(unigram_test_data)))\n",
    "\n",
    "bigram_test_data = list(ngrams(test_text.split(), 2))\n",
    "bigram_test_data = [' '.join(x) for x in bigram_test_data]\n",
    "\n",
    "trigram_test_data = list(ngrams(test_text.split(), 3))\n",
    "trigram_test_data = [' '.join(x) for x in trigram_test_data]\n",
    "\n",
    "print('Size of unigram test data:', len(unigram_test_data))\n",
    "print('Size of bigram test data:', len(bigram_test_data))\n",
    "print('Size of trigram test data:', len(trigram_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_length = []\n",
    "for i in unigram_train_data:\n",
    "    unigram_length.append(len(i))\n",
    "    \n",
    "bigram_length = []\n",
    "for i in bigram_train_data:\n",
    "    bigram_length.append(len(i))\n",
    "\n",
    "trigram_length = []\n",
    "for i in trigram_train_data:\n",
    "    trigram_length.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0 3.0 5.0\n",
      "10 5.0 6.0 11.0\n",
      "20 6.0 7.0 12.0\n",
      "30 6.0 8.0 13.0\n",
      "40 7.0 9.0 14.0\n",
      "50 8.0 9.0 15.0\n",
      "60 8.0 10.0 16.0\n",
      "70 9.0 11.0 17.0\n",
      "80 10.0 12.0 18.0\n",
      "90 12.0 14.0 21.0\n",
      "100 27.0 35.0 47.0\n",
      "90 12.0 14.0 21.0\n",
      "91 12.0 14.0 21.0\n",
      "92 12.0 15.0 21.0\n",
      "93 12.0 15.0 22.0\n",
      "94 12.0 15.0 22.0\n",
      "95 13.0 16.0 22.0\n",
      "96 13.0 16.0 23.0\n",
      "97 14.0 17.0 24.0\n",
      "98 14.0 18.0 25.0\n",
      "99 16.0 19.0 26.0\n",
      "100 27.0 35.0 47.0\n",
      "99.1 16.0 20.0 27.0\n",
      "99.2 16.0 20.0 27.0\n",
      "99.3 16.0 20.0 27.0\n",
      "99.4 17.0 20.0 28.0\n",
      "99.5 17.0 21.0 28.0\n",
      "99.6 17.0 21.0 28.0\n",
      "99.7 18.0 22.0 29.0\n",
      "99.8 19.0 22.0 30.0\n",
      "99.9 20.0 24.0 32.0\n",
      "100 27.0 35.0 47.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,101,10):\n",
    "    print(i,np.percentile(unigram_length, i), np.percentile(bigram_length, i), np.percentile(trigram_length, i))\n",
    "for i in range(90,101):\n",
    "    print(i,np.percentile(unigram_length, i), np.percentile(bigram_length, i), np.percentile(trigram_length, i))\n",
    "for i in [99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100]:\n",
    "    print(i,np.percentile(unigram_length, i), np.percentile(bigram_length, i), np.percentile(trigram_length, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "ax = sns.kdeplot(data = unigram_length, color='green', cumulative = True, label = 'unigram')\n",
    "sns.kdeplot(data = bigram_length, color='yellow', cumulative = True, ax = ax, label = 'bigram')\n",
    "sns.kdeplot(data = trigram_length, color='orange', cumulative = True, ax = ax, label = 'trigram')\n",
    "plt.xlabel('token length', fontdict = {'fontsize':15})\n",
    "plt.title('PDF for token length', fontdict = {'fontsize': 23})\n",
    "ax.legend()\n",
    "plt.xlim(0, 50)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong> Aim :</strong> Plot for determining the distribution token length of unigram, bigram, and trigram dataset.\n",
    "\n",
    "> <strong>Conclusion : </strong><br>\n",
    "1.&nbsp;99.9% of unigram token have length less than 20 characters.<br>\n",
    "2.&nbsp;99.9% of biigram token have length less than 24 characters.<br>\n",
    "3.&nbsp;99.9% of trigram token have length less than 32 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_speling_errors(token, error_rate, VOCAB):\n",
    "    \"\"\"Add some artificial spelling mistakes.\"\"\"\n",
    "    assert(0.0 <= error_rate < 1.0)\n",
    "    if len(token) < 3:\n",
    "        return token\n",
    "    rand = np.random.rand()\n",
    "    # Here are 4 different ways spelling mistakes can occur,\n",
    "    # each of which has equal chance.\n",
    "    prob = error_rate / 4.0\n",
    "    if rand < prob:\n",
    "        # Replace a character with a random character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + np.random.choice(VOCAB) \\\n",
    "                + token[random_char_index + 1:]\n",
    "    elif prob < rand < prob * 2:\n",
    "        # Delete a character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + token[random_char_index + 1:]\n",
    "    elif prob * 2 < rand < prob * 3:\n",
    "        # Add a random character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + np.random.choice(VOCAB) \\\n",
    "                + token[random_char_index:]\n",
    "    elif prob * 3 < rand < prob * 4:\n",
    "        # Transpose 2 characters.\n",
    "        random_char_index = np.random.randint(len(token) - 1)\n",
    "        token = token[:random_char_index]  + token[random_char_index + 1] \\\n",
    "                + token[random_char_index] + token[random_char_index + 2:]\n",
    "    else:\n",
    "        # No spelling errors.\n",
    "        pass\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIGRAM_VOCAB = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \\\n",
    "                 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \\\n",
    "                 '<SOW>', '<EOW>']\n",
    "\n",
    "NGRAM_VOCAB = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \\\n",
    "               'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \\\n",
    "               ' ', '<SOW>', '<EOW>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>magistrates</td>\n",
       "      <td>magistratse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cookshops</td>\n",
       "      <td>Cookshops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aristocratic</td>\n",
       "      <td>arstocratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>graced</td>\n",
       "      <td>graced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wagging</td>\n",
       "      <td>wagigng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         output        input\n",
       "0   magistrates  magistratse\n",
       "1     Cookshops    Cookshops\n",
       "2  aristocratic  arstocratic\n",
       "3        graced       graced\n",
       "4       wagging      wagigng"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_maxlen = 20\n",
    "unigram_train = []\n",
    "for token in unigram_train_data:\n",
    "    if len(token) < unigram_maxlen:\n",
    "        point = [token, add_speling_errors(token, 0.5, UNIGRAM_VOCAB)]\n",
    "        unigram_train.append(point)\n",
    "unigram_train = pd.DataFrame(unigram_train, columns = ['output', 'input'])\n",
    "unigram_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PREFACE SUPPOSING</td>\n",
       "      <td>PREFACE SUPPOSING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SUPPOSING that</td>\n",
       "      <td>SUPPOSINGt hat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that Truth</td>\n",
       "      <td>that TruEh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Truth is</td>\n",
       "      <td>Truth is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is a</td>\n",
       "      <td>si a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              output              input\n",
       "0  PREFACE SUPPOSING  PREFACE SUPPOSING\n",
       "1     SUPPOSING that     SUPPOSINGt hat\n",
       "2         that Truth         that TruEh\n",
       "3           Truth is           Truth is\n",
       "4               is a               si a"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_maxlen = 24\n",
    "bigram_train = []\n",
    "for token in bigram_train_data:\n",
    "    if len(token) < bigram_maxlen:\n",
    "        point = [token, add_speling_errors(token, 0.5, NGRAM_VOCAB)]\n",
    "        bigram_train.append(point)\n",
    "bigram_train = pd.DataFrame(bigram_train, columns = ['output', 'input'])\n",
    "bigram_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PREFACE SUPPOSING that</td>\n",
       "      <td>PREFACE SUPPOSIN that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SUPPOSING that Truth</td>\n",
       "      <td>SUPPOSsING that Truth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that Truth is</td>\n",
       "      <td>that Truth is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Truth is a</td>\n",
       "      <td>Truth si a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is a womanwhat</td>\n",
       "      <td>is a womanwhat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   output                  input\n",
       "0  PREFACE SUPPOSING that  PREFACE SUPPOSIN that\n",
       "1    SUPPOSING that Truth  SUPPOSsING that Truth\n",
       "2           that Truth is          that Truth is\n",
       "3              Truth is a             Truth si a\n",
       "4          is a womanwhat         is a womanwhat"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_maxlen = 32\n",
    "trigram_train = []\n",
    "for token in trigram_train_data:\n",
    "    if len(token) < trigram_maxlen:\n",
    "        point = [token, add_speling_errors(token, 0.5, NGRAM_VOCAB)]\n",
    "        trigram_train.append(point)\n",
    "trigram_train = pd.DataFrame(trigram_train, columns = ['output', 'input'])\n",
    "trigram_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_train['enc_inp'] = unigram_train['input'].astype(str).apply(lambda x: '<SOW> '+' '.join(list(x))+' <EOW>')\n",
    "unigram_train['dec_inp'] = unigram_train['output'].astype(str).apply(lambda x: '<SOW> '+' '.join(list(x)))\n",
    "unigram_train['dec_out'] = unigram_train['output'].astype(str).apply(lambda x: ' '.join(list(x))+' <EOW>')\n",
    "\n",
    "bigram_train['enc_inp'] = bigram_train['input'].astype(str).apply(lambda x: '<SOW>*'+'*'.join(list(x))+'*<EOW>')\n",
    "bigram_train['dec_inp'] = bigram_train['output'].astype(str).apply(lambda x: '<SOW>*'+'*'.join(list(x)))\n",
    "bigram_train['dec_out'] = bigram_train['output'].astype(str).apply(lambda x: '*'.join(list(x))+'*<EOW>')\n",
    "\n",
    "trigram_train['enc_inp'] = trigram_train['input'].astype(str).apply(lambda x: '<SOW>*'+'*'.join(list(x))+'*<EOW>')\n",
    "trigram_train['dec_inp'] = trigram_train['output'].astype(str).apply(lambda x: '<SOW>*'+'*'.join(list(x)))\n",
    "trigram_train['dec_out'] = trigram_train['output'].astype(str).apply(lambda x: '*'.join(list(x))+'*<EOW>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_train, unigram_val = train_test_split(unigram_train, test_size = 0.1, random_state = 0)\n",
    "bigram_train, bigram_val = train_test_split(bigram_train, test_size = 0.1, random_state = 0)\n",
    "trigram_train, trigram_val = train_test_split(trigram_train, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slatesll</td>\n",
       "      <td>slatesll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rumbling</td>\n",
       "      <td>rmubling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chance</td>\n",
       "      <td>chacne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bright</td>\n",
       "      <td>brnight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>ridiculous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       output       input\n",
       "0    slatesll    slatesll\n",
       "1    rumbling    rmubling\n",
       "2      chance      chacne\n",
       "3      bright     brnight\n",
       "4  ridiculous  ridiculous"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_test = []\n",
    "for token in unigram_test_data:\n",
    "    if len(token) < unigram_maxlen:\n",
    "        point = [token, add_speling_errors(token, 0.5, UNIGRAM_VOCAB)]\n",
    "        unigram_test.append(point)\n",
    "unigram_test = pd.DataFrame(unigram_test, columns = ['output', 'input'])\n",
    "unigram_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ï»¿ALICES ADVENTURES</td>\n",
       "      <td>ï»¿ALIECS ADVENTURES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADVENTURES IN</td>\n",
       "      <td>ADVENTURES IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IN WONDERLAND</td>\n",
       "      <td>IN WONDEtLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WONDERLAND Lewis</td>\n",
       "      <td>WONDERLAND Lewis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lewis Carroll</td>\n",
       "      <td>Lewis Carrlol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 output                 input\n",
       "0  ï»¿ALICES ADVENTURES  ï»¿ALIECS ADVENTURES\n",
       "1         ADVENTURES IN         ADVENTURES IN\n",
       "2         IN WONDERLAND         IN WONDEtLAND\n",
       "3      WONDERLAND Lewis      WONDERLAND Lewis\n",
       "4         Lewis Carroll         Lewis Carrlol"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_test = []\n",
    "for token in bigram_test_data:\n",
    "    if len(token) < bigram_maxlen:\n",
    "        point = [token, add_speling_errors(token, 0.5, NGRAM_VOCAB)]\n",
    "        bigram_test.append(point)\n",
    "bigram_test = pd.DataFrame(bigram_test, columns = ['output', 'input'])\n",
    "bigram_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IN WONDERLAND Lewis</td>\n",
       "      <td>IN WONDERoLAND Lewis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lewis Carroll THE</td>\n",
       "      <td>Lewis Carroll THE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EDITION CHAPTER I</td>\n",
       "      <td>EDITION CHATPER I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHAPTER I Down</td>\n",
       "      <td>CHAPTRE I Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I Down the</td>\n",
       "      <td>I Down the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                output                 input\n",
       "0  IN WONDERLAND Lewis  IN WONDERoLAND Lewis\n",
       "1    Lewis Carroll THE     Lewis Carroll THE\n",
       "2    EDITION CHAPTER I     EDITION CHATPER I\n",
       "3       CHAPTER I Down        CHAPTRE I Down\n",
       "4           I Down the            I Down the"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_test = []\n",
    "for token in trigram_test_data:\n",
    "    if len(token) < unigram_maxlen:\n",
    "        point = [token, add_speling_errors(token, 0.5, NGRAM_VOCAB)]\n",
    "        trigram_test.append(point)\n",
    "trigram_test = pd.DataFrame(trigram_test, columns = ['output', 'input'])\n",
    "trigram_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_train.to_csv('unigram_train.csv')\n",
    "unigram_test.to_csv('unigram_test.csv')\n",
    "unigram_val.to_csv('unigram_val.csv')\n",
    "\n",
    "bigram_train.to_csv('bigram_train.csv')\n",
    "bigram_test.to_csv('bigram_test.csv')\n",
    "bigram_val.to_csv('bigram_val.csv')\n",
    "\n",
    "trigram_train.to_csv('trigram_train.csv')\n",
    "trigram_test.to_csv('trigram_test.csv')\n",
    "trigram_val.to_csv('trigram_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of unigram train set : (33101, 5)\n",
      "Shape of unigram test set : (3150, 2)\n",
      "Shape of unigram validation set : (3678, 5)\n",
      "\n",
      "Shape of bigram train set : (884533, 5)\n",
      "Shape of bigram test set : (26377, 2)\n",
      "Shape of bigram validation set : (98282, 5)\n",
      "\n",
      "Shape of trigram train set : (884550, 5)\n",
      "Shape of trigram test set : (24586, 2)\n",
      "Shape of trigram validation set : (98284, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of unigram train set :',unigram_train.shape)\n",
    "print('Shape of unigram test set :',unigram_test.shape)\n",
    "print('Shape of unigram validation set :', unigram_val.shape)\n",
    "\n",
    "print('\\nShape of bigram train set :',bigram_train.shape)\n",
    "print('Shape of bigram test set :',bigram_test.shape)\n",
    "print('Shape of bigram validation set :', bigram_val.shape)\n",
    "\n",
    "print('\\nShape of trigram train set :',trigram_train.shape)\n",
    "print('Shape of trigram test set :',trigram_test.shape)\n",
    "print('Shape of trigram validation set :', trigram_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output     0\n",
      "input      0\n",
      "enc_inp    0\n",
      "dec_inp    0\n",
      "dec_out    0\n",
      "dtype: int64 output     0\n",
      "input      0\n",
      "enc_inp    0\n",
      "dec_inp    0\n",
      "dec_out    0\n",
      "dtype: int64 output    0\n",
      "input     0\n",
      "dtype: int64\n",
      "output     0\n",
      "input      0\n",
      "enc_inp    0\n",
      "dec_inp    0\n",
      "dec_out    0\n",
      "dtype: int64 output     0\n",
      "input      0\n",
      "enc_inp    0\n",
      "dec_inp    0\n",
      "dec_out    0\n",
      "dtype: int64 output    0\n",
      "input     0\n",
      "dtype: int64\n",
      "output     0\n",
      "input      0\n",
      "enc_inp    0\n",
      "dec_inp    0\n",
      "dec_out    0\n",
      "dtype: int64 output     0\n",
      "input      0\n",
      "enc_inp    0\n",
      "dec_inp    0\n",
      "dec_out    0\n",
      "dtype: int64 output    0\n",
      "input     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(unigram_train.isnull().sum(), unigram_val.isnull().sum(), unigram_test.isnull().sum())\n",
    "print(bigram_train.isnull().sum(), bigram_val.isnull().sum(), bigram_test.isnull().sum())\n",
    "print(trigram_train.isnull().sum(), trigram_val.isnull().sum(), trigram_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_train = pd.read_csv('unigram_train.csv', index_col = 0)\n",
    "unigram_test = pd.read_csv('unigram_test.csv', index_col = 0)\n",
    "unigram_val = pd.read_csv('unigram_val.csv', index_col = 0)\n",
    "\n",
    "bigram_train = pd.read_csv('bigram_train.csv', index_col = 0)\n",
    "bigram_test = pd.read_csv('bigram_test.csv', index_col = 0)\n",
    "bigram_val = pd.read_csv('bigram_val.csv', index_col = 0)\n",
    "\n",
    "trigram_train = pd.read_csv('trigram_train.csv', index_col = 0)\n",
    "trigram_test = pd.read_csv('trigram_test.csv', index_col = 0)\n",
    "trigram_val = pd.read_csv('trigram_val.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_star(input_data):\n",
    "    return tf.strings.split(input_data, sep = '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_maxlen = 20\n",
    "bigram_maxlen = 24\n",
    "trigram_maxlen = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[54, 26, 23, ...,  0,  0,  0],\n",
       "         [54, 42, 51, ...,  0,  0,  0],\n",
       "         [54, 35,  7, ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [54, 10, 27, ...,  0,  0,  0],\n",
       "         [54, 15,  7, ...,  0,  0,  0],\n",
       "         [54,  7, 14, ...,  0,  0,  0]], dtype=int64),\n",
       "  array([[54, 26, 23, ...,  0,  0,  0],\n",
       "         [54, 42, 33, ...,  0,  0,  0],\n",
       "         [54, 35,  7, ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [54, 10, 27, ...,  0,  0,  0],\n",
       "         [54, 15,  7, ...,  0,  0,  0],\n",
       "         [54,  7, 14, ...,  0,  0,  0]], dtype=int64)),\n",
       " array([[26, 23, 22, ...,  0,  0,  0],\n",
       "        [42, 33, 51, ...,  0,  0,  0],\n",
       "        [35,  7, 25, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [10, 27,  6, ...,  0,  0,  0],\n",
       "        [15,  7, 14, ...,  0,  0,  0],\n",
       "        [ 7, 14, 24, ...,  0,  0,  0]], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_vec = TextVectorization(output_sequence_length= unigram_maxlen+2, standardize = None, split='whitespace', max_tokens = len(UNIGRAM_VOCAB)+2, output_mode='int')\n",
    "unigram_vec.adapt(UNIGRAM_VOCAB)\n",
    "\n",
    "bigram_vec = TextVectorization(output_sequence_length= bigram_maxlen+2, standardize = None, split = split_on_star, max_tokens = len(NGRAM_VOCAB)+2, output_mode='int')\n",
    "bigram_vec.adapt(NGRAM_VOCAB)\n",
    "\n",
    "trigram_vec = TextVectorization(output_sequence_length= trigram_maxlen+2, standardize = None, split = split_on_star, max_tokens = len(NGRAM_VOCAB)+2, output_mode='int')\n",
    "trigram_vec.adapt(NGRAM_VOCAB)\n",
    "\n",
    "unigram_index_to_word = {idx: word for idx, word in enumerate(unigram_vec.get_vocabulary())}\n",
    "unigram_word_to_index = {word: idx for idx, word in enumerate(unigram_vec.get_vocabulary())}\n",
    "\n",
    "bigram_index_to_word = {idx: word for idx, word in enumerate(bigram_vec.get_vocabulary())}\n",
    "bigram_word_to_index = {word: idx for idx, word in enumerate(bigram_vec.get_vocabulary())}\n",
    "\n",
    "trigram_index_to_word = {idx: word for idx, word in enumerate(trigram_vec.get_vocabulary())}\n",
    "trigram_word_to_index = {word: idx for idx, word in enumerate(trigram_vec.get_vocabulary())}\n",
    "\n",
    "def unigram_mapping(x):\n",
    "    enc_inp = unigram_vec(x[:, 2])\n",
    "    dec_inp = unigram_vec(x[:, 3])\n",
    "    dec_out = unigram_vec(x[:, 4])\n",
    "    return (enc_inp, dec_inp), dec_out\n",
    "\n",
    "def bigram_mapping(x):\n",
    "    enc_inp = bigram_vec(x[:, 2])\n",
    "    dec_inp = bigram_vec(x[:, 3])\n",
    "    dec_out = bigram_vec(x[:, 4])\n",
    "    return (enc_inp, dec_inp), dec_out\n",
    "\n",
    "def trigram_mapping(x):\n",
    "    enc_inp = trigram_vec(x[:, 2])\n",
    "    dec_inp = trigram_vec(x[:, 3])\n",
    "    dec_out = trigram_vec(x[:, 4])\n",
    "    return (enc_inp, dec_inp), dec_out\n",
    "\n",
    "unigram_train_dataset = tf.data.Dataset.from_tensor_slices(unigram_train.values).repeat().batch(batch_size).map(unigram_mapping).prefetch(1)\n",
    "unigram_val_dataset = tf.data.Dataset.from_tensor_slices(unigram_val.values).repeat().batch(batch_size).map(unigram_mapping).prefetch(1)\n",
    "\n",
    "bigram_train_dataset = tf.data.Dataset.from_tensor_slices(bigram_train.values).repeat().batch(batch_size).map(bigram_mapping).prefetch(1)\n",
    "bigram_val_dataset = tf.data.Dataset.from_tensor_slices(bigram_val.values).repeat().batch(batch_size).map(bigram_mapping).prefetch(1)\n",
    "\n",
    "trigram_train_dataset = tf.data.Dataset.from_tensor_slices(trigram_train.values).repeat().batch(batch_size).map(trigram_mapping).prefetch(1)\n",
    "trigram_val_dataset = tf.data.Dataset.from_tensor_slices(trigram_val.values).repeat().batch(batch_size).map(trigram_mapping).prefetch(1)\n",
    "\n",
    "a = unigram_train_dataset.as_numpy_iterator()\n",
    "next(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Seq2Seq</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super().__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        #Initialize Embedding layer\n",
    "        self.enc_embed = Embedding(input_dim = inp_vocab_size, output_dim = embedding_size, input_length= input_length)\n",
    "        #Intialize Encoder LSTM layer\n",
    "        self.enc_lstm = LSTM(lstm_size, return_sequences = True, return_state = True, dropout = 0.4)\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "        embedding = self.enc_embed(input_sequence)\n",
    "        output_state, enc_h, enc_c = self.enc_lstm(embedding, initial_state = states)\n",
    "        return output_state, enc_h, enc_c\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "        return [tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))]\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super().__init__()\n",
    "        #Initialize Embedding layer\n",
    "        self.dec_embed = Embedding(input_dim = out_vocab_size, output_dim = embedding_size, input_length = input_length)\n",
    "        #Intialize Decoder LSTM layer\n",
    "        self.dec_lstm = LSTM(lstm_size, return_sequences = True, return_state = True, dropout = 0.4)\n",
    "    \n",
    "    def call(self,input_sequence, initial_states):\n",
    "        embedding = self.dec_embed(input_sequence)\n",
    "        output_state, dec_h, dec_c = self.dec_lstm(embedding, initial_state = initial_states)\n",
    "        return output_state, dec_h, dec_c\n",
    "\n",
    "class Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self,*params):\n",
    "        super().__init__()\n",
    "        #Create encoder object\n",
    "        self.encoder = Encoder(inp_vocab_size = params[0], embedding_size = params[2], lstm_size = params[3], input_length = params[4])\n",
    "        #Create decoder object\n",
    "        self.decoder = Decoder(out_vocab_size = params[1], embedding_size = params[2], lstm_size = params[3], input_length = params[5])\n",
    "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
    "        self.dense = Dense(params[1], activation='softmax')\n",
    "    \n",
    "    def call(self, params, training = True):\n",
    "        enc_inp, dec_inp = params[0], params[1]\n",
    "        # print(enc_inp, dec_inp)\n",
    "        initial_state = self.encoder.initialize_states(batch_size)\n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        output, _, _ = self.decoder(dec_inp ,[enc_h, enc_c])\n",
    "        output = Dropout(0.5)(output)\n",
    "        return self.dense(output)\n",
    "\n",
    "class pred_Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self,*params):\n",
    "        super().__init__()\n",
    "        #Create encoder object\n",
    "        self.encoder = Encoder(inp_vocab_size = params[0], embedding_size = params[2], lstm_size = params[3], input_length = params[4])\n",
    "        #Create decoder object\n",
    "        self.decoder = Decoder(out_vocab_size = params[1], embedding_size = params[2], lstm_size = params[3], input_length = params[5])\n",
    "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
    "        self.dense = Dense(params[1], activation='softmax')\n",
    "        self.word_to_index = params[6]\n",
    "    \n",
    "    def call(self, params):\n",
    "        enc_inp = params[0]\n",
    "        initial_state = self.encoder.initialize_states(1)\n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        pred = tf.expand_dims([self.word_to_index['<SOW>']], 0)\n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        all_pred = []\n",
    "        for t in range(max_len):  \n",
    "            pred, dec_h,dec_c = self.decoder(pred, [dec_h, dec_c])\n",
    "            pred = self.dense(pred)\n",
    "            pred = tf.argmax(pred, axis = -1)\n",
    "            all_pred.append(pred)\n",
    "        return all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(seq, vectorizer, index_to_word, gram ='uni'):\n",
    "    if gram == 'uni':\n",
    "        seq = ' '.join(list(seq))\n",
    "        seq = '<SOW> '+seq+' <EOW>'\n",
    "    else:\n",
    "        seq = '*'.join(list(seq))\n",
    "        seq = '<SOW>*'+seq+'*<EOW>'\n",
    "    seq = vectorizer([seq])\n",
    "    pred = pred_model.predict(tf.expand_dims(seq, 0))\n",
    "    output = []\n",
    "    for i in pred:\n",
    "        word = index_to_word[i[0][0]]\n",
    "        if word == '<EOW>':\n",
    "            break\n",
    "        output.append(word)\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.1 UniGram </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vec.get_vocabulary())\n",
    "embedding_dim = 100\n",
    "lstm_size = 256\n",
    "max_len = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 1.3088\n",
      "Epoch 00001: val_loss improved from inf to 1.08792, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 4s 14ms/step - loss: 1.3088 - val_loss: 1.0879\n",
      "Epoch 2/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 1.0246\n",
      "Epoch 00002: val_loss improved from 1.08792 to 0.93814, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 12ms/step - loss: 1.0246 - val_loss: 0.9381\n",
      "Epoch 3/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.9280\n",
      "Epoch 00003: val_loss improved from 0.93814 to 0.84624, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 12ms/step - loss: 0.9278 - val_loss: 0.8462\n",
      "Epoch 4/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.8515\n",
      "Epoch 00004: val_loss improved from 0.84624 to 0.78058, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 12ms/step - loss: 0.8513 - val_loss: 0.7806\n",
      "Epoch 5/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.8017\n",
      "Epoch 00005: val_loss improved from 0.78058 to 0.72511, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 12ms/step - loss: 0.8014 - val_loss: 0.7251\n",
      "Epoch 6/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.7480\n",
      "Epoch 00006: val_loss improved from 0.72511 to 0.66346, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 12ms/step - loss: 0.7478 - val_loss: 0.6635\n",
      "Epoch 7/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.6937\n",
      "Epoch 00007: val_loss improved from 0.66346 to 0.60634, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 12ms/step - loss: 0.6937 - val_loss: 0.6063\n",
      "Epoch 8/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.6483\n",
      "Epoch 00008: val_loss improved from 0.60634 to 0.55700, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 12ms/step - loss: 0.6483 - val_loss: 0.5570\n",
      "Epoch 9/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.5939\n",
      "Epoch 00009: val_loss improved from 0.55700 to 0.49638, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 12ms/step - loss: 0.5939 - val_loss: 0.4964\n",
      "Epoch 10/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.5503\n",
      "Epoch 00010: val_loss improved from 0.49638 to 0.45429, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 12ms/step - loss: 0.5504 - val_loss: 0.4543\n",
      "Epoch 11/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.5132\n",
      "Epoch 00011: val_loss improved from 0.45429 to 0.41674, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 12ms/step - loss: 0.5132 - val_loss: 0.4167\n",
      "Epoch 12/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.4749\n",
      "Epoch 00012: val_loss improved from 0.41674 to 0.38192, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 12ms/step - loss: 0.4748 - val_loss: 0.3819\n",
      "Epoch 13/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.4376\n",
      "Epoch 00013: val_loss improved from 0.38192 to 0.34618, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.4374 - val_loss: 0.3462\n",
      "Epoch 14/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.3941\n",
      "Epoch 00014: val_loss improved from 0.34618 to 0.29948, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.3941 - val_loss: 0.2995\n",
      "Epoch 15/50\n",
      "254/258 [============================>.] - ETA: 0s - loss: 0.3567\n",
      "Epoch 00015: val_loss improved from 0.29948 to 0.27217, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.3562 - val_loss: 0.2722\n",
      "Epoch 16/50\n",
      "255/258 [============================>.] - ETA: 0s - loss: 0.3254\n",
      "Epoch 00016: val_loss improved from 0.27217 to 0.24162, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.3251 - val_loss: 0.2416\n",
      "Epoch 17/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.2975\n",
      "Epoch 00017: val_loss improved from 0.24162 to 0.22571, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 4s 14ms/step - loss: 0.2975 - val_loss: 0.2257\n",
      "Epoch 18/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.2744\n",
      "Epoch 00018: val_loss improved from 0.22571 to 0.20805, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.2743 - val_loss: 0.2080\n",
      "Epoch 19/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.2558\n",
      "Epoch 00019: val_loss improved from 0.20805 to 0.19446, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.2557 - val_loss: 0.1945\n",
      "Epoch 20/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.2387\n",
      "Epoch 00020: val_loss improved from 0.19446 to 0.18305, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.2386 - val_loss: 0.1831\n",
      "Epoch 21/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2249- ETA: 0s - loss: \n",
      "Epoch 00021: val_loss improved from 0.18305 to 0.17706, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.2249 - val_loss: 0.1771\n",
      "Epoch 22/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2133\n",
      "Epoch 00022: val_loss improved from 0.17706 to 0.17050, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.2133 - val_loss: 0.1705\n",
      "Epoch 23/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2013\n",
      "Epoch 00023: val_loss improved from 0.17050 to 0.16339, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.2013 - val_loss: 0.1634\n",
      "Epoch 24/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.1915\n",
      "Epoch 00024: val_loss improved from 0.16339 to 0.16087, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1916 - val_loss: 0.1609\n",
      "Epoch 25/50\n",
      "254/258 [============================>.] - ETA: 0s - loss: 0.1833\n",
      "Epoch 00025: val_loss improved from 0.16087 to 0.15730, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1834 - val_loss: 0.1573\n",
      "Epoch 26/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.1759\n",
      "Epoch 00026: val_loss improved from 0.15730 to 0.15314, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1760 - val_loss: 0.1531\n",
      "Epoch 27/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.1687\n",
      "Epoch 00027: val_loss improved from 0.15314 to 0.15206, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1687 - val_loss: 0.1521\n",
      "Epoch 28/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.1625\n",
      "Epoch 00028: val_loss improved from 0.15206 to 0.14619, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 4s 14ms/step - loss: 0.1625 - val_loss: 0.1462\n",
      "Epoch 29/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.1574\n",
      "Epoch 00029: val_loss did not improve from 0.14619\n",
      "258/258 [==============================] - 4s 14ms/step - loss: 0.1574 - val_loss: 0.1487\n",
      "Epoch 30/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.1516\n",
      "Epoch 00030: val_loss improved from 0.14619 to 0.14454, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1516 - val_loss: 0.1445\n",
      "Epoch 31/50\n",
      "255/258 [============================>.] - ETA: 0s - loss: 0.1473\n",
      "Epoch 00031: val_loss improved from 0.14454 to 0.14326, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1472 - val_loss: 0.1433\n",
      "Epoch 32/50\n",
      "254/258 [============================>.] - ETA: 0s - loss: 0.1416\n",
      "Epoch 00032: val_loss improved from 0.14326 to 0.14313, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1415 - val_loss: 0.1431\n",
      "Epoch 33/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.1382\n",
      "Epoch 00033: val_loss improved from 0.14313 to 0.14277, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1382 - val_loss: 0.1428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n",
      "255/258 [============================>.] - ETA: 0s - loss: 0.1346\n",
      "Epoch 00034: val_loss improved from 0.14277 to 0.14092, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1344 - val_loss: 0.1409\n",
      "Epoch 35/50\n",
      "255/258 [============================>.] - ETA: 0s - loss: 0.1295\n",
      "Epoch 00035: val_loss did not improve from 0.14092\n",
      "258/258 [==============================] - 4s 14ms/step - loss: 0.1295 - val_loss: 0.1442\n",
      "Epoch 36/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00036: val_loss did not improve from 0.14092\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1263 - val_loss: 0.1426\n",
      "Epoch 37/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.1225\n",
      "Epoch 00037: val_loss did not improve from 0.14092\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1224 - val_loss: 0.1424\n",
      "Epoch 38/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00038: val_loss improved from 0.14092 to 0.13504, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1080 - val_loss: 0.1350\n",
      "Epoch 39/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.1037\n",
      "Epoch 00039: val_loss improved from 0.13504 to 0.13468, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1036 - val_loss: 0.1347\n",
      "Epoch 40/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.1014\n",
      "Epoch 00040: val_loss improved from 0.13468 to 0.13464, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1014 - val_loss: 0.1346\n",
      "Epoch 41/50\n",
      "254/258 [============================>.] - ETA: 0s - loss: 0.1003\n",
      "Epoch 00041: val_loss did not improve from 0.13464\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.1003 - val_loss: 0.1348\n",
      "Epoch 42/50\n",
      "255/258 [============================>.] - ETA: 0s - loss: 0.0993\n",
      "Epoch 00042: val_loss did not improve from 0.13464\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.0994 - val_loss: 0.1352\n",
      "Epoch 43/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.0974\n",
      "Epoch 00043: val_loss improved from 0.13464 to 0.13452, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.0974 - val_loss: 0.1345\n",
      "Epoch 44/50\n",
      "255/258 [============================>.] - ETA: 0s - loss: 0.0964- E\n",
      "Epoch 00044: val_loss improved from 0.13452 to 0.13444, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.0964 - val_loss: 0.1344\n",
      "Epoch 45/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00045: val_loss improved from 0.13444 to 0.13437, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.0964 - val_loss: 0.1344\n",
      "Epoch 46/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.0966\n",
      "Epoch 00046: val_loss improved from 0.13437 to 0.13429, saving model to seq2seq.h5\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.0965 - val_loss: 0.1343\n",
      "Epoch 47/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0956\n",
      "Epoch 00047: val_loss did not improve from 0.13429\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.0956 - val_loss: 0.1344\n",
      "Epoch 48/50\n",
      "256/258 [============================>.] - ETA: 0s - loss: 0.0966\n",
      "Epoch 00048: val_loss did not improve from 0.13429\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "258/258 [==============================] - 4s 14ms/step - loss: 0.0966 - val_loss: 0.1344\n",
      "Epoch 49/50\n",
      "255/258 [============================>.] - ETA: 0s - loss: 0.0958\n",
      "Epoch 00049: val_loss did not improve from 0.13429\n",
      "258/258 [==============================] - 4s 14ms/step - loss: 0.0958 - val_loss: 0.1344\n",
      "Epoch 50/50\n",
      "254/258 [============================>.] - ETA: 0s - loss: 0.0957\n",
      "Epoch 00050: val_loss did not improve from 0.13429\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 0.0956 - val_loss: 0.1344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x298c84fd9c8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, max_len, max_len)\n",
    "model.compile(optimizer = 'Adam',loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "callbacks = [ModelCheckpoint('seq2seq.h5', save_best_only= True, verbose = 1),\n",
    "             EarlyStopping(patience = 5, verbose = 1), \n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]\n",
    "\n",
    "model.fit(x = unigram_train_dataset, \n",
    "          steps_per_epoch = unigram_train.shape[0]//batch_size,\n",
    "          validation_data = unigram_val_dataset,\n",
    "          validation_steps = unigram_val.shape[0]//batch_size,\n",
    "          epochs = 50,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, max_len, max_len)\n",
    "pred_model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy')\n",
    "pred_model.build(input_shape=(None, 1, max_len))\n",
    "pred_model.load_weights('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  qitted\n",
      "predicted output :  quitted\n",
      "actual output : quitted\n"
     ]
    }
   ],
   "source": [
    "sentence = unigram_train['input'].values[5]\n",
    "print('input : ', sentence)\n",
    "result = predict(sentence, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', unigram_train['output'].values[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  fortissiemus\n",
      "predicted output :  fortissimeus\n",
      "actual output : fortissimus\n"
     ]
    }
   ],
   "source": [
    "sentence = unigram_train['input'].values[11]\n",
    "print('input : ', sentence)\n",
    "result = predict(sentence, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', unigram_train['output'].values[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  numbeGr\n",
      "predicted output :  number\n",
      "actual output : number\n"
     ]
    }
   ],
   "source": [
    "sentence = unigram_train['input'].values[14]\n",
    "print('input : ', sentence)\n",
    "result = predict(sentence, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', unigram_train['output'].values[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3678/3678 [03:16<00:00, 18.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 33101/33101 [29:07<00:00, 18.94it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3150/3150 [02:46<00:00, 18.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score on train:  0.8349934632437717\n",
      "BLEU Score on val:  0.7469809366693739\n",
      "BLEU Score on test:  0.6843990465288321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_bleu = 0\n",
    "for i in tqdm(range(val.shape[0])):\n",
    "    inp = unigram_val['input'].values[i]\n",
    "    out = unigram_val['output'].values[i]\n",
    "    pred = predict(inp, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "    val_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    inp = unigram_train['input'].values[i];\n",
    "    out = unigram_train['output'].values[i]\n",
    "    pred = predict(inp, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "    train_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    inp = unigram_test['input'].values[i]\n",
    "    out = unigram_test['output'].values[i]\n",
    "    pred = predict(inp, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "    test_bleu += sentence_bleu([out], pred)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/unigram_train.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/unigram_val.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/unigram_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2 BiGram </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(bigram_vec.get_vocabulary())\n",
    "embedding_dim = 100\n",
    "lstm_size = 256\n",
    "max_len = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.5047\n",
      "Epoch 00001: val_loss improved from inf to 0.11996, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 96s 14ms/step - loss: 0.5047 - val_loss: 0.1200\n",
      "Epoch 2/50\n",
      "6907/6910 [============================>.] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00002: val_loss improved from 0.11996 to 0.06599, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 100s 14ms/step - loss: 0.1246 - val_loss: 0.0660\n",
      "Epoch 3/50\n",
      "6907/6910 [============================>.] - ETA: 0s - loss: 0.0858\n",
      "Epoch 00003: val_loss improved from 0.06599 to 0.05437, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 104s 15ms/step - loss: 0.0858 - val_loss: 0.0544\n",
      "Epoch 4/50\n",
      "6907/6910 [============================>.] - ETA: 0s - loss: 0.0722\n",
      "Epoch 00004: val_loss improved from 0.05437 to 0.04924, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 104s 15ms/step - loss: 0.0722 - val_loss: 0.0492\n",
      "Epoch 5/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0649\n",
      "Epoch 00005: val_loss improved from 0.04924 to 0.04526, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 105s 15ms/step - loss: 0.0649 - val_loss: 0.0453\n",
      "Epoch 6/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0601\n",
      "Epoch 00006: val_loss improved from 0.04526 to 0.04325, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 106s 15ms/step - loss: 0.0601 - val_loss: 0.0432\n",
      "Epoch 7/50\n",
      "6907/6910 [============================>.] - ETA: 0s - loss: 0.0566\n",
      "Epoch 00007: val_loss improved from 0.04325 to 0.04161, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 105s 15ms/step - loss: 0.0566 - val_loss: 0.0416\n",
      "Epoch 8/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0539\n",
      "Epoch 00008: val_loss improved from 0.04161 to 0.04037, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 106s 15ms/step - loss: 0.0539 - val_loss: 0.0404\n",
      "Epoch 9/50\n",
      "6907/6910 [============================>.] - ETA: 0s - loss: 0.0517\n",
      "Epoch 00009: val_loss improved from 0.04037 to 0.03965, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 105s 15ms/step - loss: 0.0517 - val_loss: 0.0396\n",
      "Epoch 10/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0499\n",
      "Epoch 00010: val_loss improved from 0.03965 to 0.03907, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 104s 15ms/step - loss: 0.0499 - val_loss: 0.0391\n",
      "Epoch 11/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0484\n",
      "Epoch 00011: val_loss improved from 0.03907 to 0.03830, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 103s 15ms/step - loss: 0.0484 - val_loss: 0.0383\n",
      "Epoch 12/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0471\n",
      "Epoch 00012: val_loss improved from 0.03830 to 0.03783, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 105s 15ms/step - loss: 0.0471 - val_loss: 0.0378\n",
      "Epoch 13/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0459\n",
      "Epoch 00013: val_loss improved from 0.03783 to 0.03727, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 106s 15ms/step - loss: 0.0459 - val_loss: 0.0373\n",
      "Epoch 14/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0450\n",
      "Epoch 00014: val_loss improved from 0.03727 to 0.03696, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 105s 15ms/step - loss: 0.0450 - val_loss: 0.0370\n",
      "Epoch 15/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0440\n",
      "Epoch 00015: val_loss improved from 0.03696 to 0.03681, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 109s 16ms/step - loss: 0.0440 - val_loss: 0.0368\n",
      "Epoch 16/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0433\n",
      "Epoch 00016: val_loss improved from 0.03681 to 0.03646, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 109s 16ms/step - loss: 0.0433 - val_loss: 0.0365\n",
      "Epoch 17/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0425\n",
      "Epoch 00017: val_loss improved from 0.03646 to 0.03598, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 109s 16ms/step - loss: 0.0425 - val_loss: 0.0360\n",
      "Epoch 18/50\n",
      "6907/6910 [============================>.] - ETA: 0s - loss: 0.0418\n",
      "Epoch 00018: val_loss improved from 0.03598 to 0.03585, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 109s 16ms/step - loss: 0.0418 - val_loss: 0.0359\n",
      "Epoch 19/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0412\n",
      "Epoch 00019: val_loss improved from 0.03585 to 0.03570, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 113s 16ms/step - loss: 0.0412 - val_loss: 0.0357\n",
      "Epoch 20/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0406\n",
      "Epoch 00020: val_loss improved from 0.03570 to 0.03565, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 112s 16ms/step - loss: 0.0406 - val_loss: 0.0356\n",
      "Epoch 21/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0402\n",
      "Epoch 00021: val_loss improved from 0.03565 to 0.03554, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 111s 16ms/step - loss: 0.0402 - val_loss: 0.0355\n",
      "Epoch 22/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0396\n",
      "Epoch 00022: val_loss improved from 0.03554 to 0.03531, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 109s 16ms/step - loss: 0.0396 - val_loss: 0.0353\n",
      "Epoch 23/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0391\n",
      "Epoch 00023: val_loss improved from 0.03531 to 0.03517, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 107s 15ms/step - loss: 0.0391 - val_loss: 0.0352\n",
      "Epoch 24/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0387\n",
      "Epoch 00024: val_loss improved from 0.03517 to 0.03482, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 109s 16ms/step - loss: 0.0387 - val_loss: 0.0348\n",
      "Epoch 25/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0383\n",
      "Epoch 00025: val_loss improved from 0.03482 to 0.03456, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 108s 16ms/step - loss: 0.0383 - val_loss: 0.0346\n",
      "Epoch 26/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0379\n",
      "Epoch 00026: val_loss did not improve from 0.03456\n",
      "6910/6910 [==============================] - 109s 16ms/step - loss: 0.0379 - val_loss: 0.0348\n",
      "Epoch 27/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0375\n",
      "Epoch 00027: val_loss improved from 0.03456 to 0.03456, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 109s 16ms/step - loss: 0.0375 - val_loss: 0.0346\n",
      "Epoch 28/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0372\n",
      "Epoch 00028: val_loss improved from 0.03456 to 0.03445, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 109s 16ms/step - loss: 0.0372 - val_loss: 0.0345\n",
      "Epoch 29/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0368\n",
      "Epoch 00029: val_loss did not improve from 0.03445\n",
      "6910/6910 [==============================] - 109s 16ms/step - loss: 0.0368 - val_loss: 0.0346\n",
      "Epoch 30/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0366\n",
      "Epoch 00030: val_loss did not improve from 0.03445\n",
      "6910/6910 [==============================] - 110s 16ms/step - loss: 0.0366 - val_loss: 0.0346\n",
      "Epoch 31/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0362\n",
      "Epoch 00031: val_loss did not improve from 0.03445\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "6910/6910 [==============================] - 108s 16ms/step - loss: 0.0362 - val_loss: 0.0346\n",
      "Epoch 32/50\n",
      "6907/6910 [============================>.] - ETA: 0s - loss: 0.0308\n",
      "Epoch 00032: val_loss improved from 0.03445 to 0.03222, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 108s 16ms/step - loss: 0.0308 - val_loss: 0.0322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0292\n",
      "Epoch 00033: val_loss improved from 0.03222 to 0.03200, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 109s 16ms/step - loss: 0.0292 - val_loss: 0.0320\n",
      "Epoch 34/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0286\n",
      "Epoch 00034: val_loss improved from 0.03200 to 0.03186, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 110s 16ms/step - loss: 0.0286 - val_loss: 0.0319\n",
      "Epoch 35/50\n",
      "6907/6910 [============================>.] - ETA: 0s - loss: 0.0282\n",
      "Epoch 00035: val_loss improved from 0.03186 to 0.03174, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 108s 16ms/step - loss: 0.0282 - val_loss: 0.0317\n",
      "Epoch 36/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0279\n",
      "Epoch 00036: val_loss improved from 0.03174 to 0.03169, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 108s 16ms/step - loss: 0.0279 - val_loss: 0.0317\n",
      "Epoch 37/50\n",
      "6907/6910 [============================>.] - ETA: 0s - loss: 0.0276\n",
      "Epoch 00037: val_loss did not improve from 0.03169\n",
      "6910/6910 [==============================] - 108s 16ms/step - loss: 0.0276 - val_loss: 0.0317\n",
      "Epoch 38/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0274\n",
      "Epoch 00038: val_loss improved from 0.03169 to 0.03168, saving model to seq2seq_bigram.h5\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "6910/6910 [==============================] - 108s 16ms/step - loss: 0.0274 - val_loss: 0.0317\n",
      "Epoch 39/50\n",
      "6907/6910 [============================>.] - ETA: 0s - loss: 0.0267\n",
      "Epoch 00039: val_loss improved from 0.03168 to 0.03158, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 109s 16ms/step - loss: 0.0267 - val_loss: 0.0316\n",
      "Epoch 40/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0266\n",
      "Epoch 00040: val_loss improved from 0.03158 to 0.03155, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 107s 15ms/step - loss: 0.0266 - val_loss: 0.0316\n",
      "Epoch 41/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0265\n",
      "Epoch 00041: val_loss improved from 0.03155 to 0.03152, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 110s 16ms/step - loss: 0.0265 - val_loss: 0.0315\n",
      "Epoch 42/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0265\n",
      "Epoch 00042: val_loss did not improve from 0.03152\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "6910/6910 [==============================] - 109s 16ms/step - loss: 0.0265 - val_loss: 0.0315\n",
      "Epoch 43/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0264\n",
      "Epoch 00043: val_loss improved from 0.03152 to 0.03151, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 105s 15ms/step - loss: 0.0264 - val_loss: 0.0315\n",
      "Epoch 44/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0263\n",
      "Epoch 00044: val_loss improved from 0.03151 to 0.03151, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 111s 16ms/step - loss: 0.0263 - val_loss: 0.0315\n",
      "Epoch 45/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 00045: val_loss improved from 0.03151 to 0.03150, saving model to seq2seq_bigram.h5\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "6910/6910 [==============================] - 112s 16ms/step - loss: 0.0264 - val_loss: 0.0315\n",
      "Epoch 46/50\n",
      "6907/6910 [============================>.] - ETA: 0s - loss: 0.0263\n",
      "Epoch 00046: val_loss improved from 0.03150 to 0.03149, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 107s 15ms/step - loss: 0.0263 - val_loss: 0.0315\n",
      "Epoch 47/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0263\n",
      "Epoch 00047: val_loss improved from 0.03149 to 0.03149, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 106s 15ms/step - loss: 0.0263 - val_loss: 0.0315\n",
      "Epoch 48/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0264- ETA\n",
      "Epoch 00048: val_loss improved from 0.03149 to 0.03149, saving model to seq2seq_bigram.h5\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "6910/6910 [==============================] - 105s 15ms/step - loss: 0.0264 - val_loss: 0.0315\n",
      "Epoch 49/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 00049: val_loss improved from 0.03149 to 0.03149, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 107s 15ms/step - loss: 0.0264 - val_loss: 0.0315\n",
      "Epoch 50/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0263\n",
      "Epoch 00050: val_loss improved from 0.03149 to 0.03149, saving model to seq2seq_bigram.h5\n",
      "6910/6910 [==============================] - 108s 16ms/step - loss: 0.0263 - val_loss: 0.0315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x224068e7048>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, max_len, max_len)\n",
    "model.compile(optimizer = 'Adam',loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "callbacks = [ModelCheckpoint('seq2seq_bigram.h5', save_best_only= True, verbose = 1),\n",
    "             EarlyStopping(patience = 5, verbose = 1), \n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]\n",
    "\n",
    "model.fit(x = bigram_train_dataset, \n",
    "          steps_per_epoch = bigram_train.shape[0]//batch_size,\n",
    "          validation_data = bigram_val_dataset,\n",
    "          validation_steps = bigram_val.shape[0]//batch_size,\n",
    "          epochs = 50,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, max_len, max_len, bigram_word_to_index)\n",
    "pred_model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy')\n",
    "pred_model.build(input_shape=(None, 1, max_len))\n",
    "pred_model.load_weights('seq2seq_bigram.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  no Lne\n",
      "predicted output :  no one\n",
      "actual output : no one\n"
     ]
    }
   ],
   "source": [
    "sentence = bigram_train['input'].values[4]\n",
    "print('input : ', sentence)\n",
    "result = predict(sentence, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', bigram_train['output'].values[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  for themseves\n",
      "predicted output :  for themselves\n",
      "actual output : for themselves\n"
     ]
    }
   ],
   "source": [
    "sentence = bigram_train['input'].values[6]\n",
    "print('input : ', sentence)\n",
    "result = predict(sentence, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', bigram_train['output'].values[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  detil of\n",
      "predicted output :  detail of\n",
      "actual output : detail of\n"
     ]
    }
   ],
   "source": [
    "sentence = bigram_train['input'].values[7]\n",
    "print('input : ', sentence)\n",
    "result = predict(sentence, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', bigram_train['output'].values[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9467181213926804"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▍                                                                     | 30534/884533 [30:10<13:29:26, 17.58it/s]"
     ]
    }
   ],
   "source": [
    "val_bleu/bigram_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 98282/98282 [1:34:58<00:00, 17.25it/s]\n",
      "  3%|██▍                                                                     | 30394/884533 [29:43<14:21:38, 16.52it/s]Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x000002854C787828>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vineet Mukesh Haswan\\anaconda3\\lib\\weakref.py\", line 358, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "KeyboardInterrupt\n",
      "  3%|██▍                                                                     | 30534/884533 [29:51<13:29:26, 17.58it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-8a985075c849>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigram_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigram_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigram_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigram_index_to_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'bi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mtrain_bleu\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-6b49e0175639>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(seq, vectorizer, index_to_word, gram)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mseq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'<SOW>*'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'*<EOW>'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mseq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_bleu = 0\n",
    "for i in tqdm(range(bigram_val.shape[0])):\n",
    "    inp = bigram_val['input'].values[i]\n",
    "    out = bigram_val['output'].values[i]\n",
    "    pred = predict(inp, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "    val_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(bigram_train.shape[0])):\n",
    "    inp = bigram_train['input'].values[i];\n",
    "    out = bigram_train['output'].values[i]\n",
    "    pred = predict(inp, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "    train_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(bigram_test.shape[0])):\n",
    "    inp = bigram_test['input'].values[i]\n",
    "    out = bigram_test['output'].values[i]\n",
    "    pred = predict(inp, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "    test_bleu += sentence_bleu([out], pred)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/bigram_train.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/bigram_val.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/bigram_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.3 TriGram</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(bigram_vec.get_vocabulary())\n",
    "embedding_dim = 100\n",
    "lstm_size = 256\n",
    "max_len = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.7007\n",
      "Epoch 00001: val_loss improved from inf to 0.21410, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 122s 18ms/step - loss: 0.7007 - val_loss: 0.2141\n",
      "Epoch 2/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.1938\n",
      "Epoch 00002: val_loss improved from 0.21410 to 0.08815, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 129s 19ms/step - loss: 0.1938 - val_loss: 0.0882\n",
      "Epoch 3/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.1176\n",
      "Epoch 00003: val_loss improved from 0.08815 to 0.06765, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 129s 19ms/step - loss: 0.1176 - val_loss: 0.0676\n",
      "Epoch 4/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0948\n",
      "Epoch 00004: val_loss improved from 0.06765 to 0.05852, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 130s 19ms/step - loss: 0.0948 - val_loss: 0.0585\n",
      "Epoch 5/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0832\n",
      "Epoch 00005: val_loss improved from 0.05852 to 0.05359, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 131s 19ms/step - loss: 0.0832 - val_loss: 0.0536\n",
      "Epoch 6/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0759\n",
      "Epoch 00006: val_loss improved from 0.05359 to 0.05043, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 135s 19ms/step - loss: 0.0759 - val_loss: 0.0504\n",
      "Epoch 7/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0710\n",
      "Epoch 00007: val_loss improved from 0.05043 to 0.04756, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 134s 19ms/step - loss: 0.0710 - val_loss: 0.0476\n",
      "Epoch 8/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0671\n",
      "Epoch 00008: val_loss improved from 0.04756 to 0.04655, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 134s 19ms/step - loss: 0.0671 - val_loss: 0.0465\n",
      "Epoch 9/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0641\n",
      "Epoch 00009: val_loss improved from 0.04655 to 0.04472, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 135s 20ms/step - loss: 0.0641 - val_loss: 0.0447\n",
      "Epoch 10/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0617\n",
      "Epoch 00010: val_loss improved from 0.04472 to 0.04360, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 134s 19ms/step - loss: 0.0617 - val_loss: 0.0436\n",
      "Epoch 11/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0595\n",
      "Epoch 00011: val_loss improved from 0.04360 to 0.04321, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 136s 20ms/step - loss: 0.0595 - val_loss: 0.0432\n",
      "Epoch 12/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0579\n",
      "Epoch 00012: val_loss improved from 0.04321 to 0.04224, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0579 - val_loss: 0.0422\n",
      "Epoch 13/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0564\n",
      "Epoch 00013: val_loss improved from 0.04224 to 0.04159, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 136s 20ms/step - loss: 0.0564 - val_loss: 0.0416\n",
      "Epoch 14/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0550\n",
      "Epoch 00014: val_loss improved from 0.04159 to 0.04106, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0550 - val_loss: 0.0411\n",
      "Epoch 15/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0538\n",
      "Epoch 00015: val_loss improved from 0.04106 to 0.04039, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0538 - val_loss: 0.0404\n",
      "Epoch 16/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0528\n",
      "Epoch 00016: val_loss improved from 0.04039 to 0.04007, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 136s 20ms/step - loss: 0.0528 - val_loss: 0.0401\n",
      "Epoch 17/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0519\n",
      "Epoch 00017: val_loss improved from 0.04007 to 0.03975, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0519 - val_loss: 0.0398\n",
      "Epoch 18/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0510\n",
      "Epoch 00018: val_loss improved from 0.03975 to 0.03891, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0510 - val_loss: 0.0389\n",
      "Epoch 19/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0501\n",
      "Epoch 00019: val_loss improved from 0.03891 to 0.03879, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0501 - val_loss: 0.0388\n",
      "Epoch 20/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0494\n",
      "Epoch 00020: val_loss improved from 0.03879 to 0.03860, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0494 - val_loss: 0.0386\n",
      "Epoch 21/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0488\n",
      "Epoch 00021: val_loss improved from 0.03860 to 0.03802, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0488 - val_loss: 0.0380\n",
      "Epoch 22/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0481\n",
      "Epoch 00022: val_loss improved from 0.03802 to 0.03792, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 138s 20ms/step - loss: 0.0481 - val_loss: 0.0379\n",
      "Epoch 23/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0475\n",
      "Epoch 00023: val_loss improved from 0.03792 to 0.03753, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0475 - val_loss: 0.0375\n",
      "Epoch 24/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0470- ETA: 0s - loss: 0.04\n",
      "Epoch 00024: val_loss did not improve from 0.03753\n",
      "6910/6910 [==============================] - 139s 20ms/step - loss: 0.0470 - val_loss: 0.0377\n",
      "Epoch 25/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0464\n",
      "Epoch 00025: val_loss improved from 0.03753 to 0.03730, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 139s 20ms/step - loss: 0.0464 - val_loss: 0.0373\n",
      "Epoch 26/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0459\n",
      "Epoch 00026: val_loss did not improve from 0.03730\n",
      "6910/6910 [==============================] - 138s 20ms/step - loss: 0.0459 - val_loss: 0.0373\n",
      "Epoch 27/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0455\n",
      "Epoch 00027: val_loss improved from 0.03730 to 0.03691, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 136s 20ms/step - loss: 0.0455 - val_loss: 0.0369\n",
      "Epoch 28/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0450\n",
      "Epoch 00028: val_loss did not improve from 0.03691\n",
      "6910/6910 [==============================] - 136s 20ms/step - loss: 0.0450 - val_loss: 0.0370\n",
      "Epoch 29/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0447\n",
      "Epoch 00029: val_loss improved from 0.03691 to 0.03646, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0447 - val_loss: 0.0365\n",
      "Epoch 30/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0443\n",
      "Epoch 00030: val_loss improved from 0.03646 to 0.03634, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0443 - val_loss: 0.0363\n",
      "Epoch 31/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0439\n",
      "Epoch 00031: val_loss improved from 0.03634 to 0.03623, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 139s 20ms/step - loss: 0.0439 - val_loss: 0.0362\n",
      "Epoch 32/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0435\n",
      "Epoch 00032: val_loss did not improve from 0.03623\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0435 - val_loss: 0.0363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0432\n",
      "Epoch 00033: val_loss improved from 0.03623 to 0.03611, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0432 - val_loss: 0.0361\n",
      "Epoch 34/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0429\n",
      "Epoch 00034: val_loss improved from 0.03611 to 0.03586, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0429 - val_loss: 0.0359\n",
      "Epoch 35/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0425\n",
      "Epoch 00035: val_loss did not improve from 0.03586\n",
      "6910/6910 [==============================] - 138s 20ms/step - loss: 0.0425 - val_loss: 0.0362\n",
      "Epoch 36/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0423\n",
      "Epoch 00036: val_loss did not improve from 0.03586\n",
      "6910/6910 [==============================] - 138s 20ms/step - loss: 0.0423 - val_loss: 0.0359\n",
      "Epoch 37/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0420\n",
      "Epoch 00037: val_loss did not improve from 0.03586\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0420 - val_loss: 0.0359\n",
      "Epoch 38/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0363\n",
      "Epoch 00038: val_loss improved from 0.03586 to 0.03275, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0363 - val_loss: 0.0327\n",
      "Epoch 39/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0346\n",
      "Epoch 00039: val_loss improved from 0.03275 to 0.03238, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 137s 20ms/step - loss: 0.0346 - val_loss: 0.0324\n",
      "Epoch 40/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0339\n",
      "Epoch 00040: val_loss improved from 0.03238 to 0.03222, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 138s 20ms/step - loss: 0.0339 - val_loss: 0.0322\n",
      "Epoch 41/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0336\n",
      "Epoch 00041: val_loss improved from 0.03222 to 0.03208, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 138s 20ms/step - loss: 0.0336 - val_loss: 0.0321\n",
      "Epoch 42/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0333\n",
      "Epoch 00042: val_loss improved from 0.03208 to 0.03205, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 136s 20ms/step - loss: 0.0333 - val_loss: 0.0321\n",
      "Epoch 43/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0331\n",
      "Epoch 00043: val_loss did not improve from 0.03205\n",
      "6910/6910 [==============================] - 136s 20ms/step - loss: 0.0331 - val_loss: 0.0321\n",
      "Epoch 44/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0329\n",
      "Epoch 00044: val_loss improved from 0.03205 to 0.03197, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 139s 20ms/step - loss: 0.0329 - val_loss: 0.0320\n",
      "Epoch 45/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0327\n",
      "Epoch 00045: val_loss improved from 0.03197 to 0.03190, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 138s 20ms/step - loss: 0.0327 - val_loss: 0.0319\n",
      "Epoch 46/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0326\n",
      "Epoch 00046: val_loss did not improve from 0.03190\n",
      "6910/6910 [==============================] - 134s 19ms/step - loss: 0.0326 - val_loss: 0.0319\n",
      "Epoch 47/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0324\n",
      "Epoch 00047: val_loss did not improve from 0.03190\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "6910/6910 [==============================] - 135s 20ms/step - loss: 0.0324 - val_loss: 0.0319\n",
      "Epoch 48/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0318\n",
      "Epoch 00048: val_loss improved from 0.03190 to 0.03173, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 136s 20ms/step - loss: 0.0318 - val_loss: 0.0317\n",
      "Epoch 49/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0316\n",
      "Epoch 00049: val_loss improved from 0.03173 to 0.03169, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 134s 19ms/step - loss: 0.0316 - val_loss: 0.0317\n",
      "Epoch 50/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0315\n",
      "Epoch 00050: val_loss improved from 0.03169 to 0.03166, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 133s 19ms/step - loss: 0.0315 - val_loss: 0.0317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x224099166c8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, max_len, max_len)\n",
    "model.compile(optimizer = 'Adam',loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "callbacks = [ModelCheckpoint('seq2seq_trigram.h5', save_best_only= True, verbose = 1),\n",
    "             EarlyStopping(patience = 5, verbose = 1), \n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]\n",
    "\n",
    "model.fit(x = trigram_train_dataset, \n",
    "          steps_per_epoch = trigram_train.shape[0]//batch_size,\n",
    "          validation_data = trigram_val_dataset,\n",
    "          validation_steps = trigram_val.shape[0]//batch_size,\n",
    "          epochs = 50,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0315\n",
      "Epoch 00001: val_loss improved from inf to 0.03166, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 123s 18ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 2/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0314\n",
      "Epoch 00002: val_loss improved from 0.03166 to 0.03165, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 128s 19ms/step - loss: 0.0314 - val_loss: 0.0316\n",
      "Epoch 3/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0314\n",
      "Epoch 00003: val_loss did not improve from 0.03165\n",
      "6910/6910 [==============================] - 130s 19ms/step - loss: 0.0314 - val_loss: 0.0316\n",
      "Epoch 4/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0315\n",
      "Epoch 00004: val_loss improved from 0.03165 to 0.03163, saving model to seq2seq_trigram.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "6910/6910 [==============================] - 131s 19ms/step - loss: 0.0315 - val_loss: 0.0316\n",
      "Epoch 5/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0314\n",
      "Epoch 00005: val_loss improved from 0.03163 to 0.03161, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 132s 19ms/step - loss: 0.0314 - val_loss: 0.0316\n",
      "Epoch 6/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0313- ETA: 0s - loss: 0.0\n",
      "Epoch 00006: val_loss improved from 0.03161 to 0.03160, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 130s 19ms/step - loss: 0.0313 - val_loss: 0.0316\n",
      "Epoch 7/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0313\n",
      "Epoch 00007: val_loss improved from 0.03160 to 0.03159, saving model to seq2seq_trigram.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "6910/6910 [==============================] - 132s 19ms/step - loss: 0.0313 - val_loss: 0.0316\n",
      "Epoch 8/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0313\n",
      "Epoch 00008: val_loss improved from 0.03159 to 0.03159, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 133s 19ms/step - loss: 0.0313 - val_loss: 0.0316\n",
      "Epoch 9/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0313\n",
      "Epoch 00009: val_loss improved from 0.03159 to 0.03159, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 131s 19ms/step - loss: 0.0313 - val_loss: 0.0316\n",
      "Epoch 10/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0313\n",
      "Epoch 00010: val_loss improved from 0.03159 to 0.03159, saving model to seq2seq_trigram.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "6910/6910 [==============================] - 130s 19ms/step - loss: 0.0313 - val_loss: 0.0316\n",
      "Epoch 11/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0314\n",
      "Epoch 00011: val_loss improved from 0.03159 to 0.03159, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 133s 19ms/step - loss: 0.0314 - val_loss: 0.0316\n",
      "Epoch 12/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0314\n",
      "Epoch 00012: val_loss improved from 0.03159 to 0.03159, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 133s 19ms/step - loss: 0.0314 - val_loss: 0.0316\n",
      "Epoch 13/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0313\n",
      "Epoch 00013: val_loss improved from 0.03159 to 0.03159, saving model to seq2seq_trigram.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "6910/6910 [==============================] - 131s 19ms/step - loss: 0.0313 - val_loss: 0.0316\n",
      "Epoch 14/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0313\n",
      "Epoch 00014: val_loss did not improve from 0.03159\n",
      "6910/6910 [==============================] - 132s 19ms/step - loss: 0.0313 - val_loss: 0.0316\n",
      "Epoch 15/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0314\n",
      "Epoch 00015: val_loss improved from 0.03159 to 0.03159, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 130s 19ms/step - loss: 0.0314 - val_loss: 0.0316\n",
      "Epoch 16/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0313\n",
      "Epoch 00016: val_loss improved from 0.03159 to 0.03159, saving model to seq2seq_trigram.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "6910/6910 [==============================] - 132s 19ms/step - loss: 0.0313 - val_loss: 0.0316\n",
      "Epoch 17/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0313\n",
      "Epoch 00017: val_loss improved from 0.03159 to 0.03159, saving model to seq2seq_trigram.h5\n",
      "6910/6910 [==============================] - 127s 18ms/step - loss: 0.0313 - val_loss: 0.0316\n",
      "Epoch 18/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0313\n",
      "Epoch 00018: val_loss did not improve from 0.03159\n",
      "6910/6910 [==============================] - 127s 18ms/step - loss: 0.0313 - val_loss: 0.0316\n",
      "Epoch 19/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0313\n",
      "Epoch 00019: val_loss did not improve from 0.03159\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "6910/6910 [==============================] - 128s 19ms/step - loss: 0.0313 - val_loss: 0.0316\n",
      "Epoch 20/50\n",
      "6908/6910 [============================>.] - ETA: 0s - loss: 0.0314\n",
      "Epoch 00020: val_loss did not improve from 0.03159\n",
      "6910/6910 [==============================] - 128s 19ms/step - loss: 0.0314 - val_loss: 0.0316\n",
      "Epoch 21/50\n",
      "6909/6910 [============================>.] - ETA: 0s - loss: 0.0314\n",
      "Epoch 00021: val_loss did not improve from 0.03159\n",
      "6910/6910 [==============================] - 128s 19ms/step - loss: 0.0314 - val_loss: 0.0316\n",
      "Epoch 22/50\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0313\n",
      "Epoch 00022: val_loss did not improve from 0.03159\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "6910/6910 [==============================] - 129s 19ms/step - loss: 0.0313 - val_loss: 0.0316\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x224047a6788>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model = Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, max_len, max_len)\n",
    "model.compile(optimizer = Adam(1.0000000474974514e-05),loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "callbacks = [ModelCheckpoint('seq2seq_trigram.h5', save_best_only= True, verbose = 1),\n",
    "             EarlyStopping(patience = 5, verbose = 1), \n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]\n",
    "\n",
    "model.build(input_shape = (None, batch_size, max_len))\n",
    "model.load_weights('seq2seq_trigram.h5')\n",
    "\n",
    "model.fit(x = trigram_train_dataset, \n",
    "          steps_per_epoch = trigram_train.shape[0]//batch_size,\n",
    "          validation_data = trigram_val_dataset,\n",
    "          validation_steps = trigram_val.shape[0]//batch_size,\n",
    "          epochs = 50,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, max_len, max_len, trigram_word_to_index)\n",
    "pred_model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy')\n",
    "pred_model.build(input_shape=(None, 1, max_len))\n",
    "pred_model.load_weights('seq2seq_trigram.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  woman oAf him\n",
      "predicted output :  woman of him\n",
      "actual output : woman of him\n"
     ]
    }
   ],
   "source": [
    "sentence = trigram_train['input'].values[4]\n",
    "print('input : ', sentence)\n",
    "result = predict(sentence, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', trigram_train['output'].values[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  endurXe a collision\n",
      "predicted output :  endure a collision\n",
      "actual output : endure a collision\n"
     ]
    }
   ],
   "source": [
    "sentence = trigram_train['input'].values[7]\n",
    "print('input : ', sentence)\n",
    "result = predict(sentence, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', trigram_train['output'].values[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  marshals sitti g on\n",
      "predicted output :  marshals sitting on\n",
      "actual output : marshals sitting on\n"
     ]
    }
   ],
   "source": [
    "sentence = trigram_train['input'].values[10]\n",
    "print('input : ', sentence)\n",
    "result = predict(sentence, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', trigram_train['output'].values[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bleu = 0\n",
    "for i in tqdm(range(trigram_val.shape[0])):\n",
    "    inp = trigram_val['input'].values[i]\n",
    "    out = trigram_val['output'].values[i]\n",
    "    pred = predict(inp, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "    val_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    inp = trigram_train['input'].values[i];\n",
    "    out = trigram_train['output'].values[i]\n",
    "    pred = predict(inp, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "    train_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    inp = trigram_test['input'].values[i]\n",
    "    out = trigram_test['output'].values[i]\n",
    "    pred = predict(inp, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "    test_bleu += sentence_bleu([out], pred)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/trigram_train.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/trigram_val.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/trigram_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Seq2Seq with Attention Mechanism</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        #Initialize Embedding layer\n",
    "        self.enc_embed = Embedding(input_dim = inp_vocab_size, output_dim = embedding_size)\n",
    "        #Intialize Encoder LSTM layer\n",
    "        self.enc_lstm = LSTM(lstm_size, return_sequences = True, return_state = True, dropout = 0.4)\n",
    "        \n",
    "    def call(self,input_sequence,states):\n",
    "        embedding = self.enc_embed(input_sequence)\n",
    "        output_state, enc_h, enc_c = self.enc_lstm(embedding, initial_state = states)\n",
    "        return output_state, enc_h, enc_c\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "        return [tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))]\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,scoring_function, att_units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scoring_function = scoring_function\n",
    "        if scoring_function == 'dot':\n",
    "            self.dot = Dot(axes = (1, 2))\n",
    "        elif scoring_function == 'general':\n",
    "          # Intialize variables needed for General score function here\n",
    "            self.W = Dense(att_units)\n",
    "            self.dot = Dot(axes = (1, 2))\n",
    "        elif scoring_function == 'concat':\n",
    "          # Intialize variables needed for Concat score function here\n",
    "            self.W1 = Dense(att_units)\n",
    "            self.W2 = Dense(att_units)\n",
    "            self.V = Dense(1)\n",
    "    def call(self,decoder_hidden_state,encoder_output):\n",
    "    \n",
    "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 1)\n",
    "        \n",
    "        if self.scoring_function == 'dot':\n",
    "            # Implement Dot score function here\n",
    "            score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), encoder_output]), (0, 2,1))\n",
    "            \n",
    "        elif self.scoring_function == 'general':\n",
    "            # Implement General score function here\n",
    "            mul = self.W(encoder_output)\n",
    "            score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), mul]), (0, 2,1))\n",
    "            \n",
    "        elif self.scoring_function == 'concat':\n",
    "            # Implement General score function here\n",
    "            inter = self.W1(decoder_hidden_state) + self.W2(encoder_output)\n",
    "            tan = tf.nn.tanh(inter)\n",
    "            score = self.V(tan)\n",
    "        attention_weights = tf.nn.softmax(score, axis =1)\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class OneStepDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "        super(OneStepDecoder, self).__init__()\n",
    "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "        self.embed_dec = Embedding(input_dim = tar_vocab_size, output_dim = embedding_dim)\n",
    "        self.lstm = LSTM(dec_units, return_sequences = True, return_state = True, dropout = 0.4)\n",
    "        self.attention = Attention(scoring_function = score_fun, att_units = att_units)\n",
    "        self.fc = Dense(tar_vocab_size)\n",
    "    \n",
    "    def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "        embed = self.embed_dec(input_to_decoder)\n",
    "        context_vect, attention_weights = self.attention(state_h, encoder_output)    \n",
    "        final_inp = tf.concat([tf.expand_dims(context_vect, 1), embed], axis = -1)\n",
    "        out, dec_h, dec_c = self.lstm(final_inp, [state_h, state_c])\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        output = self.fc(out)\n",
    "        output = Dropout(0.5)(output)\n",
    "        return output, dec_h, dec_c, attention_weights, context_vect\n",
    "\n",
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_dim, enc_units, dec_units, max_len_inp, max_len_out, score_fun, att_units, batch_size):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super(encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_dim, enc_units, max_len_inp)\n",
    "        self.one_step_decoder = OneStepDecoder(out_vocab_size, embedding_dim, max_len_out, dec_units ,score_fun ,att_units)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def call(self, data):\n",
    "        enc_inp, dec_inp = data[0], data[1]\n",
    "        initial_state = self.encoder.initialize_states(self.batch_size)\n",
    "        enc_output, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        all_outputs = tf.TensorArray(dtype = tf.float32, size= max_len)\n",
    "        \n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        for timestep in range(max_len):\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            output, dec_h, dec_c, _, _ = self.one_step_decoder(dec_inp[:, timestep:timestep+1], \n",
    "                                                               enc_output, \n",
    "                                                               dec_h,\n",
    "                                                               dec_c)\n",
    "            # Store the output in tensorarray\n",
    "            all_outputs = all_outputs.write(timestep, output)\n",
    "        # Return the tensor array\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0, 2))\n",
    "        # return the decoder output\n",
    "        return all_outputs\n",
    "\n",
    "class pred_Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_dim, enc_units, dec_units, max_len_ita, max_len_eng, score_fun, att_units, word_to_index):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super(pred_Encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_dim, enc_units, max_len_ita)\n",
    "        self.one_step_decoder = OneStepDecoder(out_vocab_size, embedding_dim, max_len_eng, dec_units ,score_fun ,att_units)\n",
    "        self.batch_size = batch_size\n",
    "        self.word_to_index = word_to_index\n",
    "\n",
    "    def call(self, params):\n",
    "        enc_inp = params[0]\n",
    "        initial_state = self.encoder.initialize_states(1)\n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        pred = tf.expand_dims([self.word_to_index['<SOW>']], 0)\n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        all_pred = []\n",
    "        all_attention = []\n",
    "        for t in range(max_len):  \n",
    "            output, dec_h,dec_c, attention, _ = self.one_step_decoder(pred, output_state, dec_h, dec_c)\n",
    "            pred = tf.argmax(output, axis = -1)\n",
    "            all_pred.append(pred)\n",
    "            pred = tf.expand_dims(pred, 0)\n",
    "            all_attention.append(attention)\n",
    "        return all_pred, all_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(seq, vectorizer, index_to_word, gram = 'uni'):\n",
    "    if gram =='uni':\n",
    "        seq = '<SOW> '+' '.join(list(seq))+' <EOW>'\n",
    "    else:\n",
    "        seq = '<SOW>*'+'*'.join(list(seq))+'*<EOW>'\n",
    "    seq = vectorizer([seq])\n",
    "    pred, attention_weights = pred_model.predict(tf.expand_dims(seq, 0))\n",
    "    output = []\n",
    "    for i in pred:\n",
    "        word = index_to_word[i[0]]\n",
    "        if word == '<EOW>':\n",
    "            break\n",
    "        output.append(word)\n",
    "    return ''.join(output), np.squeeze(np.squeeze(np.array(attention_weights), 1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.1 UniGram </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "embedding_dim = 100\n",
    "att_units = 256\n",
    "maxlen = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 1.3239\n",
      "Epoch 00001: val_loss improved from inf to 0.96448, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 19s 74ms/step - loss: 1.3239 - val_loss: 0.9645\n",
      "Epoch 2/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 1.1640\n",
      "Epoch 00002: val_loss improved from 0.96448 to 0.73190, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 13s 50ms/step - loss: 1.1640 - val_loss: 0.7319\n",
      "Epoch 3/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 1.0338\n",
      "Epoch 00003: val_loss improved from 0.73190 to 0.44536, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 13s 51ms/step - loss: 1.0337 - val_loss: 0.4454\n",
      "Epoch 4/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.8934- ETA: \n",
      "Epoch 00004: val_loss improved from 0.44536 to 0.26591, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 13s 51ms/step - loss: 0.8934 - val_loss: 0.2659\n",
      "Epoch 5/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.8349\n",
      "Epoch 00005: val_loss improved from 0.26591 to 0.21211, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 13s 51ms/step - loss: 0.8349 - val_loss: 0.2121\n",
      "Epoch 6/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.8071\n",
      "Epoch 00006: val_loss improved from 0.21211 to 0.18196, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 54ms/step - loss: 0.8071 - val_loss: 0.1820\n",
      "Epoch 7/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7956\n",
      "Epoch 00007: val_loss improved from 0.18196 to 0.16000, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 13s 52ms/step - loss: 0.7956 - val_loss: 0.1600\n",
      "Epoch 8/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.7887- ETA: 0s - l\n",
      "Epoch 00008: val_loss improved from 0.16000 to 0.15305, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 52ms/step - loss: 0.7887 - val_loss: 0.1530\n",
      "Epoch 9/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7819\n",
      "Epoch 00009: val_loss improved from 0.15305 to 0.14978, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 54ms/step - loss: 0.7819 - val_loss: 0.1498\n",
      "Epoch 10/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.7781\n",
      "Epoch 00010: val_loss improved from 0.14978 to 0.14204, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 54ms/step - loss: 0.7782 - val_loss: 0.1420\n",
      "Epoch 11/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7725\n",
      "Epoch 00011: val_loss improved from 0.14204 to 0.13989, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7725 - val_loss: 0.1399\n",
      "Epoch 12/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7676\n",
      "Epoch 00012: val_loss improved from 0.13989 to 0.13404, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7676 - val_loss: 0.1340\n",
      "Epoch 13/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7644\n",
      "Epoch 00013: val_loss improved from 0.13404 to 0.12980, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 54ms/step - loss: 0.7644 - val_loss: 0.1298\n",
      "Epoch 14/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7605\n",
      "Epoch 00014: val_loss improved from 0.12980 to 0.12613, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 54ms/step - loss: 0.7605 - val_loss: 0.1261\n",
      "Epoch 15/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7610\n",
      "Epoch 00015: val_loss did not improve from 0.12613\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7610 - val_loss: 0.1295\n",
      "Epoch 16/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7585\n",
      "Epoch 00016: val_loss did not improve from 0.12613\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7585 - val_loss: 0.1265\n",
      "Epoch 17/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7560\n",
      "Epoch 00017: val_loss improved from 0.12613 to 0.12149, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7560 - val_loss: 0.1215\n",
      "Epoch 18/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7552\n",
      "Epoch 00018: val_loss improved from 0.12149 to 0.12073, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 55ms/step - loss: 0.7552 - val_loss: 0.1207\n",
      "Epoch 19/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7534\n",
      "Epoch 00019: val_loss improved from 0.12073 to 0.11933, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 55ms/step - loss: 0.7534 - val_loss: 0.1193\n",
      "Epoch 20/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7519\n",
      "Epoch 00020: val_loss improved from 0.11933 to 0.11756, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7519 - val_loss: 0.1176\n",
      "Epoch 21/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.7455\n",
      "Epoch 00021: val_loss improved from 0.11756 to 0.11589, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7454 - val_loss: 0.1159\n",
      "Epoch 22/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7493\n",
      "Epoch 00022: val_loss did not improve from 0.11589\n",
      "258/258 [==============================] - 14s 52ms/step - loss: 0.7493 - val_loss: 0.1170\n",
      "Epoch 23/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7449\n",
      "Epoch 00023: val_loss improved from 0.11589 to 0.11506, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 55ms/step - loss: 0.7449 - val_loss: 0.1151\n",
      "Epoch 24/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7447\n",
      "Epoch 00024: val_loss did not improve from 0.11506\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7447 - val_loss: 0.1168\n",
      "Epoch 25/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.7438\n",
      "Epoch 00025: val_loss did not improve from 0.11506\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7440 - val_loss: 0.1155\n",
      "Epoch 26/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7442\n",
      "Epoch 00026: val_loss improved from 0.11506 to 0.11340, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7442 - val_loss: 0.1134\n",
      "Epoch 27/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7413\n",
      "Epoch 00027: val_loss improved from 0.11340 to 0.11310, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 54ms/step - loss: 0.7413 - val_loss: 0.1131\n",
      "Epoch 28/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7421\n",
      "Epoch 00028: val_loss improved from 0.11310 to 0.11093, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7421 - val_loss: 0.1109\n",
      "Epoch 29/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7379\n",
      "Epoch 00029: val_loss improved from 0.11093 to 0.10953, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7379 - val_loss: 0.1095\n",
      "Epoch 30/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7356\n",
      "Epoch 00030: val_loss did not improve from 0.10953\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7356 - val_loss: 0.1112\n",
      "Epoch 31/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.7349\n",
      "Epoch 00031: val_loss improved from 0.10953 to 0.10774, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7350 - val_loss: 0.1077\n",
      "Epoch 32/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7343\n",
      "Epoch 00032: val_loss improved from 0.10774 to 0.10720, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7343 - val_loss: 0.1072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7364\n",
      "Epoch 00033: val_loss did not improve from 0.10720\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7364 - val_loss: 0.1104\n",
      "Epoch 34/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.7355\n",
      "Epoch 00034: val_loss improved from 0.10720 to 0.10713, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7355 - val_loss: 0.1071\n",
      "Epoch 35/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.7310\n",
      "Epoch 00035: val_loss did not improve from 0.10713\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7311 - val_loss: 0.1073\n",
      "Epoch 36/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.7241\n",
      "Epoch 00036: val_loss improved from 0.10713 to 0.10300, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 54ms/step - loss: 0.7241 - val_loss: 0.1030\n",
      "Epoch 37/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7243\n",
      "Epoch 00037: val_loss improved from 0.10300 to 0.10202, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7243 - val_loss: 0.1020\n",
      "Epoch 38/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7227\n",
      "Epoch 00038: val_loss improved from 0.10202 to 0.10193, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7227 - val_loss: 0.1019\n",
      "Epoch 39/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7210\n",
      "Epoch 00039: val_loss improved from 0.10193 to 0.10119, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7210 - val_loss: 0.1012\n",
      "Epoch 40/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7227\n",
      "Epoch 00040: val_loss did not improve from 0.10119\n",
      "258/258 [==============================] - 14s 54ms/step - loss: 0.7227 - val_loss: 0.1014\n",
      "Epoch 41/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.7211\n",
      "Epoch 00041: val_loss improved from 0.10119 to 0.10118, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7213 - val_loss: 0.1012\n",
      "Epoch 42/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7175\n",
      "Epoch 00042: val_loss did not improve from 0.10118\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "258/258 [==============================] - 14s 54ms/step - loss: 0.7175 - val_loss: 0.1013\n",
      "Epoch 43/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7193\n",
      "Epoch 00043: val_loss improved from 0.10118 to 0.10111, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7193 - val_loss: 0.1011\n",
      "Epoch 44/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7186\n",
      "Epoch 00044: val_loss improved from 0.10111 to 0.10101, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7186 - val_loss: 0.1010\n",
      "Epoch 45/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7193\n",
      "Epoch 00045: val_loss improved from 0.10101 to 0.10088, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 54ms/step - loss: 0.7193 - val_loss: 0.1009\n",
      "Epoch 46/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7194\n",
      "Epoch 00046: val_loss improved from 0.10088 to 0.10086, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7194 - val_loss: 0.1009\n",
      "Epoch 47/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7210\n",
      "Epoch 00047: val_loss improved from 0.10086 to 0.10083, saving model to Attention_concat_lstm.h5\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7210 - val_loss: 0.1008\n",
      "Epoch 48/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7186\n",
      "Epoch 00048: val_loss did not improve from 0.10083\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.7186 - val_loss: 0.1009\n",
      "Epoch 49/50\n",
      "257/258 [============================>.] - ETA: 0s - loss: 0.7182\n",
      "Epoch 00049: val_loss did not improve from 0.10083\n",
      "258/258 [==============================] - 14s 54ms/step - loss: 0.7182 - val_loss: 0.1009\n",
      "Epoch 50/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7206\n",
      "Epoch 00050: val_loss did not improve from 0.10083\n",
      "258/258 [==============================] - 14s 54ms/step - loss: 0.7206 - val_loss: 0.1008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x298c951fd88>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, lstm_size, maxlen, maxlen, 'concat', att_units, batch_size)\n",
    "model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "\n",
    "callbacks = [ModelCheckpoint('Attention_concat_lstm.h5', save_best_only= True, verbose = 1),\n",
    "             EarlyStopping(patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]\n",
    "\n",
    "model.fit(x = unigram_train_dataset, \n",
    "          steps_per_epoch = unigram_train.shape[0]//batch_size,\n",
    "          validation_data = unigram_val_dataset,\n",
    "          validation_steps = unigram_val.shape[0]//batch_size,\n",
    "          epochs = 50,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, lstm_size, maxlen, maxlen, 'concat', att_units)\n",
    "pred_model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "pred_model.build(input_shape= (None, 1, maxlen))\n",
    "pred_model.load_weights('Attention_concat_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  qitted\n",
      "predicted output :  quitted\n",
      "actual output : quitted\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = unigram_train['input'].values[5]\n",
    "result, attention_plot = predict(sentence, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', unigram_train['output'].values[5])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  fortissiemus\n",
      "predicted output :  fortissimus\n",
      "actual output : fortissimus\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = unigram_train['input'].values[11]\n",
    "result, attention_plot = predict(sentence, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', unigram_train['output'].values[11])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: numbeGr\n",
      "predicted output: number\n",
      "actual output: number\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = unigram_train['input'].values[14]\n",
    "result, attention_plot = predict(sentence, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "\n",
    "print('input:', sentence)\n",
    "print('predicted output:',result)\n",
    "print('actual output:', unigram_train['output'].values[14])\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3678/3678 [03:40<00:00, 16.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 33101/33101 [32:52<00:00, 16.78it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3150/3150 [03:06<00:00, 16.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score on train:  0.8698739128364643\n",
      "BLEU Score on val:  0.792070857805782\n",
      "BLEU Score on test:  0.7071433929743683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_bleu = 0\n",
    "for i in tqdm(range(unigram_val.shape[0])):\n",
    "    inp = unigram_val['input'].values[i]\n",
    "    out = unigram_val['output'].values[i]\n",
    "    pred, _ = predict(inp, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "    val_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(unigram_train.shape[0])):\n",
    "    inp = unigram_train['input'].values[i];\n",
    "    out = unigram_train['output'].values[i]\n",
    "    pred, _ = predict(inp, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "    train_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(unigram_test.shape[0])):\n",
    "    inp = unigram_test['input'].values[i]\n",
    "    out = unigram_test['output'].values[i]\n",
    "    pred, _ = predict(inp, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "    test_bleu += sentence_bleu([out], pred)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/unigram_train.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/unigram_val.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/unigram_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2 BiGram </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "embedding_dim = 100\n",
    "att_units = 256\n",
    "maxlen = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8288\n",
      "Epoch 00001: val_loss improved from inf to 0.06538, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 471s 68ms/step - loss: 0.8288 - val_loss: 0.0654\n",
      "Epoch 2/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7482\n",
      "Epoch 00002: val_loss improved from 0.06538 to 0.05269, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 486s 70ms/step - loss: 0.7482 - val_loss: 0.0527\n",
      "Epoch 3/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7420\n",
      "Epoch 00003: val_loss improved from 0.05269 to 0.04566, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 498s 72ms/step - loss: 0.7420 - val_loss: 0.0457\n",
      "Epoch 4/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7379\n",
      "Epoch 00004: val_loss improved from 0.04566 to 0.04211, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 474s 69ms/step - loss: 0.7379 - val_loss: 0.0421\n",
      "Epoch 5/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7357\n",
      "Epoch 00005: val_loss improved from 0.04211 to 0.03942, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 463s 67ms/step - loss: 0.7357 - val_loss: 0.0394\n",
      "Epoch 6/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7333\n",
      "Epoch 00006: val_loss improved from 0.03942 to 0.03707, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 467s 68ms/step - loss: 0.7333 - val_loss: 0.0371\n",
      "Epoch 7/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7320\n",
      "Epoch 00007: val_loss improved from 0.03707 to 0.03556, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 486s 70ms/step - loss: 0.7320 - val_loss: 0.0356\n",
      "Epoch 8/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7315\n",
      "Epoch 00008: val_loss improved from 0.03556 to 0.03482, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 472s 68ms/step - loss: 0.7315 - val_loss: 0.0348\n",
      "Epoch 9/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7305\n",
      "Epoch 00009: val_loss improved from 0.03482 to 0.03351, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 461s 67ms/step - loss: 0.7305 - val_loss: 0.0335\n",
      "Epoch 10/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7293\n",
      "Epoch 00010: val_loss improved from 0.03351 to 0.03246, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 459s 66ms/step - loss: 0.7293 - val_loss: 0.0325\n",
      "Epoch 11/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7283\n",
      "Epoch 00011: val_loss improved from 0.03246 to 0.03200, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 468s 68ms/step - loss: 0.7283 - val_loss: 0.0320\n",
      "Epoch 12/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7279\n",
      "Epoch 00012: val_loss improved from 0.03200 to 0.03148, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 459s 66ms/step - loss: 0.7279 - val_loss: 0.0315\n",
      "Epoch 13/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7278\n",
      "Epoch 00013: val_loss improved from 0.03148 to 0.03059, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 461s 67ms/step - loss: 0.7278 - val_loss: 0.0306\n",
      "Epoch 14/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7273\n",
      "Epoch 00014: val_loss improved from 0.03059 to 0.02997, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 465s 67ms/step - loss: 0.7273 - val_loss: 0.0300\n",
      "Epoch 15/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7262\n",
      "Epoch 00015: val_loss improved from 0.02997 to 0.02988, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 468s 68ms/step - loss: 0.7262 - val_loss: 0.0299\n",
      "Epoch 16/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7260\n",
      "Epoch 00016: val_loss improved from 0.02988 to 0.02935, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 466s 67ms/step - loss: 0.7260 - val_loss: 0.0294\n",
      "Epoch 17/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7258\n",
      "Epoch 00017: val_loss improved from 0.02935 to 0.02916, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 469s 68ms/step - loss: 0.7258 - val_loss: 0.0292\n",
      "Epoch 18/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7256\n",
      "Epoch 00018: val_loss improved from 0.02916 to 0.02896, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 478s 69ms/step - loss: 0.7256 - val_loss: 0.0290\n",
      "Epoch 19/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7254- ETA:\n",
      "Epoch 00019: val_loss improved from 0.02896 to 0.02854, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 477s 69ms/step - loss: 0.7254 - val_loss: 0.0285\n",
      "Epoch 20/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7250\n",
      "Epoch 00020: val_loss improved from 0.02854 to 0.02826, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 485s 70ms/step - loss: 0.7250 - val_loss: 0.0283\n",
      "Epoch 21/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7245\n",
      "Epoch 00021: val_loss improved from 0.02826 to 0.02785, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 483s 70ms/step - loss: 0.7245 - val_loss: 0.0278\n",
      "Epoch 22/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7247\n",
      "Epoch 00022: val_loss did not improve from 0.02785\n",
      "6910/6910 [==============================] - 473s 69ms/step - loss: 0.7247 - val_loss: 0.0288\n",
      "Epoch 23/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7239\n",
      "Epoch 00023: val_loss did not improve from 0.02785\n",
      "6910/6910 [==============================] - 467s 68ms/step - loss: 0.7239 - val_loss: 0.0279\n",
      "Epoch 24/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7236\n",
      "Epoch 00024: val_loss improved from 0.02785 to 0.02764, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 459s 66ms/step - loss: 0.7236 - val_loss: 0.0276\n",
      "Epoch 25/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7239\n",
      "Epoch 00025: val_loss improved from 0.02764 to 0.02751, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 467s 68ms/step - loss: 0.7239 - val_loss: 0.0275\n",
      "Epoch 26/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7238\n",
      "Epoch 00026: val_loss did not improve from 0.02751\n",
      "6910/6910 [==============================] - 465s 67ms/step - loss: 0.7238 - val_loss: 0.0277\n",
      "Epoch 27/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7236\n",
      "Epoch 00027: val_loss improved from 0.02751 to 0.02704, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 473s 69ms/step - loss: 0.7236 - val_loss: 0.0270\n",
      "Epoch 28/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7231\n",
      "Epoch 00028: val_loss did not improve from 0.02704\n",
      "6910/6910 [==============================] - 464s 67ms/step - loss: 0.7231 - val_loss: 0.0271\n",
      "Epoch 29/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7233\n",
      "Epoch 00029: val_loss improved from 0.02704 to 0.02701, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 680s 98ms/step - loss: 0.7233 - val_loss: 0.0270\n",
      "Epoch 30/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7232\n",
      "Epoch 00030: val_loss improved from 0.02701 to 0.02620, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 459s 66ms/step - loss: 0.7232 - val_loss: 0.0262\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7231\n",
      "Epoch 00031: val_loss did not improve from 0.02620\n",
      "6910/6910 [==============================] - 472s 68ms/step - loss: 0.7231 - val_loss: 0.0263\n",
      "Epoch 32/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7230\n",
      "Epoch 00032: val_loss improved from 0.02620 to 0.02616, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 514s 74ms/step - loss: 0.7230 - val_loss: 0.0262\n",
      "Epoch 33/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7226\n",
      "Epoch 00033: val_loss did not improve from 0.02616\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "6910/6910 [==============================] - 486s 70ms/step - loss: 0.7226 - val_loss: 0.0263\n",
      "Epoch 34/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7206\n",
      "Epoch 00034: val_loss improved from 0.02616 to 0.02441, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 496s 72ms/step - loss: 0.7206 - val_loss: 0.0244\n",
      "Epoch 35/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7193\n",
      "Epoch 00035: val_loss improved from 0.02441 to 0.02397, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 472s 68ms/step - loss: 0.7193 - val_loss: 0.0240\n",
      "Epoch 36/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7191\n",
      "Epoch 00036: val_loss improved from 0.02397 to 0.02379, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 468s 68ms/step - loss: 0.7191 - val_loss: 0.0238\n",
      "Epoch 37/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7188\n",
      "Epoch 00037: val_loss improved from 0.02379 to 0.02357, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 459s 66ms/step - loss: 0.7188 - val_loss: 0.0236\n",
      "Epoch 38/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7187\n",
      "Epoch 00038: val_loss improved from 0.02357 to 0.02348, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 478s 69ms/step - loss: 0.7187 - val_loss: 0.0235\n",
      "Epoch 39/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7183\n",
      "Epoch 00039: val_loss improved from 0.02348 to 0.02332, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 460s 67ms/step - loss: 0.7183 - val_loss: 0.0233\n",
      "Epoch 40/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7179\n",
      "Epoch 00040: val_loss did not improve from 0.02332\n",
      "6910/6910 [==============================] - 468s 68ms/step - loss: 0.7179 - val_loss: 0.0233\n",
      "Epoch 41/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7183\n",
      "Epoch 00041: val_loss improved from 0.02332 to 0.02320, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 461s 67ms/step - loss: 0.7183 - val_loss: 0.0232\n",
      "Epoch 42/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7180\n",
      "Epoch 00042: val_loss did not improve from 0.02320\n",
      "6910/6910 [==============================] - 466s 67ms/step - loss: 0.7180 - val_loss: 0.0232\n",
      "Epoch 43/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7178\n",
      "Epoch 00043: val_loss improved from 0.02320 to 0.02312, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 455s 66ms/step - loss: 0.7178 - val_loss: 0.0231\n",
      "Epoch 44/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7180\n",
      "Epoch 00044: val_loss improved from 0.02312 to 0.02305, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 456s 66ms/step - loss: 0.7180 - val_loss: 0.0230\n",
      "Epoch 45/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7179\n",
      "Epoch 00045: val_loss improved from 0.02305 to 0.02298, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 467s 68ms/step - loss: 0.7179 - val_loss: 0.0230\n",
      "Epoch 46/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7173\n",
      "Epoch 00046: val_loss improved from 0.02298 to 0.02295, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 456s 66ms/step - loss: 0.7173 - val_loss: 0.0229\n",
      "Epoch 47/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7176\n",
      "Epoch 00047: val_loss improved from 0.02295 to 0.02288, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 456s 66ms/step - loss: 0.7176 - val_loss: 0.0229\n",
      "Epoch 48/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7171\n",
      "Epoch 00048: val_loss improved from 0.02288 to 0.02276, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 456s 66ms/step - loss: 0.7171 - val_loss: 0.0228\n",
      "Epoch 49/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7173\n",
      "Epoch 00049: val_loss did not improve from 0.02276\n",
      "6910/6910 [==============================] - 456s 66ms/step - loss: 0.7173 - val_loss: 0.0228\n",
      "Epoch 50/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7169\n",
      "Epoch 00050: val_loss improved from 0.02276 to 0.02275, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 456s 66ms/step - loss: 0.7169 - val_loss: 0.0227\n",
      "Epoch 51/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7173\n",
      "Epoch 00051: val_loss improved from 0.02275 to 0.02267, saving model to Attention_concat_lstm_bigram.h5\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "6910/6910 [==============================] - 456s 66ms/step - loss: 0.7173 - val_loss: 0.0227\n",
      "Epoch 52/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7171\n",
      "Epoch 00052: val_loss improved from 0.02267 to 0.02262, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 466s 67ms/step - loss: 0.7171 - val_loss: 0.0226\n",
      "Epoch 53/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7170\n",
      "Epoch 00053: val_loss improved from 0.02262 to 0.02258, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 458s 66ms/step - loss: 0.7170 - val_loss: 0.0226\n",
      "Epoch 54/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7168\n",
      "Epoch 00054: val_loss improved from 0.02258 to 0.02256, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 458s 66ms/step - loss: 0.7168 - val_loss: 0.0226\n",
      "Epoch 55/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7165\n",
      "Epoch 00055: val_loss improved from 0.02256 to 0.02251, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 457s 66ms/step - loss: 0.7165 - val_loss: 0.0225\n",
      "Epoch 56/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7169\n",
      "Epoch 00056: val_loss improved from 0.02251 to 0.02251, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 456s 66ms/step - loss: 0.7169 - val_loss: 0.0225\n",
      "Epoch 57/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7173\n",
      "Epoch 00057: val_loss improved from 0.02251 to 0.02251, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 456s 66ms/step - loss: 0.7173 - val_loss: 0.0225\n",
      "Epoch 58/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7169\n",
      "Epoch 00058: val_loss improved from 0.02251 to 0.02250, saving model to Attention_concat_lstm_bigram.h5\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "6910/6910 [==============================] - 455s 66ms/step - loss: 0.7169 - val_loss: 0.0225\n",
      "Epoch 59/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7168\n",
      "Epoch 00059: val_loss did not improve from 0.02250\n",
      "6910/6910 [==============================] - 456s 66ms/step - loss: 0.7168 - val_loss: 0.0225\n",
      "Epoch 60/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7164\n",
      "Epoch 00060: val_loss improved from 0.02250 to 0.02250, saving model to Attention_concat_lstm_bigram.h5\n",
      "6910/6910 [==============================] - 456s 66ms/step - loss: 0.7164 - val_loss: 0.0225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7172\n",
      "Epoch 00061: val_loss did not improve from 0.02250\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "6910/6910 [==============================] - 454s 66ms/step - loss: 0.7172 - val_loss: 0.0225\n",
      "Epoch 62/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7168\n",
      "Epoch 00062: val_loss did not improve from 0.02250\n",
      "6910/6910 [==============================] - 458s 66ms/step - loss: 0.7168 - val_loss: 0.0225\n",
      "Epoch 63/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7165\n",
      "Epoch 00063: val_loss did not improve from 0.02250\n",
      "6910/6910 [==============================] - 459s 66ms/step - loss: 0.7165 - val_loss: 0.0225\n",
      "Epoch 64/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7171\n",
      "Epoch 00064: val_loss did not improve from 0.02250\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "6910/6910 [==============================] - 454s 66ms/step - loss: 0.7171 - val_loss: 0.0225\n",
      "Epoch 65/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.7165\n",
      "Epoch 00065: val_loss did not improve from 0.02250\n",
      "6910/6910 [==============================] - 454s 66ms/step - loss: 0.7165 - val_loss: 0.0225\n",
      "Epoch 00065: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x224063cde88>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, lstm_size, maxlen, maxlen, 'concat', att_units, batch_size)\n",
    "model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "\n",
    "callbacks = [ModelCheckpoint('Attention_concat_lstm_bigram.h5', save_best_only= True, verbose = 1),\n",
    "             EarlyStopping(patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]\n",
    "\n",
    "model.fit(x = bigram_train_dataset, \n",
    "          steps_per_epoch = bigram_train.shape[0]//batch_size,\n",
    "          validation_data = bigram_val_dataset,\n",
    "          validation_steps = bigram_val.shape[0]//batch_size,\n",
    "          epochs = 100,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, lstm_size, maxlen, maxlen, 'concat', att_units, bigram_word_to_index)\n",
    "pred_model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "pred_model.build(input_shape= (None, 1, maxlen))\n",
    "pred_model.load_weights('Attention_concat_lstm_bigram.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  no Lne\n",
      "predicted output :  no one\n",
      "actual output : no one\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAJBCAYAAACtcQS7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASP0lEQVR4nO3cW6ylZ13H8d8z3Z0OrULDqeHCADZwUSUBMxpOMUSiFBK80Vi1oEB0AinR1pjWxMiVF6RUQzRSHVQSQgJBvNCkWuOhRCXtxUQIxCFCSUETyIRWDj3Qmc7sx4u9C81m6uy2a623M7/PJ9nZ2e9697v+edp5851nrVljzhkAgAvdgaUHAADYBNEDAFQQPQBABdEDAFQQPQBABdEDAFQQPQBABdEDAFTYWnoAuowxrkhyXZKrkswkx5N8YM55YtHBAEiSjDHemJ379A8necOc83/GGL+W5J455z8vO91TI3p2jTGuSfL6JM/Pnh2wOefPLjLUBWaM8Zoktyc5keTO3cPXJrlhjPGGOeedj/vLcJ4ZY7wwyfvmnL+w9CwXCvfp9RtjXJvkT5P8eXbW+uLdhy5KcmOS8zp6vLyVZIzxviQfSfKiJN9Mct+eL1bjliQfTfLSOedb55xvTfLSJB9L8geLTgard3mSn1t6iAuF+/TG3Jjk1+ecNyQ5/ZjjdyV5+TIjrY6dnh2/kuSX5pyfWHqQC9zLk7xtzrn96IE55/YY4w+TfHq5sYDzgPv0Zrwk39uJf6wHkjxzw7OsnJ2eHQeSfGbpIQp8K8mLz3L8xdn5mxvA43Gf3oyvZmcHfq+fTPKlDc+ycqJnx9Ekb1l6iAIfS/IXY4xrxxgvHmO8aIzxliQfzM7LXgCPx316M44m+aPd92AmyQ+NMX41yc1Jbl1urNXw8taOy5P88hjjp5N8Nskjj31wzvkbi0x14bkxyUjyl/ne/3uPZOcP0u8sNRQ8GWOMvz3HKef9SwFPM+7TGzDnvHmM8awk/5jkUJI7kpxMcsuc808WHW4Fxpxz6RkWN8a44/95eM45f2pjwxQYY1ya5MrsBNDdc86HFh4JnrAxxof2c96c8+3rnqWB+/Rm7d6nr8rOK0LH55wPLDzSSogeAKCC9/QAABVEDwBQQfTsMcY4svQMLaz15ljrzbDOm2OtN+NCW2fR8/0uqP/AT3PWenOs9WZY582x1ptxQa2z6AEAKqz1X28dHIfmM8Zla7v+OpzKyRzMJUuP8YS95GUPLj3CE/b1+87kec+5aOkxnpAvfvHZS4/wpJw6/WAObp1ffxbnww8vPcIT9sg8mYvH+Xf/yHn4j3gfyclcfB7eq8835+s6359v3DvnfN7e42v9cMJnjMvyykNvWudTsOvv/+GupUeo8Marf3HpEWrM43cvPUKNeebM0iN08BExG/NP8xNfOdtxL28BABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABW2znXCGOOTSY4n+WaSI0m2k3w4yY1zzu21TgcAsCL73em5NsnpJK9O8u4k1ye5Zl1DAQCs2n6j5/ic8z1zzi/MOT+e5I4krz/biWOMI2OMY2OMY6dycmWDAgA8FfuNns/u+fmrSZ5/thPnnEfnnIfnnIcP5pKnNBwAwKrsN3oe2fPzfAK/CwCwOOECAFQQPQBABdEDAFQ45+f0zDlfd5Zjb1vHMAAA62KnBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCosLXeq2/lwHOfs9anYMcbX/KapUeocOpvHlp6hBr3f+THlx6hxrM/+h9Lj1Bhnjy59Aj17PQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABVEDwBQQfQAABX2FT1jjEvGGO8fY5wYYzw8xrhrjPHadQ8HALAq+93puTnJNUnekeQVST6X5PYxxgvWNRgAwCqdM3rGGJcleVeSm+act805P5/knUlOJLluzfMBAKzEfnZ6rkxycZJPPXpgznkmyZ1Jrtp78hjjyBjj2Bjj2Knt76xsUACAp2I/0TN2v8+zPPZ9x+acR+ech+echw8eeMZTGg4AYFX2Ez13JzmV5LtvXB5jXJTkVUmOr2kuAICV2jrXCXPOB8cYtyZ57xjj3iT3JLkhyRVJPrDm+QAAVuKc0bPrpt3vH0pyeZJPJ7l6zvm1tUwFALBi+4qeOefJJNfvfgEAnHd8IjMAUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVRA8AUEH0AAAVttZ69TNnsv2tb6/1KdgxT59eeoQKh96+vfQINW7991uWHqHGb/711UuPUGGeOrX0CD3m2Q/b6QEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKogeAKCC6AEAKmyt+oJjjCNJjiTJoXHZqi8PAPCkrHynZ855dM55eM55+OA4tOrLAwA8KV7eAgAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqbK336ls58Nxnr/Up2LF94utLj1Bh3n//0iPUuP7H3rz0CDW+9aaXLj1ChfteNpYeocfv/tVZD9vpAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAqiB4AoILoAQAq7Ct6xhiXjDHeP8Y4McZ4eIxx1xjjteseDgBgVfa703NzkmuSvCPJK5J8LsntY4wXrGswAIBVOmf0jDEuS/KuJDfNOW+bc34+yTuTnEhy3VnOPzLGODbGOHZq+6GVDwwA8GTsZ6fnyiQXJ/nUowfmnGeS3Jnkqr0nzzmPzjkPzzkPHzxw6coGBQB4KvYTPWP3+zzLY2c7BgDwtLOf6Lk7yakk333j8hjjoiSvSnJ8TXMBAKzU1rlOmHM+OMa4Ncl7xxj3JrknyQ1JrkjygTXPBwCwEueMnl037X7/UJLLk3w6ydVzzq+tZSoAgBXbV/TMOU8muX73CwDgvOMTmQGACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACqIHAKggegCACltrvfpFB7L9zEvX+hTs2P7yd5YeocNDDy09QY8DFy09QY1n/d1/Lj1Chf/9kR9deoR6dnoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgAqiBwCoIHoAgApb5zphjPHJJMeTfDPJkSTbST6c5MY55/ZapwMAWJH97vRcm+R0klcneXeS65Ncs66hAABWbb/Rc3zO+Z455xfmnB9PckeS15/txDHGkTHGsTHGsVOnH1rZoAAAT8V+o+eze37+apLnn+3EOefROefhOefhg1uXPqXhAABWZb/R88ien+cT+F0AgMUJFwCggugBACqIHgCgwjk/p2fO+bqzHHvbOoYBAFgXOz0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBUED0AQAXRAwBU2Frr1U+fyYH7vr3Wp2DH9pxLjwCrNbeXnqDG9gMPLD1ChSv/7CtLj1Dji49z3E4PAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBB9AAAFUQPAFBhX9Ezdtw4xvjSGOM7Y4zPjTHesu7hAABWZWuf5/1+kp9Pcl2S/0ryqiQfHGN8Y85527qGAwBYlXNGzxjjsiS/leRn5pz/tnv4njHGT2Qngm7bc/6RJEeS5NBFP7jaaQEAnqT97PRcleRQktvHGPMxxy9O8uW9J885jyY5miTPOnjF3Ps4AMAS9hM9j77v581J/nvPY4+sdhwAgPXYT/QcT3IyyQvnnP+y5nkAANbinNEz57x/jHFLklvGGCPJvyb5gSSvTLK9+3IWAMDT2n7/9dbvJTmR5LeT3Jrk20k+k+TmNc0FALBS+4qeOedM8se7XwAA5x2fyAwAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AEAF0QMAVBA9AECFMedc38XH+HqSr6ztCdbjuUnuXXqIEtZ6c6z1ZljnzbHWm3G+rvML55zP23twrdFzPhpjHJtzHl56jgbWenOs9WZY582x1ptxoa2zl7cAgAqiBwCoIHq+39GlByhirTfHWm+Gdd4ca70ZF9Q6e08PAFDBTg8AUEH0AAAVRA8AUEH0AAAVRA8AUOH/AO97Zb9kHLWlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = bigram_train['input'].values[4]\n",
    "result, attention_plot = predict(sentence, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', bigram_train['output'].values[4])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  for themseves\n",
      "predicted output :  for themselves\n",
      "actual output : for themselves\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAJGCAYAAADyN0eUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAcPElEQVR4nO3df5Dkd13n8dd7fyRrEgKFgChCEr0LEn+UP1YkqBgVQe9KFAoLy8Ahlq56eCIcF0+rtDyOOwGj5S+kXOSEK/As5E44iKJHCYZS0VsUjEZEYhK8AzaGhCTk1252P/fHTHCZm86P7n7Pt6fzeFRN7WxPT/erpmd2n/Pt7pkaYwQAoMueqQcAAOtNbAAArcQGANBKbAAArcQGANBKbAAArcQGANBKbAAArcQGANBq39QDAHZKVT0yyROTPCJbvtkaY/zKJKPgAaD8uHLggaCqnp3k15JUkhuTnPqP3xhjfM4kw+ABQGwADwhVdW2S1yV5yRjjrqn3wAOJ2AAeEKrqxiRfMcb4+6m3wAONB4guUVV9VlW9pKreVFW/VVX/oao+a+pdsJOq6luq6m1VdWVVPXrztO+tqm+ceNobkvzLiTfsOqt6e1bVb1fVM6rqtCl3zLKqH7epiI0lqaqvTvKhJN+V5PYkdyS5OMnfVdWFU25jPVXVs6rqcFW9uar+56kvE266OMkbk/xdkvOS7N98094kl0y1a9OLknzL5sfrP1bVT576MuWwqrqgqh57yt+/qapeX1U/VlV7J9y1yrfn7Un+a5KjVfXqqnrSxHs+ZZU/blN9rq18bFTVk6pqNzxr5tIk/y3J+WOM54wxnpPk/CS/meRnJ13G/VZVj6mq2ub0qqrHTLFpy46fSfL6JOcm+USSj295mcolSb5vjPHCJKc+LuI9Sb50mkmf8v1Jvjkbz0Z5epLvOOXlmRPuSpLXJPmyJKmqz03yliQPTfL8JC+dcNfK3p5jjO/KxrOK/k2SRyV5R1VdW1U/XVVfOOW2rPDHLVN9ro0xVvolyYkkj9h8/e+TfObUm2bsvD3JY7c5/QuS3D7hrv1J/nS7bV7u8eP2qc+7Lad/ZpITK7DvaJJnTr1jm123JTln8/Vbknze5uufP+XXweaG65K8cOqP0Yxtn8jGNypJ8sIk79x8/euTXOP2vE9bH57kh5L8VZK7Jt6ysh+3qT7XVv7IRjaeonbe5uvnZnWPxtyUf9p5qvOyceNOYoxxfHODRwLfP5XtP2ZnZeMusqntSfK+qUds4yPZOKK31ZOSXLXDW7bam2Syu5juxd4kxzZf/8Ykv7P5+lVJpnzc1yrfnp9SVQeSfEOSp2Zj7z9Mu2ilP26TfK7thrsn/nuSP6yqj2bjH/8jVXViuzOOMT5vR5d9ut9M8pqquiTJH2dj69ckeVk27l6Z0uuSfF+SfzfxjpVXVb+4+epI8tNVddspb96b5PFZjf/kDyd5dpKfmnjHVoeT/GJVfe/m3x9dVV+b5BWZfuuvZ+NxVC+ZeMd2/irJD1bV27LxH8CPbZ7+qCTXT7ZqhW/PqtqT5MnZuE2/PRtHI9+U5MljjMun3JYV/rhlos+13RAbP5CN70b+eZKfy8Y/GLdMumh7l2Tju+H/kn/6uB5P8qok/36qUZvOTHJxVX1TkvcmufXUN44xfniSVavpizf/rCSPyz99B5DN1/88G4/P2XGnhFCycWTj7tv0L7PxufYpU92mY4xXVNWDk/yvJAeSvDPJnUkuHWO8copNpzgjyfdW1VOzQh+zTT+a5M1JXpzkdWOMKzZPf1qSP5tq1Irfnh9J8uAkv5vkeUneNsY4ds/vsjNW/OM2yefarvo5G1X160l+eIyxirGRJKmqM7Jxv1wl+dAY47Z7eZd2VfXOe3jzGGN8w46N2SU2P9deMMa4eeotd7uX2/FUk9+mm18HF2Qjiq4cY3xyyj3J6n8dbD4T4Owxxo2nnHZuktvGGNdNtWtzxyrenoeSvHGMMdnd1PdmFT9uyTSfa7sqNgCA3WdVH2wJAKwJsQEAtNrVsbF5n91Ksm0+tt1/q7orsW1ets1nVbet6q5k57bt6thIsrI3YGybl23336ruSmybl23zWdVtq7or2aFtuz02AIAVt+PPRjlt3xnjwOkPWcplHb/rtuzfd8ZSLitJHnze8p6VdOsNx3PmQ/ff+xnvo5uufdDSLuvY8Vtz2v4zl3JZJ/cvt1eP3/nJ7D/9rKVc1p5PLPdZx8fHHdlfB5ZzYUv8ujueO7M/py/t8pbJtvnYNp9V3baqu5Llb7slN14/xnj41tN3/Id6HTj9IXnCF6zmEaVvfv0fTT1hpsu+/6KpJ2zr9keu5hdQkpz1lr+YesJM467j934m/n+eqg8r7R3jTddud7q7UQCAVmIDAGglNgCAVmIDAGglNgCAVmIDAGglNgCAVmIDAGglNgCAVmIDAGglNgCAVmIDAGi1lNioqj1V9atV9fGqGlV10TIuFwDY/Zb1W1//RZLnJbkoyd8nuWFJlwsA7HLLio1/luSjY4w/XtLlAQBrYuHYqKrXJnnu5usjybVjjHMXvVwAYD0s48jGC5Jcm+R7knxlkhNLuEwAYE0sHBtjjJuq6pYkJ8YYH9vuPFV1KMmhJDlw2oMXvUoAYBfZkae+jjEOjzEOjjEO7t93xk5cJQCwIvycDQCgldgAAFqJDQCgldgAAFotJTbGGJf62RoAwHYc2QAAWokNAKCV2AAAWokNAKCV2AAAWokNAKCV2AAAWokNAKCV2AAAWokNAKCV2AAAWokNAKDVvp2+whpJnTy501d7n/z+V58z9YSZvuNPfn/qCdt69c9829QTZjr7vEdPPWGmk1f/w9QTZhrHj009AVgzjmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3mjo2qOr2qfr6qjlbVHVX1nqr6mmWOAwB2v0WObLwiybOSfE+SL0tyRZK3V9VnL2MYALAe5oqNqjozyQ8m+dExxmVjjL9J8gNJjiZ5/jbnP1RVR6rqyLG7bltoMACwu8x7ZOPzk+xP8kd3nzDGOJHkT5JcsPXMY4zDY4yDY4yDp+07Y86rBAB2o3ljozb/HNu8bbvTAIAHqHlj40NJjiX51ANCq2pvkguTXLmEXQDAmtg3zzuNMW6tqlcleVlVXZ/k6iQvTPJZSX5lifsAgF1urtjY9KObf/56kock+Ysk3zzG+OjCqwCAtTF3bIwx7kzyI5svAADb8hNEAYBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaLXIb32dz10nsuf6m3b8au+LE7ffMfWEmd78tY+besK23vP+V049YaanPOf7pp4w0+nXXT/1hJlOfOLY1BNmq5p6we40xtQLeIBzZAMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaLX02Kiq05Z9mQDA7rVv0Quoqncl+ZsktyZ5bpJrknzlopcLAKyHZR3ZeHaSSvK1Sf7Vki4TAFgDCx/Z2HT1GOPfLumyAIA1sqzYeO89vbGqDiU5lCQH9j5oSVcJAOwGy7ob5dZ7euMY4/AY4+AY4+Bpez5jSVcJAOwGnvoKALQSGwBAK7EBALRa+AGiY4yLlrADAFhTjmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQauFfxHZ/jdP25a5HP2ynr/Y+qeuun3rCTCdvumXqCdt62tc8feoJM13423829YSZ/uIZnz/1hNlu/uTUC2YbJ6deMNsYUy+AleXIBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK12JDaq6lBVHamqI8eP37oTVwkArIgdiY0xxuExxsExxsH9+8/ciasEAFaEu1EAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBotW+nr7BuvzN7/vrqnb7a++Tk8WNTT9h17rrmw1NPmOl/f+neqSfM9HsfefPUE2b6sv/0r6eeMNNn/+YHpp4w04kbbpx6wmy1ot9Xnjwx9QJ2yIp+BgIA60JsAACtxAYA0EpsAACtxAYA0EpsAACtxAYA0EpsAACtxAYA0EpsAACtxAYA0EpsAACtxAYA0EpsAACtxAYA0GopsVFV76qqX17GZQEA68WRDQCg1cKxUVWvTfJ1SZ5fVWPz5dxFLxcAWA/7lnAZL0hyfpIPJPnxzdP+cQmXCwCsgYVjY4xxU1UdS3LbGONj252nqg4lOZQkB+rMRa8SANhFduQxG2OMw2OMg2OMg6fVgZ24SgBgRXiAKADQalmxcSzJ3iVdFgCwRpYVG9ckeXxVnVtVD6sqR0wAgCTLi41Ls3F048psPBPlMUu6XABgl1vGU18zxvhgkguXcVkAwHpxdwcA0EpsAACtxAYA0EpsAACtxAYA0EpsAACtxAYA0EpsAACtxAYA0EpsAACtxAYA0EpsAACtlvKL2O6XPZU6bf+OX+19UjX1gtnGmHrB9lZ1V7LSt+e3fN4Tpp4w077vWt3b9IM/fv7UE2Y688Or+73bo3736NQTtlU3f3LqCTOduP7jU0+YaZw4MfWE2Wb887G6Xx0AwFoQGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAq7lio6reVVW/vOwxAMD6cWQDAGglNgCAVovExp6q+s9VdX1VXVdVl1aVeAEAPs0icXBxkruSPDHJDyX5kSTPWsYoAGB9LBIbV44xfnKM8cExxhuTvDPJN253xqo6VFVHqurIsZN3LHCVAMBus0hs/OWWv38kySO2O+MY4/AY4+AY4+Bpew4scJUAwG6zSGwc3/L3seDlAQBrSBwAAK3EBgDQSmwAAK32zfNOY4yLtjntuxcdAwCsH0c2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaDXXL2JbWNUkV3uvxph6Acu0wrfnOHFy6gkzfeYb/nzqCTPd/iNfMfWEmW7+wuNTT5jpEe990NQTtrX/uuunnjBbrfD34rW6/7bNssIfTQBgHYgNAKCV2AAAWokNAKCV2AAAWokNAKCV2AAAWokNAKCV2AAAWokNAKCV2AAAWokNAKCV2AAAWokNAKCV2AAAWokNAKCV2AAAWs0dG7Xhkqq6qqpur6orqurZyxwHAOx++xZ435cmeWaS5yf52yQXJnl1Vd04xrhsGeMAgN1vrtioqjOTvCjJU8YY7948+eqqenw24uOyLec/lORQkhzYc9b8awGAXWfeIxsXJDmQ5O1VNU45fX+Sa7aeeYxxOMnhJHnw/oePrW8HANbXvLFx92M9vjXJh7e87fj8cwCAdTNvbFyZ5M4k54wx/mCJewCANTNXbIwxbqmqS5NcWlWV5PIkZyV5QpKTm3ebAAAs9GyUn0hyNMmLk7wqyc1J3pfkFUvYBQCsibljY4wxkvzS5gsAwLb8BFEAoJXYAABaiQ0AoJXYAABaiQ0AoJXYAABaiQ0AoJXYAABaiQ0AoJXYAABaiQ0AoJXYAABaLfJbX4E5jbuOTz1htjGmXjDT5/7sn009YaYPve6Lp54w0x0PP33qCdu6/clfMPWEmc6+4uNTT5jtEzdPvWC2j21/siMbAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtBIbAECre42NqnpXVb2qqn62qm6oqn+sqhdU1elV9cqq+kRVfbiqnrMTgwGA3eW+Htm4OMktSb4qycuS/HySNyf5YJKDSV6X5Neq6nM6RgIAu9d9jY2/HmP81Bjj75L8XJLrkxwfY/zCGONDSV6SpJI8cbt3rqpDVXWkqo4cO3nHUoYDALvDfY2Nv7z7lTHGSHJdkitOOe14khuTPGK7dx5jHB5jHBxjHDxtz4EF5gIAu819jY3jW/4+ZpzmAacAwKcRBwBAK7EBALQSGwBAq333doYxxkXbnPZF25z2yCVtAgDWiCMbAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtLrXX8S2bOPEyZy85ZM7fbWwWmqFO7+mHnAP9u6desFM5//QNVNPmO1/fMbUC7b1sTeeM/WEmc560IGpJ8y09+bd93/oCv+LBwCsA7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALRaKDaq6klV9Z6q+mRV3VRVf1pVX7SscQDA7rdv3nesqn1J3pLkNUkuTrI/yZcnObGcaQDAOpg7NpKcneQhSd46xrhq87QPLD4JAFgnc9+NMsa4Iclrk/xeVV1WVS+qqkdvd96qOlRVR6rqyPFxx7xXCQDsQgs9ZmOM8bwkX5Xk8iRPS/LBqnrqNuc7PMY4OMY4uL8OLHKVAMAus/CzUcYY7x9jvHyMcVGSdyV57qKXCQCsj7ljo6rOq6qXVdUTq+qcqvr6JF+S5MrlzQMAdrtFHiB6W5Lzk/xWkoclOZrkDUlevoRdAMCamDs2xhhHkzxjiVsAgDXkJ4gCAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK3EBgDQSmwAAK0W+RXz8xkj49ixHb9aWCknT0y9YLaqqRfMNO68c+oJs5115tQLZnv6rVMv2Naz3v2OqSfM9PYXf93UE2Y64+jO/9e9KEc2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaDV3bNSGS6rqqqq6vaquqKpnL3McALD77VvgfV+a5JlJnp/kb5NcmOTVVXXjGOOyZYwDAHa/uWKjqs5M8qIkTxljvHvz5Kur6vHZiA+xAQAkmf/IxgVJDiR5e1WNU07fn+SarWeuqkNJDiXJgZwx51UCALvRvLFx92M9vjXJh7e87fjWM48xDic5nCRn10PH1rcDAOtr3ti4MsmdSc4ZY/zBEvcAAGtmrtgYY9xSVZcmubSqKsnlSc5K8oQkJzePZAAALPRslJ9IcjTJi5O8KsnNSd6X5BVL2AUArIm5Y2OMMZL80uYLAMC2/ARRAKCV2AAAWokNAKCV2AAAWokNAKCV2AAAWokNAKCV2AAAWokNAKCV2AAAWokNAKCV2AAAWi3yW1/nN8YkVwvcB74+53L8cY+ZesJMp/2fG6aesK0//M5HTj1hpmtevLpfB4/7mTOmnnC/ObIBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAq6XFRlW9tqretqzLAwDWgyMbAEArsQEAtBIbAEArsQEAtNq3E1dSVYeSHEqSAzljJ64SAFgRO3JkY4xxeIxxcIxxcH9O34mrBABWhLtRAIBWYgMAaCU2AIBWYgMAaLW0Z6OMMb57WZcFAKwPRzYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBotbRfxAbwQLbnj94/9YSZ7pp6wAx7Tj996gkz7f34l049Yabbf+HOqSfM9uTtT3ZkAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoJTYAgFZiAwBoNVdsVNX3V9XRqtq35fTfqKq3LGcaALAO5j2y8cYkD0ny5LtPqKozk3xbktcvYRcAsCbmio0xxo1JfifJxaec/PQkdyV569bzV9WhqjpSVUeO5865hgIAu9Mij9l4fZJvr6ozNv9+cZI3jTHu2HrGMcbhMcbBMcbB/Tl9gasEAHabRWLjbdk4kvFtVfWIbNyl4i4UAODT7Lv3s2xvjHFnVb0pG0c0HpbkY0n+cFnDAID1MHdsbHp9knckOS/Jb4wxTi4+CQBYJ4vGxuVJ/m+SC5J85+JzAIB1s1BsjDFGknOXMwUAWEd+gigA0EpsAACtxAYA0EpsAACtxAYA0EpsAACtxAYA0EpsAACtxAYA0EpsAACtxAYA0EpsAACtFv2trwCsuL0PetDUE7Z18rHnTD1hpse+8qNTT5jp2mc9auoJ95sjGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAK7EBALQSGwBAq7ljozZcUlVXVdXtVXVFVT17meMAgN1v3wLv+9Ikz0zy/CR/m+TCJK+uqhvHGJctYxwAsPvNFRtVdWaSFyV5yhjj3ZsnX11Vj89GfFy25fyHkhxKkgM5Y/61AMCuM++RjQuSHEjy9qoap5y+P8k1W888xjic5HCSnF0PHVvfDgCsr3lj4+7Henxrkg9vedvx+ecAAOtm3ti4MsmdSc4ZY/zBEvcAAGtmrtgYY9xSVZcmubSqKsnlSc5K8oQkJzfvNgEAWOjZKD+R5GiSFyd5VZKbk7wvySuWsAsAWBNzx8YYYyT5pc0XAIBt+QmiAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtBIbAEArsQEAtFrkt74CcLcxpl4w08nbbpt6wrbGe6+cesJMJ/av7n+Pj3nrgaknzPQ3M053ZAMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaCU2AIBWYgMAaLVQbFTVk6rqPVX1yaq6qar+tKq+aFnjAIDdb9+871hV+5K8JclrklycZH+SL09yYjnTAIB1MHdsJDk7yUOSvHWMcdXmaR/Y7oxVdSjJoSQ5kDMWuEoAYLeZ+26UMcYNSV6b5Peq6rKqelFVPXrGeQ+PMQ6OMQ7uz+nzXiUAsAst9JiNMcbzknxVksuTPC3JB6vqqcsYBgCsh4WfjTLGeP8Y4+VjjIuSvCvJcxe9TABgfcwdG1V1XlW9rKqeWFXnVNXXJ/mSJFcubx4AsNst8gDR25Kcn+S3kjwsydEkb0jy8iXsAgDWxNyxMcY4muQZS9wCAKwhP0EUAGglNgCAVmIDAGglNgCAVmIDAGglNgCAVmIDAGglNgCAVmIDAGglNgCAVmIDAGglNgCAVjXG2NkrrPrHJNcu6eIeluT6JV3Wstk2H9vuv1Xdldg2L9vms6rbVnVXsvxt54wxHr71xB2PjWWqqiNjjINT79iObfOx7f5b1V2JbfOybT6rum1VdyU7t83dKABAK7EBALTa7bFxeOoB98C2+dh2/63qrsS2edk2n1Xdtqq7kh3atqsfswEArL7dfmQDAFhxYgMAaCU2AIBWYgMAaCU2AIBW/w8//Q/fklCQTAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = bigram_train['input'].values[6]\n",
    "result, attention_plot = predict(sentence, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', bigram_train['output'].values[6])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  detil of\n",
      "predicted output :  detail of\n",
      "actual output : detail of\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = bigram_train['input'].values[7]\n",
    "result, attention_plot = predict(sentence, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', bigram_train['output'].values[7])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bleu = 0\n",
    "for i in tqdm(range(bigram_val.shape[0])):\n",
    "    inp = bigram_val['input'].values[i]\n",
    "    out = bigram_val['output'].values[i]\n",
    "    pred, _ = predict(inp, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "    val_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(bigram_train.shape[0])):\n",
    "    inp = bigram_train['input'].values[i];\n",
    "    out = bigram_train['output'].values[i]\n",
    "    pred, _ = predict(inp, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "    train_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(bigram_test.shape[0])):\n",
    "    inp = bigram_test['input'].values[i]\n",
    "    out = bigram_test['output'].values[i]\n",
    "    pred, _ = predict(inp, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "    test_bleu += sentence_bleu([out], pred)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/bigram_train.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/bigram_val.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/bigram_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.3 TriGram </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "embedding_dim = 100\n",
    "att_units = 256\n",
    "maxlen = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.9333\n",
      "Epoch 00001: val_loss improved from inf to 0.05902, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 619s 90ms/step - loss: 0.9333 - val_loss: 0.0590\n",
      "Epoch 2/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8491\n",
      "Epoch 00002: val_loss improved from 0.05902 to 0.04622, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 622s 90ms/step - loss: 0.8491 - val_loss: 0.0462\n",
      "Epoch 3/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8422\n",
      "Epoch 00003: val_loss improved from 0.04622 to 0.04152, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 625s 90ms/step - loss: 0.8422 - val_loss: 0.0415\n",
      "Epoch 4/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8382\n",
      "Epoch 00004: val_loss improved from 0.04152 to 0.03638, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 632s 91ms/step - loss: 0.8382 - val_loss: 0.0364\n",
      "Epoch 5/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8360\n",
      "Epoch 00005: val_loss improved from 0.03638 to 0.03372, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 629s 91ms/step - loss: 0.8360 - val_loss: 0.0337\n",
      "Epoch 6/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8345\n",
      "Epoch 00006: val_loss improved from 0.03372 to 0.03284, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 626s 91ms/step - loss: 0.8345 - val_loss: 0.0328\n",
      "Epoch 7/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8329\n",
      "Epoch 00007: val_loss improved from 0.03284 to 0.03111, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 626s 91ms/step - loss: 0.8329 - val_loss: 0.0311\n",
      "Epoch 8/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8318\n",
      "Epoch 00008: val_loss improved from 0.03111 to 0.03011, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 627s 91ms/step - loss: 0.8318 - val_loss: 0.0301\n",
      "Epoch 9/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8309\n",
      "Epoch 00009: val_loss improved from 0.03011 to 0.02913, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 626s 91ms/step - loss: 0.8309 - val_loss: 0.0291\n",
      "Epoch 10/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8303\n",
      "Epoch 00010: val_loss improved from 0.02913 to 0.02846, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 626s 91ms/step - loss: 0.8303 - val_loss: 0.0285\n",
      "Epoch 11/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8299\n",
      "Epoch 00011: val_loss improved from 0.02846 to 0.02764, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 625s 90ms/step - loss: 0.8299 - val_loss: 0.0276\n",
      "Epoch 12/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8291\n",
      "Epoch 00012: val_loss improved from 0.02764 to 0.02715, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 635s 92ms/step - loss: 0.8291 - val_loss: 0.0272\n",
      "Epoch 13/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8290\n",
      "Epoch 00013: val_loss improved from 0.02715 to 0.02653, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 628s 91ms/step - loss: 0.8290 - val_loss: 0.0265\n",
      "Epoch 14/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8283\n",
      "Epoch 00014: val_loss improved from 0.02653 to 0.02601, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 630s 91ms/step - loss: 0.8283 - val_loss: 0.0260\n",
      "Epoch 15/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8282\n",
      "Epoch 00015: val_loss improved from 0.02601 to 0.02585, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 631s 91ms/step - loss: 0.8282 - val_loss: 0.0258\n",
      "Epoch 16/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8276\n",
      "Epoch 00016: val_loss improved from 0.02585 to 0.02510, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 632s 91ms/step - loss: 0.8276 - val_loss: 0.0251\n",
      "Epoch 17/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8272\n",
      "Epoch 00017: val_loss improved from 0.02510 to 0.02482, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 632s 91ms/step - loss: 0.8272 - val_loss: 0.0248\n",
      "Epoch 18/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8267\n",
      "Epoch 00018: val_loss improved from 0.02482 to 0.02455, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 631s 91ms/step - loss: 0.8267 - val_loss: 0.0245\n",
      "Epoch 19/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8264\n",
      "Epoch 00019: val_loss improved from 0.02455 to 0.02399, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 635s 92ms/step - loss: 0.8264 - val_loss: 0.0240\n",
      "Epoch 20/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8261\n",
      "Epoch 00020: val_loss did not improve from 0.02399\n",
      "6910/6910 [==============================] - 634s 92ms/step - loss: 0.8261 - val_loss: 0.0240\n",
      "Epoch 21/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8261\n",
      "Epoch 00021: val_loss did not improve from 0.02399\n",
      "6910/6910 [==============================] - 640s 93ms/step - loss: 0.8261 - val_loss: 0.0243\n",
      "Epoch 22/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8257\n",
      "Epoch 00022: val_loss improved from 0.02399 to 0.02359, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 632s 92ms/step - loss: 0.8257 - val_loss: 0.0236\n",
      "Epoch 23/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8259\n",
      "Epoch 00023: val_loss improved from 0.02359 to 0.02338, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 633s 92ms/step - loss: 0.8259 - val_loss: 0.0234\n",
      "Epoch 24/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8253\n",
      "Epoch 00024: val_loss did not improve from 0.02338\n",
      "6910/6910 [==============================] - 631s 91ms/step - loss: 0.8253 - val_loss: 0.0234\n",
      "Epoch 25/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8251\n",
      "Epoch 00025: val_loss improved from 0.02338 to 0.02304, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 647s 94ms/step - loss: 0.8251 - val_loss: 0.0230\n",
      "Epoch 26/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8251\n",
      "Epoch 00026: val_loss improved from 0.02304 to 0.02287, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 657s 95ms/step - loss: 0.8251 - val_loss: 0.0229\n",
      "Epoch 27/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8246\n",
      "Epoch 00027: val_loss improved from 0.02287 to 0.02246, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 701s 101ms/step - loss: 0.8246 - val_loss: 0.0225\n",
      "Epoch 28/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8247\n",
      "Epoch 00028: val_loss improved from 0.02246 to 0.02219, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 700s 101ms/step - loss: 0.8247 - val_loss: 0.0222\n",
      "Epoch 29/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8247\n",
      "Epoch 00029: val_loss did not improve from 0.02219\n",
      "6910/6910 [==============================] - 657s 95ms/step - loss: 0.8247 - val_loss: 0.0226\n",
      "Epoch 30/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8284\n",
      "Epoch 00030: val_loss did not improve from 0.02219\n",
      "6910/6910 [==============================] - 648s 94ms/step - loss: 0.8284 - val_loss: 0.0229\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8247\n",
      "Epoch 00031: val_loss did not improve from 0.02219\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "6910/6910 [==============================] - 645s 93ms/step - loss: 0.8247 - val_loss: 0.0227\n",
      "Epoch 32/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8227\n",
      "Epoch 00032: val_loss improved from 0.02219 to 0.02104, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 660s 96ms/step - loss: 0.8227 - val_loss: 0.0210\n",
      "Epoch 33/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8221\n",
      "Epoch 00033: val_loss improved from 0.02104 to 0.02061, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 679s 98ms/step - loss: 0.8221 - val_loss: 0.0206\n",
      "Epoch 34/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8214\n",
      "Epoch 00034: val_loss improved from 0.02061 to 0.02026, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 656s 95ms/step - loss: 0.8214 - val_loss: 0.0203\n",
      "Epoch 35/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8212\n",
      "Epoch 00035: val_loss did not improve from 0.02026\n",
      "6910/6910 [==============================] - 658s 95ms/step - loss: 0.8212 - val_loss: 0.0203\n",
      "Epoch 36/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8211\n",
      "Epoch 00036: val_loss improved from 0.02026 to 0.02003, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 648s 94ms/step - loss: 0.8211 - val_loss: 0.0200\n",
      "Epoch 37/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8209\n",
      "Epoch 00037: val_loss improved from 0.02003 to 0.01989, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 656s 95ms/step - loss: 0.8209 - val_loss: 0.0199\n",
      "Epoch 38/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8207\n",
      "Epoch 00038: val_loss improved from 0.01989 to 0.01973, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 651s 94ms/step - loss: 0.8207 - val_loss: 0.0197\n",
      "Epoch 39/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8205\n",
      "Epoch 00039: val_loss did not improve from 0.01973\n",
      "6910/6910 [==============================] - 641s 93ms/step - loss: 0.8205 - val_loss: 0.0197\n",
      "Epoch 40/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8203\n",
      "Epoch 00040: val_loss improved from 0.01973 to 0.01965, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 694s 100ms/step - loss: 0.8203 - val_loss: 0.0196\n",
      "Epoch 41/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8205\n",
      "Epoch 00041: val_loss improved from 0.01965 to 0.01948, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 649s 94ms/step - loss: 0.8205 - val_loss: 0.0195\n",
      "Epoch 42/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8204\n",
      "Epoch 00042: val_loss improved from 0.01948 to 0.01943, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 653s 94ms/step - loss: 0.8204 - val_loss: 0.0194\n",
      "Epoch 43/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8200\n",
      "Epoch 00043: val_loss improved from 0.01943 to 0.01934, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 671s 97ms/step - loss: 0.8200 - val_loss: 0.0193\n",
      "Epoch 44/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8200\n",
      "Epoch 00044: val_loss did not improve from 0.01934\n",
      "6910/6910 [==============================] - 679s 98ms/step - loss: 0.8200 - val_loss: 0.0194\n",
      "Epoch 45/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8203\n",
      "Epoch 00045: val_loss improved from 0.01934 to 0.01928, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 658s 95ms/step - loss: 0.8203 - val_loss: 0.0193\n",
      "Epoch 46/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8198- ETA:\n",
      "Epoch 00046: val_loss improved from 0.01928 to 0.01921, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 653s 95ms/step - loss: 0.8198 - val_loss: 0.0192\n",
      "Epoch 47/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8194\n",
      "Epoch 00047: val_loss improved from 0.01921 to 0.01915, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 650s 94ms/step - loss: 0.8194 - val_loss: 0.0191\n",
      "Epoch 48/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8199\n",
      "Epoch 00048: val_loss did not improve from 0.01915\n",
      "6910/6910 [==============================] - 661s 96ms/step - loss: 0.8199 - val_loss: 0.0192\n",
      "Epoch 49/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8197\n",
      "Epoch 00049: val_loss improved from 0.01915 to 0.01910, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 659s 95ms/step - loss: 0.8197 - val_loss: 0.0191\n",
      "Epoch 50/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8198\n",
      "Epoch 00050: val_loss improved from 0.01910 to 0.01904, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 642s 93ms/step - loss: 0.8198 - val_loss: 0.0190\n",
      "Epoch 51/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8194\n",
      "Epoch 00051: val_loss improved from 0.01904 to 0.01896, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 646s 93ms/step - loss: 0.8194 - val_loss: 0.0190\n",
      "Epoch 52/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8194\n",
      "Epoch 00052: val_loss improved from 0.01896 to 0.01895, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 652s 94ms/step - loss: 0.8194 - val_loss: 0.0190\n",
      "Epoch 53/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8198\n",
      "Epoch 00053: val_loss improved from 0.01895 to 0.01893, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 670s 97ms/step - loss: 0.8198 - val_loss: 0.0189\n",
      "Epoch 54/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8194\n",
      "Epoch 00054: val_loss improved from 0.01893 to 0.01883, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 657s 95ms/step - loss: 0.8194 - val_loss: 0.0188\n",
      "Epoch 55/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8196\n",
      "Epoch 00055: val_loss improved from 0.01883 to 0.01872, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 654s 95ms/step - loss: 0.8196 - val_loss: 0.0187\n",
      "Epoch 56/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8190\n",
      "Epoch 00056: val_loss did not improve from 0.01872\n",
      "6910/6910 [==============================] - 651s 94ms/step - loss: 0.8190 - val_loss: 0.0188\n",
      "Epoch 57/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8196\n",
      "Epoch 00057: val_loss did not improve from 0.01872\n",
      "6910/6910 [==============================] - 652s 94ms/step - loss: 0.8196 - val_loss: 0.0188\n",
      "Epoch 58/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8190\n",
      "Epoch 00058: val_loss did not improve from 0.01872\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "6910/6910 [==============================] - 658s 95ms/step - loss: 0.8190 - val_loss: 0.0187\n",
      "Epoch 59/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8188\n",
      "Epoch 00059: val_loss improved from 0.01872 to 0.01862, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 660s 95ms/step - loss: 0.8188 - val_loss: 0.0186\n",
      "Epoch 60/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8192\n",
      "Epoch 00060: val_loss improved from 0.01862 to 0.01860, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 672s 97ms/step - loss: 0.8192 - val_loss: 0.0186\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8188\n",
      "Epoch 00061: val_loss improved from 0.01860 to 0.01858, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 673s 97ms/step - loss: 0.8188 - val_loss: 0.0186\n",
      "Epoch 62/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8192\n",
      "Epoch 00062: val_loss improved from 0.01858 to 0.01857, saving model to Attention_concat_lstm_trigram.h5\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "6910/6910 [==============================] - 684s 99ms/step - loss: 0.8192 - val_loss: 0.0186\n",
      "Epoch 63/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8191\n",
      "Epoch 00063: val_loss improved from 0.01857 to 0.01856, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 650s 94ms/step - loss: 0.8191 - val_loss: 0.0186\n",
      "Epoch 64/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8191\n",
      "Epoch 00064: val_loss improved from 0.01856 to 0.01856, saving model to Attention_concat_lstm_trigram.h5\n",
      "6910/6910 [==============================] - 640s 93ms/step - loss: 0.8191 - val_loss: 0.0186\n",
      "Epoch 65/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8192\n",
      "Epoch 00065: val_loss improved from 0.01856 to 0.01854, saving model to Attention_concat_lstm_trigram.h5\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "6910/6910 [==============================] - 638s 92ms/step - loss: 0.8192 - val_loss: 0.0185\n",
      "Epoch 66/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8191\n",
      "Epoch 00066: val_loss did not improve from 0.01854\n",
      "6910/6910 [==============================] - 2044s 296ms/step - loss: 0.8191 - val_loss: 0.0185\n",
      "Epoch 67/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8187\n",
      "Epoch 00067: val_loss did not improve from 0.01854\n",
      "6910/6910 [==============================] - 642s 93ms/step - loss: 0.8187 - val_loss: 0.0185\n",
      "Epoch 68/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8187\n",
      "Epoch 00068: val_loss did not improve from 0.01854\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "6910/6910 [==============================] - 672s 97ms/step - loss: 0.8187 - val_loss: 0.0185\n",
      "Epoch 69/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8188\n",
      "Epoch 00069: val_loss did not improve from 0.01854\n",
      "6910/6910 [==============================] - 689s 100ms/step - loss: 0.8188 - val_loss: 0.0185\n",
      "Epoch 70/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.8191\n",
      "Epoch 00070: val_loss did not improve from 0.01854\n",
      "6910/6910 [==============================] - 652s 94ms/step - loss: 0.8191 - val_loss: 0.0185\n",
      "Epoch 00070: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2269658dcc8>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, lstm_size, maxlen, maxlen, 'concat', att_units, batch_size)\n",
    "model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "\n",
    "callbacks = [ModelCheckpoint('Attention_concat_lstm_trigram.h5', save_best_only= True, verbose = 1),\n",
    "             EarlyStopping(patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]\n",
    "\n",
    "model.fit(x = trigram_train_dataset, \n",
    "          steps_per_epoch = trigram_train.shape[0]//batch_size,\n",
    "          validation_data = trigram_val_dataset,\n",
    "          validation_steps = trigram_val.shape[0]//batch_size,\n",
    "          epochs = 100,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, lstm_size, maxlen, maxlen, 'concat', att_units, trigram_word_to_index)\n",
    "pred_model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "pred_model.build(input_shape= (None, 1, maxlen))\n",
    "pred_model.load_weights('Attention_concat_lstm_trigram.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  woman oAf him\n",
      "predicted output :  woman of him\n",
      "actual output : woman of him\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = trigram_train['input'].values[4]\n",
    "result, attention_plot = predict(sentence, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', trigram_train['output'].values[4])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  endurXe a collision\n",
      "predicted output :  endure a collision\n",
      "actual output : endure a collision\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = trigram_train['input'].values[7]\n",
    "result, attention_plot = predict(sentence, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', trigram_train['output'].values[7])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  marshals sitti g on\n",
      "predicted output :  marshals sitting on\n",
      "actual output : marshals sitting on\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = trigram_train['input'].values[10]\n",
    "result, attention_plot = predict(sentence, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', trigram_train['output'].values[10])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bleu = 0\n",
    "for i in tqdm(range(trigram_val.shape[0])):\n",
    "    inp = trigram_val['input'].values[i]\n",
    "    out = trigram_val['output'].values[i]\n",
    "    pred, _ = predict(inp, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "    val_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(trigram_train.shape[0])):\n",
    "    inp = trigram_train['input'].values[i];\n",
    "    out = trigram_train['output'].values[i]\n",
    "    pred, _ = predict(inp, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "    train_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(trigram_test.shape[0])):\n",
    "    inp = trigram_test['input'].values[i]\n",
    "    out = trigram_test['output'].values[i]\n",
    "    pred, _ = predict(inp, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "    test_bleu += sentence_bleu([out], pred)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/trigram_train.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/trigram_val.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/trigram_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Seq2Seq Bi-LSTM with Attention Mechanism</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_size, input_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.enc_embed = Embedding(input_dim = vocab_size, output_dim = embedding_size)\n",
    "        self.enc_lstm = Bidirectional(LSTM(lstm_size, return_sequences = True, return_state = True, dropout = 0.4))\n",
    "    \n",
    "    def call(self, input_sequence, states):\n",
    "        embedding = self.enc_embed(input_sequence)\n",
    "        output_state, enc_frwd_h, enc_frwd_c, enc_bkwd_h, enc_bkwd_c = self.enc_lstm(embedding, initial_state = states)\n",
    "        return output_state, enc_frwd_h, enc_frwd_c, enc_bkwd_h, enc_bkwd_c\n",
    "    \n",
    "    def initialize_states(self, batch_size):\n",
    "        return [tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size)),\n",
    "                tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))]\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,scoring_function, att_units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scoring_function = scoring_function\n",
    "        if scoring_function == 'dot':\n",
    "            self.dot = Dot(axes = (1, 2))\n",
    "        elif scoring_function == 'general':\n",
    "            self.W = Dense(att_units)\n",
    "            self.dot = Dot(axes = (1, 2))\n",
    "        elif scoring_function == 'concat':\n",
    "            self.W1 = Dense(att_units)\n",
    "            self.W2 = Dense(att_units)\n",
    "            self.W3 = Dense(att_units)\n",
    "            self.V = Dense(1)\n",
    "            \n",
    "    def call(self, dec_frwd_state, dec_bkwd_state, encoder_output):\n",
    "        dec_frwd_state = tf.expand_dims(dec_frwd_state, 1) \n",
    "        dec_bkwd_state = tf.expand_dims(dec_bkwd_state, 1)\n",
    "#         \n",
    "        if self.scoring_function == 'dot':\n",
    "            score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), encoder_output]), (0, 2,1))           \n",
    "        elif self.scoring_function == 'general':\n",
    "            mul = self.W(encoder_output)\n",
    "            score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), mul]), (0, 2,1))           \n",
    "        elif self.scoring_function == 'concat':\n",
    "            inter = self.W1(dec_frwd_state) + self.W2(dec_bkwd_state) + self.W3(encoder_output)\n",
    "            tan = tf.nn.tanh(inter)\n",
    "            score = self.V(tan)\n",
    "        attention_weights = tf.nn.softmax(score, axis =1)\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class OneStepDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "        super(OneStepDecoder, self).__init__()\n",
    "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "        self.embed_dec = Embedding(input_dim = vocab_size, output_dim = embedding_dim)\n",
    "        self.lstm = Bidirectional(LSTM(dec_units, return_sequences = True, return_state = True, dropout = 0.4))\n",
    "        self.attention = Attention(scoring_function = score_fun, att_units = att_units)\n",
    "        self.fc = Dense(vocab_size)\n",
    "    \n",
    "    def call(self,input_to_decoder, encoder_output, state_frwd_h, state_frwd_c, state_bkwd_h, state_bkwd_c):\n",
    "        embed = self.embed_dec(input_to_decoder)\n",
    "        context_vect, attention_weights = self.attention(state_frwd_h, state_bkwd_h, encoder_output)    \n",
    "        final_inp = tf.concat([tf.expand_dims(context_vect, 1), embed], axis = -1)\n",
    "        out, dec_frwd_h, dec_frwd_c, dec_bkwd_h, dec_bkwd_c = self.lstm(final_inp, [state_frwd_h, state_frwd_c, state_bkwd_h, state_bkwd_c])\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = Dropout(0.5)(out)\n",
    "        output = self.fc(out)\n",
    "        return output, dec_frwd_h, dec_frwd_c, dec_bkwd_h, dec_bkwd_c, attention_weights, context_vect\n",
    "\n",
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, dec_units, max_len, score_fun, att_units, batch_size):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super(encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, enc_units, max_len)\n",
    "        self.one_step_decoder = OneStepDecoder(vocab_size, embedding_dim, max_len, dec_units ,score_fun ,att_units)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def call(self, data):\n",
    "        enc_inp, dec_inp = data[0], data[1]\n",
    "        initial_state = self.encoder.initialize_states(self.batch_size)\n",
    "        enc_output, enc_frwd_h, enc_frwd_c, enc_bkwd_h, enc_bkwd_c = self.encoder(enc_inp, initial_state)\n",
    "        all_outputs = tf.TensorArray(dtype = tf.float32, size= max_len)\n",
    "        \n",
    "        dec_frwd_h = enc_frwd_h\n",
    "        dec_frwd_c = enc_frwd_c\n",
    "        dec_bkwd_h = enc_bkwd_h\n",
    "        dec_bkwd_c = enc_bkwd_c\n",
    "        for timestep in range(max_len):\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            output, dec_frwd_h, dec_frwd_c, dec_bkwd_h, dec_bkwd_c, _, _ = self.one_step_decoder(dec_inp[:, timestep:timestep+1], enc_output, dec_frwd_h, dec_frwd_c, dec_bkwd_h, dec_bkwd_c)\n",
    "            # Store the output in tensorarray\n",
    "            all_outputs = all_outputs.write(timestep, output)\n",
    "        # Return the tensor array\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0, 2))\n",
    "        # return the decoder output\n",
    "        return all_outputs\n",
    "    \n",
    "class pred_Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_dim, enc_units, dec_units, max_len_ita, max_len_eng, score_fun, att_units, word_to_index):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super(pred_Encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_dim, enc_units, max_len_ita)\n",
    "        self.one_step_decoder = OneStepDecoder(out_vocab_size, embedding_dim, max_len_eng, dec_units, score_fun, att_units)\n",
    "        self.word_to_index = word_to_index\n",
    "        \n",
    "    def call(self, params):\n",
    "        enc_inp = params[0]\n",
    "        initial_state = self.encoder.initialize_states(1)\n",
    "        enc_output, enc_frwd_h, enc_frwd_c, enc_bkwd_h, enc_bkwd_c = self.encoder(enc_inp, initial_state)\n",
    "        pred = tf.expand_dims([self.word_to_index['<SOW>']], 0)\n",
    "        all_pred = []\n",
    "        all_attention = []\n",
    "\n",
    "        dec_frwd_h = enc_frwd_h\n",
    "        dec_frwd_c = enc_frwd_c\n",
    "        dec_bkwd_h = enc_bkwd_h\n",
    "        dec_bkwd_c = enc_bkwd_c\n",
    "        for timestep in range(max_len):\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            output, dec_frwd_h, dec_frwd_c, dec_bkwd_h, dec_bkwd_c, attention, _ = self.one_step_decoder(pred, enc_output, dec_frwd_h, dec_frwd_c, dec_bkwd_h, dec_bkwd_c)\n",
    "            pred = tf.argmax(output, axis = -1)\n",
    "            all_pred.append(pred)\n",
    "            pred = tf.expand_dims(pred, 0)\n",
    "            all_attention.append(attention)\n",
    "\n",
    "        return all_pred, all_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.1 UniGram</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "embedding_dim = 100\n",
    "att_units = 256\n",
    "maxlen = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.9726\n",
      "Epoch 00001: val_loss improved from inf to 0.53295, saving model to concat_best.h5\n",
      "258/258 [==============================] - 33s 127ms/step - loss: 0.9726 - val_loss: 0.5329\n",
      "Epoch 2/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.3148\n",
      "Epoch 00002: val_loss improved from 0.53295 to 0.16296, saving model to concat_best.h5\n",
      "258/258 [==============================] - 23s 89ms/step - loss: 0.3148 - val_loss: 0.1630\n",
      "Epoch 3/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1810\n",
      "Epoch 00003: val_loss improved from 0.16296 to 0.13154, saving model to concat_best.h5\n",
      "258/258 [==============================] - 24s 91ms/step - loss: 0.1810 - val_loss: 0.1315\n",
      "Epoch 4/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1521\n",
      "Epoch 00004: val_loss improved from 0.13154 to 0.12262, saving model to concat_best.h5\n",
      "258/258 [==============================] - 24s 93ms/step - loss: 0.1521 - val_loss: 0.1226\n",
      "Epoch 5/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1372\n",
      "Epoch 00005: val_loss improved from 0.12262 to 0.11498, saving model to concat_best.h5\n",
      "258/258 [==============================] - 24s 93ms/step - loss: 0.1372 - val_loss: 0.1150\n",
      "Epoch 6/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1274\n",
      "Epoch 00006: val_loss improved from 0.11498 to 0.11055, saving model to concat_best.h5\n",
      "258/258 [==============================] - 24s 93ms/step - loss: 0.1274 - val_loss: 0.1105\n",
      "Epoch 7/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1208\n",
      "Epoch 00007: val_loss improved from 0.11055 to 0.10576, saving model to concat_best.h5\n",
      "258/258 [==============================] - 24s 92ms/step - loss: 0.1208 - val_loss: 0.1058\n",
      "Epoch 8/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1139\n",
      "Epoch 00008: val_loss did not improve from 0.10576\n",
      "258/258 [==============================] - 24s 93ms/step - loss: 0.1139 - val_loss: 0.1067\n",
      "Epoch 9/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00009: val_loss improved from 0.10576 to 0.10549, saving model to concat_best.h5\n",
      "258/258 [==============================] - 24s 93ms/step - loss: 0.1096 - val_loss: 0.1055\n",
      "Epoch 10/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 00010: val_loss improved from 0.10549 to 0.10507, saving model to concat_best.h5\n",
      "258/258 [==============================] - 24s 94ms/step - loss: 0.1053 - val_loss: 0.1051\n",
      "Epoch 11/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 00011: val_loss improved from 0.10507 to 0.10283, saving model to concat_best.h5\n",
      "258/258 [==============================] - 24s 94ms/step - loss: 0.1010 - val_loss: 0.1028\n",
      "Epoch 12/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00012: val_loss improved from 0.10283 to 0.10107, saving model to concat_best.h5\n",
      "258/258 [==============================] - 24s 95ms/step - loss: 0.0977 - val_loss: 0.1011\n",
      "Epoch 13/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0922\n",
      "Epoch 00013: val_loss improved from 0.10107 to 0.10079, saving model to concat_best.h5\n",
      "258/258 [==============================] - 25s 95ms/step - loss: 0.0922 - val_loss: 0.1008\n",
      "Epoch 14/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0890\n",
      "Epoch 00014: val_loss did not improve from 0.10079\n",
      "258/258 [==============================] - 25s 95ms/step - loss: 0.0890 - val_loss: 0.1013\n",
      "Epoch 15/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0858\n",
      "Epoch 00015: val_loss did not improve from 0.10079\n",
      "258/258 [==============================] - 25s 95ms/step - loss: 0.0858 - val_loss: 0.1018\n",
      "Epoch 16/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0827- ETA\n",
      "Epoch 00016: val_loss did not improve from 0.10079\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "258/258 [==============================] - 25s 96ms/step - loss: 0.0827 - val_loss: 0.1014\n",
      "Epoch 17/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0728\n",
      "Epoch 00017: val_loss improved from 0.10079 to 0.09775, saving model to concat_best.h5\n",
      "258/258 [==============================] - 25s 96ms/step - loss: 0.0728 - val_loss: 0.0977\n",
      "Epoch 18/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0703\n",
      "Epoch 00018: val_loss improved from 0.09775 to 0.09762, saving model to concat_best.h5\n",
      "258/258 [==============================] - 25s 96ms/step - loss: 0.0703 - val_loss: 0.0976\n",
      "Epoch 19/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0689\n",
      "Epoch 00019: val_loss improved from 0.09762 to 0.09739, saving model to concat_best.h5\n",
      "258/258 [==============================] - 25s 96ms/step - loss: 0.0689 - val_loss: 0.0974\n",
      "Epoch 20/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0680\n",
      "Epoch 00020: val_loss did not improve from 0.09739\n",
      "258/258 [==============================] - 25s 96ms/step - loss: 0.0680 - val_loss: 0.0975\n",
      "Epoch 21/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0665\n",
      "Epoch 00021: val_loss improved from 0.09739 to 0.09734, saving model to concat_best.h5\n",
      "258/258 [==============================] - 25s 96ms/step - loss: 0.0665 - val_loss: 0.0973\n",
      "Epoch 22/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0660\n",
      "Epoch 00022: val_loss did not improve from 0.09734\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "258/258 [==============================] - 25s 96ms/step - loss: 0.0660 - val_loss: 0.0978\n",
      "Epoch 23/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0643\n",
      "Epoch 00023: val_loss did not improve from 0.09734\n",
      "258/258 [==============================] - 25s 96ms/step - loss: 0.0643 - val_loss: 0.0977\n",
      "Epoch 24/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0644\n",
      "Epoch 00024: val_loss did not improve from 0.09734\n",
      "258/258 [==============================] - 25s 96ms/step - loss: 0.0644 - val_loss: 0.0976\n",
      "Epoch 25/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0641\n",
      "Epoch 00025: val_loss did not improve from 0.09734\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "258/258 [==============================] - 25s 96ms/step - loss: 0.0641 - val_loss: 0.0977\n",
      "Epoch 26/50\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0638\n",
      "Epoch 00026: val_loss did not improve from 0.09734\n",
      "258/258 [==============================] - 25s 96ms/step - loss: 0.0638 - val_loss: 0.0976\n",
      "Epoch 00026: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29a98dd50c8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = encoder_decoder(vocab_size, embedding_dim, lstm_size, lstm_size, maxlen, 'concat', att_units, batch_size)\n",
    "model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "\n",
    "callbacks = [ModelCheckpoint('concat_best.h5', save_best_only= True, verbose = 1),\n",
    "             EarlyStopping(patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]\n",
    "\n",
    "model.fit(x = unigram_train_dataset, \n",
    "          steps_per_epoch = unigram_train.shape[0]//batch_size,\n",
    "          validation_data = unigram_val_dataset,\n",
    "          validation_steps = unigram_val.shape[0]//batch_size,\n",
    "          epochs = 50,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, lstm_size, max_len, max_len, 'concat', att_units)\n",
    "pred_model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "pred_model.build(input_shape= (None, 1, maxlen))\n",
    "pred_model.load_weights('concat_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: qitted\n",
      "predicted output: quitted\n",
      "actual output: quitted\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = unigram_train['input'].values[5]\n",
    "result, attention_plot = predict(sentence, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', unigram_train['output'].values[5])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: fortissiemus\n",
      "predicted output: fortissimus\n",
      "actual output: fortissimus\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = unigram_train['input'].values[11]\n",
    "result, attention_plot = predict(sentence, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', unigram_train['output'].values[11])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: numbeGr\n",
      "predicted output: number\n",
      "actual output: number\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = unigram_train['input'].values[14]\n",
    "result, attention_plot = predict(sentence, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', unigram_train['output'].values[14])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3678/3678 [04:14<00:00, 14.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 33101/33101 [38:27<00:00, 14.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3150/3150 [03:38<00:00, 14.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score on train:  0.8553528187207379\n",
      "BLEU Score on val:  0.8027546198959468\n",
      "BLEU Score on test:  0.712202060966304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_bleu = 0\n",
    "for i in tqdm(range(unigram_val.shape[0])):\n",
    "    inp = unigram_val['input'].values[i]\n",
    "    out = unigram_val['output'].values[i]\n",
    "    pred, _ = predict(inp, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "    val_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(unigram_train.shape[0])):\n",
    "    inp = unigram_train['input'].values[i];\n",
    "    out = unigram_train['output'].values[i]\n",
    "    pred, _ = predict(inp, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "    train_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(unigram_test.shape[0])):\n",
    "    inp = unigram_test['input'].values[i]\n",
    "    out = unigram_test['output'].values[i]\n",
    "    pred, _ = predict(inp, unigram_vec, unigram_index_to_word, gram = 'uni')\n",
    "    test_bleu += sentence_bleu([out], pred)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/unigram_train.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/unigram_val.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/unigram_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.2 BiGram </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "embedding_dim = 100\n",
    "att_units = 256\n",
    "maxlen = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00001: val_loss improved from inf to 0.04310, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 847s 123ms/step - loss: 0.1299 - val_loss: 0.0431\n",
      "Epoch 2/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0464\n",
      "Epoch 00002: val_loss improved from 0.04310 to 0.03256, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 869s 126ms/step - loss: 0.0464 - val_loss: 0.0326\n",
      "Epoch 3/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0377\n",
      "Epoch 00003: val_loss improved from 0.03256 to 0.02836, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 878s 127ms/step - loss: 0.0377 - val_loss: 0.0284\n",
      "Epoch 4/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0334\n",
      "Epoch 00004: val_loss improved from 0.02836 to 0.02540, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 890s 129ms/step - loss: 0.0334 - val_loss: 0.0254\n",
      "Epoch 5/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0305\n",
      "Epoch 00005: val_loss improved from 0.02540 to 0.02419, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 877s 127ms/step - loss: 0.0305 - val_loss: 0.0242\n",
      "Epoch 6/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0285\n",
      "Epoch 00006: val_loss improved from 0.02419 to 0.02344, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 864s 125ms/step - loss: 0.0285 - val_loss: 0.0234\n",
      "Epoch 7/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0270\n",
      "Epoch 00007: val_loss improved from 0.02344 to 0.02228, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 904s 131ms/step - loss: 0.0270 - val_loss: 0.0223\n",
      "Epoch 8/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0260\n",
      "Epoch 00008: val_loss improved from 0.02228 to 0.02181, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 934s 135ms/step - loss: 0.0260 - val_loss: 0.0218\n",
      "Epoch 9/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0250\n",
      "Epoch 00009: val_loss improved from 0.02181 to 0.02144, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 922s 133ms/step - loss: 0.0250 - val_loss: 0.0214\n",
      "Epoch 10/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0243\n",
      "Epoch 00010: val_loss improved from 0.02144 to 0.02106, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 884s 128ms/step - loss: 0.0243 - val_loss: 0.0211\n",
      "Epoch 11/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0235\n",
      "Epoch 00011: val_loss improved from 0.02106 to 0.02065, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 871s 126ms/step - loss: 0.0235 - val_loss: 0.0207\n",
      "Epoch 12/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 00012: val_loss did not improve from 0.02065\n",
      "6910/6910 [==============================] - 874s 126ms/step - loss: 0.0229 - val_loss: 0.0207\n",
      "Epoch 13/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0223\n",
      "Epoch 00013: val_loss improved from 0.02065 to 0.02025, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 978s 141ms/step - loss: 0.0223 - val_loss: 0.0203\n",
      "Epoch 14/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0218\n",
      "Epoch 00014: val_loss improved from 0.02025 to 0.02014, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 836s 121ms/step - loss: 0.0218 - val_loss: 0.0201\n",
      "Epoch 15/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0215\n",
      "Epoch 00015: val_loss did not improve from 0.02014\n",
      "6910/6910 [==============================] - 853s 123ms/step - loss: 0.0215 - val_loss: 0.0202\n",
      "Epoch 16/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0210\n",
      "Epoch 00016: val_loss improved from 0.02014 to 0.01981, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 881s 128ms/step - loss: 0.0210 - val_loss: 0.0198\n",
      "Epoch 17/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 00017: val_loss improved from 0.01981 to 0.01966, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 849s 123ms/step - loss: 0.0206 - val_loss: 0.0197\n",
      "Epoch 18/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0203\n",
      "Epoch 00018: val_loss improved from 0.01966 to 0.01951, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 830s 120ms/step - loss: 0.0203 - val_loss: 0.0195\n",
      "Epoch 19/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0201\n",
      "Epoch 00019: val_loss did not improve from 0.01951\n",
      "6910/6910 [==============================] - 832s 120ms/step - loss: 0.0201 - val_loss: 0.0195\n",
      "Epoch 20/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0197\n",
      "Epoch 00020: val_loss improved from 0.01951 to 0.01923, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 816s 118ms/step - loss: 0.0197 - val_loss: 0.0192\n",
      "Epoch 21/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0195\n",
      "Epoch 00021: val_loss did not improve from 0.01923\n",
      "6910/6910 [==============================] - 815s 118ms/step - loss: 0.0195 - val_loss: 0.0194\n",
      "Epoch 22/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0191\n",
      "Epoch 00022: val_loss did not improve from 0.01923\n",
      "6910/6910 [==============================] - 846s 122ms/step - loss: 0.0191 - val_loss: 0.0193\n",
      "Epoch 23/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0190\n",
      "Epoch 00023: val_loss improved from 0.01923 to 0.01905, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 844s 122ms/step - loss: 0.0190 - val_loss: 0.0191\n",
      "Epoch 24/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0187\n",
      "Epoch 00024: val_loss did not improve from 0.01905\n",
      "6910/6910 [==============================] - 843s 122ms/step - loss: 0.0187 - val_loss: 0.0196\n",
      "Epoch 25/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0184\n",
      "Epoch 00025: val_loss improved from 0.01905 to 0.01900, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 866s 125ms/step - loss: 0.0184 - val_loss: 0.0190\n",
      "Epoch 26/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0182\n",
      "Epoch 00026: val_loss did not improve from 0.01900\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "6910/6910 [==============================] - 949s 137ms/step - loss: 0.0182 - val_loss: 0.0191\n",
      "Epoch 27/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0159\n",
      "Epoch 00027: val_loss improved from 0.01900 to 0.01819, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 881s 128ms/step - loss: 0.0159 - val_loss: 0.0182\n",
      "Epoch 28/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0148\n",
      "Epoch 00028: val_loss improved from 0.01819 to 0.01799, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 836s 121ms/step - loss: 0.0148 - val_loss: 0.0180\n",
      "Epoch 29/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0143\n",
      "Epoch 00029: val_loss improved from 0.01799 to 0.01787, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 834s 121ms/step - loss: 0.0143 - val_loss: 0.0179\n",
      "Epoch 30/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0139\n",
      "Epoch 00030: val_loss improved from 0.01787 to 0.01779, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 832s 120ms/step - loss: 0.0139 - val_loss: 0.0178\n",
      "Epoch 31/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0136\n",
      "Epoch 00031: val_loss improved from 0.01779 to 0.01773, saving model to concat_best_bigram.h5\n",
      "6910/6910 [==============================] - 828s 120ms/step - loss: 0.0136 - val_loss: 0.0177\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0134\n",
      "Epoch 00032: val_loss did not improve from 0.01773\n",
      "6910/6910 [==============================] - 829s 120ms/step - loss: 0.0134 - val_loss: 0.0178\n",
      "Epoch 33/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0132\n",
      "Epoch 00033: val_loss did not improve from 0.01773\n",
      "6910/6910 [==============================] - 820s 119ms/step - loss: 0.0132 - val_loss: 0.0178\n",
      "Epoch 34/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0130\n",
      "Epoch 00034: val_loss did not improve from 0.01773\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "6910/6910 [==============================] - 820s 119ms/step - loss: 0.0130 - val_loss: 0.0178\n",
      "Epoch 35/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0127\n",
      "Epoch 00035: val_loss did not improve from 0.01773\n",
      "6910/6910 [==============================] - 819s 119ms/step - loss: 0.0127 - val_loss: 0.0178\n",
      "Epoch 36/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0126\n",
      "Epoch 00036: val_loss did not improve from 0.01773\n",
      "6910/6910 [==============================] - 820s 119ms/step - loss: 0.0126 - val_loss: 0.0177\n",
      "Epoch 00036: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22706ba1ac8>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = encoder_decoder(vocab_size, embedding_dim, lstm_size, lstm_size, maxlen, 'concat', att_units, batch_size)\n",
    "model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "\n",
    "callbacks = [ModelCheckpoint('concat_best_bigram.h5', save_best_only= True, verbose = 1),\n",
    "             EarlyStopping(patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]\n",
    "\n",
    "model.fit(x = bigram_train_dataset, \n",
    "          steps_per_epoch = bigram_train.shape[0]//batch_size,\n",
    "          validation_data = bigram_val_dataset,\n",
    "          validation_steps = bigram_val.shape[0]//batch_size,\n",
    "          epochs = 100,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, lstm_size, maxlen, maxlen, 'concat', att_units, bigram_word_to_index)\n",
    "pred_model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "pred_model.build(input_shape= (None, 1, maxlen))\n",
    "pred_model.load_weights('concat_best_bigram.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  no Lne\n",
      "predicted output :  no one\n",
      "actual output : no one\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = bigram_train['input'].values[4]\n",
    "result, attention_plot = predict(sentence, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', bigram_train['output'].values[4])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  for themseves\n",
      "predicted output :  for themselves\n",
      "actual output : for themselves\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = bigram_train['input'].values[6]\n",
    "result, attention_plot = predict(sentence, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', bigram_train['output'].values[6])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  detil of\n",
      "predicted output :  detail of\n",
      "actual output : detail of\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = bigram_train['input'].values[7]\n",
    "result, attention_plot = predict(sentence, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', bigram_train['output'].values[7])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bleu = 0\n",
    "for i in tqdm(range(bigram_val.shape[0])):\n",
    "    inp = bigram_val['input'].values[i]\n",
    "    out = bigram_val['output'].values[i]\n",
    "    pred, _ = predict(inp, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "    val_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(bigram_train.shape[0])):\n",
    "    inp = bigram_train['input'].values[i];\n",
    "    out = bigram_train['output'].values[i]\n",
    "    pred, _ = predict(inp, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "    train_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(bigram_test.shape[0])):\n",
    "    inp = bigram_test['input'].values[i]\n",
    "    out = bigram_test['output'].values[i]\n",
    "    pred, _ = predict(inp, bigram_vec, bigram_index_to_word, gram = 'bi')\n",
    "    test_bleu += sentence_bleu([out], pred)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/bigram_train.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/bigram_val.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/bigram_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.3 TriGram</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "embedding_dim = 100\n",
    "att_units = 256\n",
    "maxlen = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.1372\n",
      "Epoch 00001: val_loss improved from inf to 0.04223, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1156s 167ms/step - loss: 0.1372 - val_loss: 0.0422\n",
      "Epoch 2/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0439\n",
      "Epoch 00002: val_loss improved from 0.04223 to 0.03123, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1154s 167ms/step - loss: 0.0439 - val_loss: 0.0312\n",
      "Epoch 3/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0349\n",
      "Epoch 00003: val_loss improved from 0.03123 to 0.02453, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1158s 168ms/step - loss: 0.0349 - val_loss: 0.0245\n",
      "Epoch 4/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0303\n",
      "Epoch 00004: val_loss improved from 0.02453 to 0.02350, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1159s 168ms/step - loss: 0.0303 - val_loss: 0.0235\n",
      "Epoch 5/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0277\n",
      "Epoch 00005: val_loss improved from 0.02350 to 0.02035, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1163s 168ms/step - loss: 0.0277 - val_loss: 0.0204\n",
      "Epoch 6/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0256\n",
      "Epoch 00006: val_loss improved from 0.02035 to 0.01934, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1158s 168ms/step - loss: 0.0256 - val_loss: 0.0193\n",
      "Epoch 7/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0241\n",
      "Epoch 00007: val_loss improved from 0.01934 to 0.01881, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1157s 167ms/step - loss: 0.0241 - val_loss: 0.0188\n",
      "Epoch 8/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0229\n",
      "Epoch 00008: val_loss improved from 0.01881 to 0.01879, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1162s 168ms/step - loss: 0.0229 - val_loss: 0.0188\n",
      "Epoch 9/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0219\n",
      "Epoch 00009: val_loss improved from 0.01879 to 0.01782, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1174s 170ms/step - loss: 0.0219 - val_loss: 0.0178\n",
      "Epoch 10/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0215\n",
      "Epoch 00010: val_loss improved from 0.01782 to 0.01715, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1185s 171ms/step - loss: 0.0215 - val_loss: 0.0171\n",
      "Epoch 11/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0206\n",
      "Epoch 00011: val_loss improved from 0.01715 to 0.01670, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1195s 173ms/step - loss: 0.0206 - val_loss: 0.0167\n",
      "Epoch 12/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0200\n",
      "Epoch 00012: val_loss improved from 0.01670 to 0.01650, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1195s 173ms/step - loss: 0.0200 - val_loss: 0.0165\n",
      "Epoch 13/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0194\n",
      "Epoch 00013: val_loss improved from 0.01650 to 0.01627, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1182s 171ms/step - loss: 0.0194 - val_loss: 0.0163\n",
      "Epoch 14/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0189\n",
      "Epoch 00014: val_loss improved from 0.01627 to 0.01613, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1188s 172ms/step - loss: 0.0189 - val_loss: 0.0161\n",
      "Epoch 15/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0186\n",
      "Epoch 00015: val_loss improved from 0.01613 to 0.01572, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1218s 176ms/step - loss: 0.0186 - val_loss: 0.0157\n",
      "Epoch 16/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0182\n",
      "Epoch 00016: val_loss improved from 0.01572 to 0.01561, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1225s 177ms/step - loss: 0.0182 - val_loss: 0.0156\n",
      "Epoch 17/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0179\n",
      "Epoch 00017: val_loss did not improve from 0.01561\n",
      "6910/6910 [==============================] - 1229s 178ms/step - loss: 0.0179 - val_loss: 0.0164\n",
      "Epoch 18/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0175\n",
      "Epoch 00018: val_loss improved from 0.01561 to 0.01544, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1236s 179ms/step - loss: 0.0175 - val_loss: 0.0154\n",
      "Epoch 19/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0172\n",
      "Epoch 00019: val_loss improved from 0.01544 to 0.01532, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1295s 187ms/step - loss: 0.0172 - val_loss: 0.0153\n",
      "Epoch 20/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00020: val_loss improved from 0.01532 to 0.01530, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1249s 181ms/step - loss: 0.0168 - val_loss: 0.0153\n",
      "Epoch 21/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00021: val_loss improved from 0.01530 to 0.01508, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1230s 178ms/step - loss: 0.0168 - val_loss: 0.0151\n",
      "Epoch 22/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0164\n",
      "Epoch 00022: val_loss improved from 0.01508 to 0.01506, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1227s 178ms/step - loss: 0.0164 - val_loss: 0.0151\n",
      "Epoch 23/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0163\n",
      "Epoch 00023: val_loss did not improve from 0.01506\n",
      "6910/6910 [==============================] - 1206s 175ms/step - loss: 0.0163 - val_loss: 0.0151\n",
      "Epoch 24/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0160\n",
      "Epoch 00024: val_loss did not improve from 0.01506\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "6910/6910 [==============================] - 1206s 175ms/step - loss: 0.0160 - val_loss: 0.0158\n",
      "Epoch 25/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0146\n",
      "Epoch 00025: val_loss improved from 0.01506 to 0.01435, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1221s 177ms/step - loss: 0.0146 - val_loss: 0.0144\n",
      "Epoch 26/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0133\n",
      "Epoch 00026: val_loss improved from 0.01435 to 0.01406, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1249s 181ms/step - loss: 0.0133 - val_loss: 0.0141\n",
      "Epoch 27/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0128\n",
      "Epoch 00027: val_loss improved from 0.01406 to 0.01390, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1269s 184ms/step - loss: 0.0128 - val_loss: 0.0139\n",
      "Epoch 28/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0124\n",
      "Epoch 00028: val_loss improved from 0.01390 to 0.01382, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1301s 188ms/step - loss: 0.0124 - val_loss: 0.0138\n",
      "Epoch 29/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0122\n",
      "Epoch 00029: val_loss improved from 0.01382 to 0.01374, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1398s 202ms/step - loss: 0.0122 - val_loss: 0.0137\n",
      "Epoch 30/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0119\n",
      "Epoch 00030: val_loss improved from 0.01374 to 0.01366, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1362s 197ms/step - loss: 0.0119 - val_loss: 0.0137\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0117\n",
      "Epoch 00031: val_loss improved from 0.01366 to 0.01364, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1326s 192ms/step - loss: 0.0117 - val_loss: 0.0136\n",
      "Epoch 32/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0116\n",
      "Epoch 00032: val_loss did not improve from 0.01364\n",
      "6910/6910 [==============================] - 1265s 183ms/step - loss: 0.0116 - val_loss: 0.0136\n",
      "Epoch 33/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0114\n",
      "Epoch 00033: val_loss improved from 0.01364 to 0.01358, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1266s 183ms/step - loss: 0.0114 - val_loss: 0.0136\n",
      "Epoch 34/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0113\n",
      "Epoch 00034: val_loss improved from 0.01358 to 0.01357, saving model to concat_best_trigram.h5\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "6910/6910 [==============================] - 1287s 186ms/step - loss: 0.0113 - val_loss: 0.0136\n",
      "Epoch 35/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0111\n",
      "Epoch 00035: val_loss improved from 0.01357 to 0.01353, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1298s 188ms/step - loss: 0.0111 - val_loss: 0.0135\n",
      "Epoch 36/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0110\n",
      "Epoch 00036: val_loss improved from 0.01353 to 0.01350, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1385s 200ms/step - loss: 0.0110 - val_loss: 0.0135\n",
      "Epoch 37/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0110\n",
      "Epoch 00037: val_loss improved from 0.01350 to 0.01350, saving model to concat_best_trigram.h5\n",
      "6910/6910 [==============================] - 1369s 198ms/step - loss: 0.0110 - val_loss: 0.0135\n",
      "Epoch 38/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 00038: val_loss improved from 0.01350 to 0.01349, saving model to concat_best_trigram.h5\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "6910/6910 [==============================] - 1333s 193ms/step - loss: 0.0109 - val_loss: 0.0135\n",
      "Epoch 39/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 00039: val_loss did not improve from 0.01349\n",
      "6910/6910 [==============================] - 1392s 201ms/step - loss: 0.0109 - val_loss: 0.0135\n",
      "Epoch 40/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 00040: val_loss did not improve from 0.01349\n",
      "6910/6910 [==============================] - 1350s 195ms/step - loss: 0.0109 - val_loss: 0.0135\n",
      "Epoch 41/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 00041: val_loss did not improve from 0.01349\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "6910/6910 [==============================] - 1396s 202ms/step - loss: 0.0109 - val_loss: 0.0135\n",
      "Epoch 42/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 00042: val_loss did not improve from 0.01349\n",
      "6910/6910 [==============================] - 1398s 202ms/step - loss: 0.0109 - val_loss: 0.0135\n",
      "Epoch 43/100\n",
      "6910/6910 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 00043: val_loss did not improve from 0.01349\n",
      "6910/6910 [==============================] - 1371s 198ms/step - loss: 0.0109 - val_loss: 0.0135\n",
      "Epoch 00043: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x226efe79bc8>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = encoder_decoder(vocab_size, embedding_dim, lstm_size, lstm_size, maxlen, 'concat', att_units, batch_size)\n",
    "model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "\n",
    "\n",
    "callbacks = [ModelCheckpoint('concat_best_trigram.h5', save_best_only= True, verbose = 1),\n",
    "             EarlyStopping(patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(patience = 3, verbose = 1)]\n",
    "\n",
    "model.fit(x = trigram_train_dataset, \n",
    "          steps_per_epoch = trigram_train.shape[0]//batch_size,\n",
    "          validation_data = trigram_val_dataset,\n",
    "          validation_steps = trigram_val.shape[0]//batch_size,\n",
    "          epochs = 100,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, lstm_size, maxlen, maxlen, 'concat', att_units, trigram_word_to_index)\n",
    "pred_model.compile(optimizer = 'Adam', loss = loss_function)\n",
    "pred_model.build(input_shape= (None, 1, maxlen))\n",
    "pred_model.load_weights('concat_best_trigram.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  woman oAf him\n",
      "predicted output :  woman of him\n",
      "actual output : woman of him\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = trigram_train['input'].values[4]\n",
    "result, attention_plot = predict(sentence, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', trigram_train['output'].values[4])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  endurXe a collision\n",
      "predicted output :  endure a collision\n",
      "actual output : endure a collision\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = trigram_train['input'].values[7]\n",
    "result, attention_plot = predict(sentence, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', trigram_train['output'].values[7])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  marshals sitti g on\n",
      "predicted output :  marshals sitting on\n",
      "actual output : marshals sitting on\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = trigram_train['input'].values[10]\n",
    "result, attention_plot = predict(sentence, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "\n",
    "print('input : ', sentence)\n",
    "print('predicted output : ',result)\n",
    "print('actual output :', trigram_train['output'].values[10])\n",
    "\n",
    "attention_plot = attention_plot[:len(list(result)), :len(list(sentence))]\n",
    "plot_attention(attention_plot, list(sentence), list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bleu = 0\n",
    "for i in tqdm(range(trigram_val.shape[0])):\n",
    "    inp = trigram_val['input'].values[i]\n",
    "    out = trigram_val['output'].values[i]\n",
    "    pred, _ = predict(inp, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "    val_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(trigram_train.shape[0])):\n",
    "    inp = trigram_train['input'].values[i];\n",
    "    out = trigram_train['output'].values[i]\n",
    "    pred, _ = predict(inp, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "    train_bleu += sentence_bleu([out], pred)\n",
    "\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(trigram_test.shape[0])):\n",
    "    inp = trigram_test['input'].values[i]\n",
    "    out = trigram_test['output'].values[i]\n",
    "    pred, _ = predict(inp, trigram_vec, trigram_index_to_word, gram = 'tri')\n",
    "    test_bleu += sentence_bleu([out], pred)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/trigram_train.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/trigram_val.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/trigram_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+------------------+----------------+-----------------+\n",
      "|                    Model Name                   | Train BLEU Score | Val BLEU Score | Test BLEU Score |\n",
      "+-------------------------------------------------+------------------+----------------+-----------------+\n",
      "|                     Seq2Seq                     |      0.834       |     0.746      |      0.684      |\n",
      "|         Seq2Seq with Attention Mechanism        |      0.869       |     0.792      |      0.707      |\n",
      "| Bi-directional Seq2Seq with Attention Mechanism |      0.855       |     0.802      |      0.712      |\n",
      "+-------------------------------------------------+------------------+----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable \n",
    "\n",
    "myTable = PrettyTable([\"n-gram\", \"Model Name\", \"Train BLEU Score\", \"Val BLEU Score\", \"Test BLEU Score\"])\n",
    "myTable.add_row([\"1-gram\", \"Seq2Seq\", \"0.834\", \"0.746\", \"0.684\"]) \n",
    "myTable.add_row([\"2-gram\", \"Seq2Seq\", \"0.834\", \"0.746\", \"0.684\"]) \n",
    "myTable.add_row([\"3-gram\", \"Seq2Seq\", \"0.834\", \"0.746\", \"0.684\"]) \n",
    "\n",
    "myTable.add_row([\"1-gram\", \"Seq2Seq with Attention Mechanism\", \"0.869\", \"0.792\", \"0.707\"]) \n",
    "myTable.add_row([\"2-gram\", \"Seq2Seq with Attention Mechanism\", \"0.869\", \"0.792\", \"0.707\"]) \n",
    "myTable.add_row([\"3-gram\", \"Seq2Seq with Attention Mechanism\", \"0.869\", \"0.792\", \"0.707\"]) \n",
    "\n",
    "myTable.add_row([\"1-gram\", \"Bi-directional Seq2Seq with Attention Mechanism\", \"0.855\", \"0.802\", \"0.712\"]) \n",
    "myTable.add_row([\"2-gram\", \"Bi-directional Seq2Seq with Attention Mechanism\", \"0.855\", \"0.802\", \"0.712\"]) \n",
    "myTable.add_row([\"3-gram\", \"Bi-directional Seq2Seq with Attention Mechanism\", \"0.855\", \"0.802\", \"0.712\"]) \n",
    "\n",
    "print(myTable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
