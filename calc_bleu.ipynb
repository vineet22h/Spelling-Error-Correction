{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of unigram train set : (33101, 5)\n",
      "Shape of unigram test set : (3150, 2)\n",
      "Shape of unigram validation set : (3678, 5)\n",
      "\n",
      "Shape of bigram train set : (884533, 5)\n",
      "Shape of bigram test set : (26377, 5)\n",
      "Shape of bigram validation set : (98282, 5)\n",
      "\n",
      "Shape of trigram train set : (884550, 5)\n",
      "Shape of trigram test set : (24586, 5)\n",
      "Shape of trigram validation set : (98284, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[54, 23,  4, ...,  0,  0,  0],\n",
       "       [54,  7, 12, ...,  0,  0,  0],\n",
       "       [54, 34, 20, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [54,  3, 23, ...,  0,  0,  0],\n",
       "       [54,  9, 16, ...,  0,  0,  0],\n",
       "       [54, 15, 27, ...,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import*\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "UNIGRAM_VOCAB = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',                  'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',                  '<SOW>', '<EOW>']\n",
    "\n",
    "NGRAM_VOCAB = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',                'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',                ' ', '<SOW>', '<EOW>']\n",
    "\n",
    "unigram_maxlen = 20\n",
    "bigram_maxlen = 24\n",
    "trigram_maxlen = 32\n",
    "\n",
    "unigram_train = pd.read_csv('unigram_train.csv', index_col = 0)\n",
    "unigram_test = pd.read_csv('unigram_test.csv', index_col = 0)\n",
    "unigram_val = pd.read_csv('unigram_val.csv', index_col = 0)\n",
    "\n",
    "bigram_train = pd.read_csv('bigram_train.csv', index_col = 0)\n",
    "bigram_test = pd.read_csv('bigram_test.csv', index_col = 0)\n",
    "bigram_val = pd.read_csv('bigram_val.csv', index_col = 0)\n",
    "\n",
    "trigram_train = pd.read_csv('trigram_train.csv', index_col = 0)\n",
    "trigram_test = pd.read_csv('trigram_test.csv', index_col = 0)\n",
    "trigram_val = pd.read_csv('trigram_val.csv', index_col = 0)\n",
    "\n",
    "bigram_test['enc_inp'] = bigram_test['input'].astype(str).apply(lambda x: '<SOW>*'+'*'.join(list(x))+'*<EOW>')\n",
    "bigram_test['dec_inp'] = bigram_test['output'].astype(str).apply(lambda x: '<SOW>*'+'*'.join(list(x)))\n",
    "bigram_test['dec_out'] = bigram_test['output'].astype(str).apply(lambda x: '*'.join(list(x))+'*<EOW>')\n",
    "\n",
    "trigram_test['enc_inp'] = trigram_test['input'].astype(str).apply(lambda x: '<SOW>*'+'*'.join(list(x))+'*<EOW>')\n",
    "trigram_test['dec_inp'] = trigram_test['output'].astype(str).apply(lambda x: '<SOW>*'+'*'.join(list(x)))\n",
    "trigram_test['dec_out'] = trigram_test['output'].astype(str).apply(lambda x: '*'.join(list(x))+'*<EOW>')\n",
    "\n",
    "print('Shape of unigram train set :',unigram_train.shape)\n",
    "print('Shape of unigram test set :',unigram_test.shape)\n",
    "print('Shape of unigram validation set :', unigram_val.shape)\n",
    "\n",
    "print('\\nShape of bigram train set :',bigram_train.shape)\n",
    "print('Shape of bigram test set :',bigram_test.shape)\n",
    "print('Shape of bigram validation set :', bigram_val.shape)\n",
    "\n",
    "print('\\nShape of trigram train set :',trigram_train.shape)\n",
    "print('Shape of trigram test set :',trigram_test.shape)\n",
    "print('Shape of trigram validation set :', trigram_val.shape)\n",
    "\n",
    "def split_on_star(input_data):\n",
    "    return tf.strings.split(input_data, sep = '*')\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "unigram_maxlen = 20\n",
    "bigram_maxlen = 24\n",
    "trigram_maxlen = 32\n",
    "\n",
    "unigram_vec = TextVectorization(output_sequence_length= unigram_maxlen+2, standardize = None, split='whitespace', max_tokens = len(UNIGRAM_VOCAB)+2, output_mode='int')\n",
    "unigram_vec.adapt(UNIGRAM_VOCAB)\n",
    "\n",
    "bigram_vec = TextVectorization(output_sequence_length= bigram_maxlen+2, standardize = None, split = split_on_star, max_tokens = len(NGRAM_VOCAB)+2, output_mode='int')\n",
    "bigram_vec.adapt(NGRAM_VOCAB)\n",
    "\n",
    "trigram_vec = TextVectorization(output_sequence_length= trigram_maxlen+2, standardize = None, split = split_on_star, max_tokens = len(NGRAM_VOCAB)+2, output_mode='int')\n",
    "trigram_vec.adapt(NGRAM_VOCAB)\n",
    "\n",
    "unigram_index_to_word = {idx: word for idx, word in enumerate(unigram_vec.get_vocabulary())}\n",
    "unigram_word_to_index = {word: idx for idx, word in enumerate(unigram_vec.get_vocabulary())}\n",
    "\n",
    "bigram_index_to_word = {idx: word for idx, word in enumerate(bigram_vec.get_vocabulary())}\n",
    "bigram_word_to_index = {word: idx for idx, word in enumerate(bigram_vec.get_vocabulary())}\n",
    "\n",
    "trigram_index_to_word = {idx: word for idx, word in enumerate(trigram_vec.get_vocabulary())}\n",
    "trigram_word_to_index = {word: idx for idx, word in enumerate(trigram_vec.get_vocabulary())}\n",
    "\n",
    "def pred_bigram_mapping(x):\n",
    "    enc_inp = bigram_vec(x[:, 2])\n",
    "    return enc_inp\n",
    "\n",
    "def pred_trigram_mapping(x):\n",
    "    enc_inp = trigram_vec(x[:, 2])\n",
    "    return enc_inp\n",
    "\n",
    "train_in = bigram_train.values.shape[0]%batch_size\n",
    "val_in = bigram_val.shape[0]%batch_size\n",
    "test_in = bigram_test.shape[0]%batch_size\n",
    "\n",
    "bigram_train_dataset = tf.data.Dataset.from_tensor_slices(bigram_train.values[:-train_in, :]).batch(batch_size).map(pred_bigram_mapping).prefetch(1)\n",
    "bigram_val_dataset = tf.data.Dataset.from_tensor_slices(bigram_val.values[:-val_in, :]).batch(batch_size).map(pred_bigram_mapping).prefetch(1)\n",
    "bigram_test_dataset = tf.data.Dataset.from_tensor_slices(bigram_test.values[:-test_in, :]).batch(batch_size).map(pred_bigram_mapping).prefetch(1)\n",
    "\n",
    "train_in = trigram_train.values.shape[0]%batch_size\n",
    "val_in = trigram_val.shape[0]%batch_size\n",
    "test_in = trigram_test.shape[0]%batch_size\n",
    "\n",
    "trigram_train_dataset = tf.data.Dataset.from_tensor_slices(trigram_train.values[:-train_in, :]).batch(batch_size).map(pred_trigram_mapping).prefetch(1)\n",
    "trigram_val_dataset = tf.data.Dataset.from_tensor_slices(trigram_val.values[:-val_in, :]).batch(batch_size).map(pred_trigram_mapping).prefetch(1)\n",
    "trigram_test_dataset = tf.data.Dataset.from_tensor_slices(trigram_test.values[:-test_in, :]).batch(batch_size).map(pred_trigram_mapping).prefetch(1)\n",
    "\n",
    "a = next(bigram_train_dataset.as_numpy_iterator())\n",
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super().__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        #Initialize Embedding layer\n",
    "        self.enc_embed = Embedding(input_dim = inp_vocab_size, output_dim = embedding_size, input_length= input_length)\n",
    "        #Intialize Encoder LSTM layer\n",
    "        self.enc_lstm = LSTM(lstm_size, return_sequences = True, return_state = True, dropout = 0.4)\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "        embedding = self.enc_embed(input_sequence)\n",
    "        output_state, enc_h, enc_c = self.enc_lstm(embedding, initial_state = states)\n",
    "        return output_state, enc_h, enc_c\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "        return [tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))]\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super().__init__()\n",
    "        #Initialize Embedding layer\n",
    "        self.dec_embed = Embedding(input_dim = out_vocab_size, output_dim = embedding_size, input_length = input_length)\n",
    "        #Intialize Decoder LSTM layer\n",
    "        self.dec_lstm = LSTM(lstm_size, return_sequences = True, return_state = True, dropout = 0.4)\n",
    "    \n",
    "    def call(self,input_sequence, initial_states):\n",
    "        embedding = self.dec_embed(input_sequence)\n",
    "        output_state, dec_h, dec_c = self.dec_lstm(embedding, initial_state = initial_states)\n",
    "        return output_state, dec_h, dec_c\n",
    "\n",
    "class Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self,*params):\n",
    "        super().__init__()\n",
    "        #Create encoder object\n",
    "        self.encoder = Encoder(inp_vocab_size = params[0], embedding_size = params[2], lstm_size = params[3], input_length = params[4])\n",
    "        #Create decoder object\n",
    "        self.decoder = Decoder(out_vocab_size = params[1], embedding_size = params[2], lstm_size = params[3], input_length = params[5])\n",
    "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
    "        self.dense = Dense(params[1], activation='softmax')\n",
    "    \n",
    "    def call(self, params, training = True):\n",
    "        enc_inp, dec_inp = params[0], params[1]\n",
    "        # print(enc_inp, dec_inp)\n",
    "        initial_state = self.encoder.initialize_states(batch_size)\n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        output, _, _ = self.decoder(dec_inp ,[enc_h, enc_c])\n",
    "        output = Dropout(0.5)(output)\n",
    "        return self.dense(output)\n",
    "\n",
    "class pred_Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self,*params):\n",
    "        super().__init__()\n",
    "        #Create encoder object\n",
    "        self.encoder = Encoder(inp_vocab_size = params[0], embedding_size = params[2], lstm_size = params[3], input_length = params[4])\n",
    "        #Create decoder object\n",
    "        self.decoder = Decoder(out_vocab_size = params[1], embedding_size = params[2], lstm_size = params[3], input_length = params[5])\n",
    "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
    "        self.dense = Dense(params[1], activation='softmax')\n",
    "        self.word_to_index = params[6]\n",
    "        self.max_len = params[4]\n",
    "    \n",
    "    def call(self, params):\n",
    "        enc_inp = params\n",
    "        initial_state = self.encoder.initialize_states(batch_size)\n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        pred = tf.fill((batch_size, 1), self.word_to_index['<SOW>'])\n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        all_outputs = tf.TensorArray(dtype = tf.int64, size= self.max_len)\n",
    "        for t in range(self.max_len):  \n",
    "            pred, dec_h,dec_c = self.decoder(pred, [dec_h, dec_c])\n",
    "            pred = self.dense(pred)\n",
    "            pred = tf.argmax(pred, axis = -1)\n",
    "            all_outputs = all_outputs.write(t, pred)\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0, 2))\n",
    "        all_outputs = tf.reshape(all_outputs, (batch_size, self.max_len))\n",
    "        return all_outputs\n",
    "    \n",
    "def idx_to_word(idx, index_to_word):\n",
    "    output = []\n",
    "    for j in idx:\n",
    "        word = index_to_word[j]\n",
    "        if word == '<EOW>':\n",
    "            break\n",
    "        output.append(word)\n",
    "    return ''.join(output)\n",
    "\n",
    "def predict(seq, vectorizer, index_to_word, gram ='uni'):\n",
    "    if gram == 'uni':\n",
    "        seq = ' '.join(list(seq))\n",
    "        seq = '<SOW> '+seq+' <EOW>'\n",
    "    else:\n",
    "        seq = '*'.join(list(seq))\n",
    "        seq = '<SOW>*'+seq+'*<EOW>'\n",
    "    seq = vectorizer([seq])\n",
    "    pred = pred_model.predict(tf.expand_dims(seq, 0))\n",
    "    output = idx_to_word(pred, index_to_word)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 884480/884480 [01:37<00:00, 9105.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 98176/98176 [00:10<00:00, 8988.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 26368/26368 [00:02<00:00, 9383.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score on train:  0.9654521214897293\n",
      "BLEU Score on val:  0.9467498122935679\n",
      "BLEU Score on test:  0.931299275417208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(bigram_vec.get_vocabulary())\n",
    "embedding_dim = 100\n",
    "lstm_size = 256\n",
    "max_len = 26\n",
    "shape = bigram_val.shape[0]\n",
    "\n",
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, max_len, max_len, bigram_word_to_index)\n",
    "pred_model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy')\n",
    "pred_model.build(input_shape=(batch_size, max_len))\n",
    "pred_model.load_weights('seq2seq_bigram.h5')\n",
    "\n",
    "train_pred = pred_model.predict(bigram_train_dataset)\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(train_pred.shape[0])):\n",
    "    output = idx_to_word(train_pred[i], bigram_index_to_word)\n",
    "    inp = bigram_train.output.values[i]\n",
    "    train_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "val_pred = pred_model.predict(bigram_val_dataset)\n",
    "val_bleu = 0\n",
    "for i in tqdm(range(val_pred.shape[0])):\n",
    "    output = idx_to_word(val_pred[i], bigram_index_to_word)\n",
    "    inp = bigram_val.output.values[i]\n",
    "    val_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "test_pred = pred_model.predict(bigram_test_dataset)\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(test_pred.shape[0])):\n",
    "    output = idx_to_word(test_pred[i], bigram_index_to_word)\n",
    "    inp = bigram_test.output.values[i]\n",
    "    test_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/train_pred.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/val_pred.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/test_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 884480/884480 [02:05<00:00, 7070.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 98176/98176 [00:15<00:00, 6236.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 24576/24576 [00:03<00:00, 7938.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score on train:  0.9684256422311769\n",
      "BLEU Score on val:  0.9575594555349892\n",
      "BLEU Score on test:  0.949065764861953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(trigram_vec.get_vocabulary())\n",
    "embedding_dim = 100\n",
    "lstm_size = 256\n",
    "max_len = 34\n",
    "\n",
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, max_len, max_len, trigram_word_to_index)\n",
    "pred_model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy')\n",
    "pred_model.build(input_shape=(batch_size, max_len))\n",
    "pred_model.load_weights('seq2seq_trigram.h5')\n",
    "\n",
    "train_pred = pred_model.predict(trigram_train_dataset)\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(train_pred.shape[0])):\n",
    "    output = idx_to_word(train_pred[i], trigram_index_to_word)\n",
    "    inp = trigram_train.output.values[i]\n",
    "    train_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "val_pred = pred_model.predict(trigram_val_dataset)\n",
    "val_bleu = 0\n",
    "for i in tqdm(range(val_pred.shape[0])):\n",
    "    output = idx_to_word(val_pred[i], trigram_index_to_word)\n",
    "    inp = trigram_val.output.values[i]\n",
    "    val_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "test_pred = pred_model.predict(trigram_test_dataset)\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(test_pred.shape[0])):\n",
    "    output = idx_to_word(test_pred[i], trigram_index_to_word)\n",
    "    inp = trigram_test.output.values[i]\n",
    "    test_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/train_pred.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/val_pred.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/test_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        #Initialize Embedding layer\n",
    "        self.enc_embed = Embedding(input_dim = inp_vocab_size, output_dim = embedding_size)\n",
    "        #Intialize Encoder LSTM layer\n",
    "        self.enc_lstm = LSTM(lstm_size, return_sequences = True, return_state = True, dropout = 0.4)\n",
    "        \n",
    "    def call(self,input_sequence,states):\n",
    "        embedding = self.enc_embed(input_sequence)\n",
    "        output_state, enc_h, enc_c = self.enc_lstm(embedding, initial_state = states)\n",
    "        return output_state, enc_h, enc_c\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "        return [tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))]\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,scoring_function, att_units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scoring_function = scoring_function\n",
    "        if scoring_function == 'dot':\n",
    "            self.dot = Dot(axes = (1, 2))\n",
    "        elif scoring_function == 'general':\n",
    "          # Intialize variables needed for General score function here\n",
    "            self.W = Dense(att_units)\n",
    "            self.dot = Dot(axes = (1, 2))\n",
    "        elif scoring_function == 'concat':\n",
    "          # Intialize variables needed for Concat score function here\n",
    "            self.W1 = Dense(att_units)\n",
    "            self.W2 = Dense(att_units)\n",
    "            self.V = Dense(1)\n",
    "    def call(self,decoder_hidden_state,encoder_output):\n",
    "    \n",
    "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 1)\n",
    "        \n",
    "        if self.scoring_function == 'dot':\n",
    "            # Implement Dot score function here\n",
    "            score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), encoder_output]), (0, 2,1))\n",
    "            \n",
    "        elif self.scoring_function == 'general':\n",
    "            # Implement General score function here\n",
    "            mul = self.W(encoder_output)\n",
    "            score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), mul]), (0, 2,1))\n",
    "            \n",
    "        elif self.scoring_function == 'concat':\n",
    "            # Implement General score function here\n",
    "            inter = self.W1(decoder_hidden_state) + self.W2(encoder_output)\n",
    "            tan = tf.nn.tanh(inter)\n",
    "            score = self.V(tan)\n",
    "        attention_weights = tf.nn.softmax(score, axis =1)\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class OneStepDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "        super(OneStepDecoder, self).__init__()\n",
    "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "        self.embed_dec = Embedding(input_dim = tar_vocab_size, output_dim = embedding_dim)\n",
    "        self.lstm = LSTM(dec_units, return_sequences = True, return_state = True, dropout = 0.4)\n",
    "        self.attention = Attention(scoring_function = score_fun, att_units = att_units)\n",
    "        self.fc = Dense(tar_vocab_size)\n",
    "    \n",
    "    def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "        embed = self.embed_dec(input_to_decoder)\n",
    "        context_vect, attention_weights = self.attention(state_h, encoder_output)  \n",
    "        embed = tf.reshape(embed, (batch_size,1, 100))\n",
    "        context_vect = tf.reshape(context_vect, (batch_size, 1, 256))\n",
    "        final_inp = tf.concat([context_vect, embed], axis = -1)\n",
    "        out, dec_h, dec_c = self.lstm(final_inp, [state_h, state_c])\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        output = self.fc(out)\n",
    "        output = Dropout(0.5)(output)\n",
    "        return output, dec_h, dec_c, attention_weights, context_vect\n",
    "\n",
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_dim, enc_units, dec_units, max_len_inp, max_len_out, score_fun, att_units, batch_size):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super(encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_dim, enc_units, max_len_inp)\n",
    "        self.one_step_decoder = OneStepDecoder(out_vocab_size, embedding_dim, max_len_out, dec_units ,score_fun ,att_units)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def call(self, data):\n",
    "        enc_inp, dec_inp = data[0], data[1]\n",
    "        initial_state = self.encoder.initialize_states(self.batch_size)\n",
    "        enc_output, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        all_outputs = tf.TensorArray(dtype = tf.float32, size= max_len)\n",
    "        \n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        for timestep in range(max_len):\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            output, dec_h, dec_c, _, _ = self.one_step_decoder(dec_inp[:, timestep:timestep+1], \n",
    "                                                               enc_output, \n",
    "                                                               dec_h,\n",
    "                                                               dec_c)\n",
    "            # Store the output in tensorarray\n",
    "            all_outputs = all_outputs.write(timestep, output)\n",
    "        # Return the tensor array\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0, 2))\n",
    "        # return the decoder output\n",
    "        return all_outputs\n",
    "\n",
    "class pred_Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_dim, enc_units, dec_units, max_len_ita, max_len_eng, score_fun, att_units, word_to_index):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super(pred_Encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_dim, enc_units, max_len_ita)\n",
    "        self.one_step_decoder = OneStepDecoder(out_vocab_size, embedding_dim, max_len_eng, dec_units ,score_fun ,att_units)\n",
    "        self.batch_size = batch_size\n",
    "        self.word_to_index = word_to_index\n",
    "        self.max_len = max_len_ita\n",
    "\n",
    "    def call(self, params):\n",
    "        enc_inp = params\n",
    "        initial_state = self.encoder.initialize_states(batch_size)\n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        pred = tf.fill((batch_size, 1), self.word_to_index['<SOW>'])\n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        all_outputs = tf.TensorArray(dtype = tf.int64, size= self.max_len)\n",
    "        for t in range(self.max_len):  \n",
    "            output, dec_h,dec_c, attention, _ = self.one_step_decoder(pred, output_state, dec_h, dec_c)\n",
    "            pred = tf.argmax(output, axis = -1)\n",
    "            all_outputs = all_outputs.write(t, pred)\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0))\n",
    "        return all_outputs\n",
    "    \n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def predict(seq, vectorizer, index_to_word, gram = 'uni'):\n",
    "    if gram =='uni':\n",
    "        seq = '<SOW> '+' '.join(list(seq))+' <EOW>'\n",
    "    else:\n",
    "        seq = '<SOW>*'+'*'.join(list(seq))+'*<EOW>'\n",
    "    seq = vectorizer([seq])\n",
    "    pred, attention_weights = pred_model.predict(tf.expand_dims(seq, 0))\n",
    "    output = idx_to_word(pred, index_to_word)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 884480/884480 [01:43<00:00, 8574.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 98176/98176 [00:11<00:00, 8424.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 26368/26368 [00:02<00:00, 9064.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score on train:  0.9719945427791346\n",
      "BLEU Score on val:  0.9588801560719675\n",
      "BLEU Score on test:  0.9460340413418259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(bigram_vec.get_vocabulary())\n",
    "embedding_dim = 100\n",
    "lstm_size = 256\n",
    "att_units = 256\n",
    "max_len = 26\n",
    "\n",
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, lstm_size, max_len, max_len, 'concat', att_units, bigram_word_to_index)\n",
    "pred_model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy')\n",
    "pred_model.build(input_shape=(batch_size, max_len))\n",
    "pred_model.load_weights('Attention_concat_lstm_bigram.h5')\n",
    "\n",
    "train_pred = pred_model.predict(bigram_train_dataset)\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(train_pred.shape[0])):\n",
    "    output = idx_to_word(train_pred[i], bigram_index_to_word)\n",
    "    inp = bigram_train.output.values[i]\n",
    "    train_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "val_pred = pred_model.predict(bigram_val_dataset)\n",
    "val_bleu = 0\n",
    "for i in tqdm(range(val_pred.shape[0])):\n",
    "    output = idx_to_word(val_pred[i], bigram_index_to_word)\n",
    "    inp = bigram_val.output.values[i]\n",
    "    val_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "test_pred = pred_model.predict(bigram_test_dataset)\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(test_pred.shape[0])):\n",
    "    output = idx_to_word(test_pred[i], bigram_index_to_word)\n",
    "    inp = bigram_test.output.values[i]\n",
    "    test_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/train_pred.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/val_pred.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/test_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 884480/884480 [02:07<00:00, 6952.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 98176/98176 [00:13<00:00, 7106.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 24576/24576 [00:03<00:00, 7524.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score on train:  0.9811416412867453\n",
      "BLEU Score on val:  0.9743928120876085\n",
      "BLEU Score on test:  0.96227561809527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(trigram_vec.get_vocabulary())\n",
    "embedding_dim = 100\n",
    "lstm_size = 256\n",
    "att_units = 256\n",
    "max_len = 34\n",
    "\n",
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, lstm_size, max_len, max_len, 'concat', att_units, trigram_word_to_index)\n",
    "pred_model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy')\n",
    "pred_model.build(input_shape=(batch_size, max_len))\n",
    "pred_model.load_weights('Attention_concat_lstm_trigram.h5')\n",
    "\n",
    "train_pred = pred_model.predict(trigram_train_dataset)\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(train_pred.shape[0])):\n",
    "    output = idx_to_word(train_pred[i], trigram_index_to_word)\n",
    "    inp = trigram_train.output.values[i]\n",
    "    train_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "val_pred = pred_model.predict(trigram_val_dataset)\n",
    "val_bleu = 0\n",
    "for i in tqdm(range(val_pred.shape[0])):\n",
    "    output = idx_to_word(val_pred[i], trigram_index_to_word)\n",
    "    inp = trigram_val.output.values[i]\n",
    "    val_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "test_pred = pred_model.predict(trigram_test_dataset)\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(test_pred.shape[0])):\n",
    "    output = idx_to_word(test_pred[i], trigram_index_to_word)\n",
    "    inp = trigram_test.output.values[i]\n",
    "    test_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/train_pred.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/val_pred.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/test_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_size, input_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.enc_embed = Embedding(input_dim = vocab_size, output_dim = embedding_size)\n",
    "        self.enc_lstm = Bidirectional(LSTM(lstm_size, return_sequences = True, return_state = True, dropout = 0.4))\n",
    "    \n",
    "    def call(self, input_sequence, states):\n",
    "        embedding = self.enc_embed(input_sequence)\n",
    "        output_state, enc_frwd_h, enc_frwd_c, enc_bkwd_h, enc_bkwd_c = self.enc_lstm(embedding, initial_state = states)\n",
    "        return output_state, enc_frwd_h, enc_frwd_c, enc_bkwd_h, enc_bkwd_c\n",
    "    \n",
    "    def initialize_states(self, batch_size):\n",
    "        return [tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size)),\n",
    "                tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))]\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,scoring_function, att_units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scoring_function = scoring_function\n",
    "        if scoring_function == 'dot':\n",
    "            self.dot = Dot(axes = (1, 2))\n",
    "        elif scoring_function == 'general':\n",
    "            self.W = Dense(att_units)\n",
    "            self.dot = Dot(axes = (1, 2))\n",
    "        elif scoring_function == 'concat':\n",
    "            self.W1 = Dense(att_units)\n",
    "            self.W2 = Dense(att_units)\n",
    "            self.W3 = Dense(att_units)\n",
    "            self.V = Dense(1)\n",
    "            \n",
    "    def call(self, dec_frwd_state, dec_bkwd_state, encoder_output):\n",
    "        dec_frwd_state = tf.expand_dims(dec_frwd_state, 1) \n",
    "        dec_bkwd_state = tf.expand_dims(dec_bkwd_state, 1)\n",
    "#         \n",
    "        if self.scoring_function == 'dot':\n",
    "            score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), encoder_output]), (0, 2,1))           \n",
    "        elif self.scoring_function == 'general':\n",
    "            mul = self.W(encoder_output)\n",
    "            score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), mul]), (0, 2,1))           \n",
    "        elif self.scoring_function == 'concat':\n",
    "            inter = self.W1(dec_frwd_state) + self.W2(dec_bkwd_state) + self.W3(encoder_output)\n",
    "            tan = tf.nn.tanh(inter)\n",
    "            score = self.V(tan)\n",
    "        attention_weights = tf.nn.softmax(score, axis =1)\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class OneStepDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "        super(OneStepDecoder, self).__init__()\n",
    "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "        self.embed_dec = Embedding(input_dim = vocab_size, output_dim = embedding_dim)\n",
    "        self.lstm = Bidirectional(LSTM(dec_units, return_sequences = True, return_state = True, dropout = 0.4))\n",
    "        self.attention = Attention(scoring_function = score_fun, att_units = att_units)\n",
    "        self.fc = Dense(vocab_size)\n",
    "    \n",
    "    def call(self,input_to_decoder, encoder_output, state_frwd_h, state_frwd_c, state_bkwd_h, state_bkwd_c):\n",
    "        embed = self.embed_dec(input_to_decoder)\n",
    "        context_vect, attention_weights = self.attention(state_frwd_h, state_bkwd_h, encoder_output)  \n",
    "        embed = tf.reshape(embed, (batch_size,1, 100))\n",
    "        context_vect = tf.reshape(context_vect, (batch_size, 1, 512))\n",
    "        final_inp = tf.concat([context_vect, embed], axis = -1)\n",
    "        out, dec_frwd_h, dec_frwd_c, dec_bkwd_h, dec_bkwd_c = self.lstm(final_inp, [state_frwd_h, state_frwd_c, state_bkwd_h, state_bkwd_c])\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = Dropout(0.5)(out)\n",
    "        output = self.fc(out)\n",
    "        return output, dec_frwd_h, dec_frwd_c, dec_bkwd_h, dec_bkwd_c, attention_weights, context_vect\n",
    "\n",
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, dec_units, max_len, score_fun, att_units, batch_size):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super(encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, enc_units, max_len)\n",
    "        self.one_step_decoder = OneStepDecoder(vocab_size, embedding_dim, max_len, dec_units ,score_fun ,att_units)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def call(self, data):\n",
    "        enc_inp, dec_inp = data[0], data[1]\n",
    "        initial_state = self.encoder.initialize_states(self.batch_size)\n",
    "        enc_output, enc_frwd_h, enc_frwd_c, enc_bkwd_h, enc_bkwd_c = self.encoder(enc_inp, initial_state)\n",
    "        all_outputs = tf.TensorArray(dtype = tf.float32, size= max_len)\n",
    "        \n",
    "        dec_frwd_h = enc_frwd_h\n",
    "        dec_frwd_c = enc_frwd_c\n",
    "        dec_bkwd_h = enc_bkwd_h\n",
    "        dec_bkwd_c = enc_bkwd_c\n",
    "        for timestep in range(max_len):\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            output, dec_frwd_h, dec_frwd_c, dec_bkwd_h, dec_bkwd_c, _, _ = self.one_step_decoder(dec_inp[:, timestep:timestep+1], enc_output, dec_frwd_h, dec_frwd_c, dec_bkwd_h, dec_bkwd_c)\n",
    "            # Store the output in tensorarray\n",
    "            all_outputs = all_outputs.write(timestep, output)\n",
    "        # Return the tensor array\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0, 2))\n",
    "        # return the decoder output\n",
    "        return all_outputs\n",
    "    \n",
    "class pred_Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_dim, enc_units, dec_units, max_len_ita, max_len_eng, score_fun, att_units, word_to_index):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super(pred_Encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_dim, enc_units, max_len_ita)\n",
    "        self.one_step_decoder = OneStepDecoder(out_vocab_size, embedding_dim, max_len_eng, dec_units, score_fun, att_units)\n",
    "        self.word_to_index = word_to_index\n",
    "        self.max_len = max_len_ita\n",
    "        \n",
    "    def call(self, params):\n",
    "        enc_inp = params\n",
    "        initial_state = self.encoder.initialize_states(batch_size)\n",
    "        enc_output, enc_frwd_h, enc_frwd_c, enc_bkwd_h, enc_bkwd_c = self.encoder(enc_inp, initial_state)\n",
    "        pred = tf.fill((batch_size, 1), self.word_to_index['<SOW>'])\n",
    "        all_outputs = tf.TensorArray(dtype = tf.int64, size= self.max_len)\n",
    "        \n",
    "        dec_frwd_h = enc_frwd_h\n",
    "        dec_frwd_c = enc_frwd_c\n",
    "        dec_bkwd_h = enc_bkwd_h\n",
    "        dec_bkwd_c = enc_bkwd_c\n",
    "        for timestep in range(self.max_len):\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            output, dec_frwd_h, dec_frwd_c, dec_bkwd_h, dec_bkwd_c, _, _ = self.one_step_decoder(pred, enc_output, dec_frwd_h, dec_frwd_c, dec_bkwd_h, dec_bkwd_c)\n",
    "            pred = tf.argmax(output, axis = -1)\n",
    "            all_outputs = all_outputs.write(timestep, pred)\n",
    "#             pred = tf.expand_dims(pred, 0)\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0))\n",
    "        return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 884480/884480 [01:40<00:00, 8810.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767/767 [==============================] - 40s 52ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 98176/98176 [00:11<00:00, 8791.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/206 [..............................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 11s 52ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 26368/26368 [00:02<00:00, 9222.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score on train:  0.9808526374708109\n",
      "BLEU Score on val:  0.9669617055111845\n",
      "BLEU Score on test:  0.9539630640021209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(bigram_vec.get_vocabulary())\n",
    "embedding_dim = 100\n",
    "lstm_size = 256\n",
    "att_units = 256\n",
    "max_len = 26\n",
    "\n",
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, lstm_size, max_len, max_len, 'concat', att_units, bigram_word_to_index)\n",
    "pred_model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy')\n",
    "pred_model.build(input_shape=(batch_size, max_len))\n",
    "pred_model.load_weights('concat_best_bigram.h5')\n",
    "\n",
    "train_pred = pred_model.predict(bigram_train_dataset, verbose=1)\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(train_pred.shape[0])):\n",
    "    output = idx_to_word(train_pred[i], bigram_index_to_word)\n",
    "    inp = bigram_train.output.values[i]\n",
    "    train_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "val_pred = pred_model.predict(bigram_val_dataset, verbose=1)\n",
    "val_bleu = 0\n",
    "for i in tqdm(range(val_pred.shape[0])):\n",
    "    output = idx_to_word(val_pred[i], bigram_index_to_word)\n",
    "    inp = bigram_val.output.values[i]\n",
    "    val_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "test_pred = pred_model.predict(bigram_test_dataset, verbose=1)\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(test_pred.shape[0])):\n",
    "    output = idx_to_word(test_pred[i], bigram_index_to_word)\n",
    "    inp = bigram_test.output.values[i]\n",
    "    test_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/train_pred.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/val_pred.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/test_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 884480/884480 [02:15<00:00, 6530.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767/767 [==============================] - 55s 71ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 98176/98176 [00:14<00:00, 6996.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 14s 74ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24576/24576 [00:03<00:00, 7288.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score on train:  0.9889611211686458\n",
      "BLEU Score on val:  0.9813112757412255\n",
      "BLEU Score on test:  0.9693013446155896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(trigram_vec.get_vocabulary())\n",
    "embedding_dim = 100\n",
    "lstm_size = 256\n",
    "att_units = 256\n",
    "max_len = 34\n",
    "\n",
    "pred_model = pred_Encoder_decoder(vocab_size, vocab_size, embedding_dim, lstm_size, lstm_size, max_len, max_len, 'concat', att_units, trigram_word_to_index)\n",
    "pred_model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy')\n",
    "pred_model.build(input_shape=(batch_size, max_len))\n",
    "pred_model.load_weights('concat_best_trigram.h5')\n",
    "\n",
    "train_pred = pred_model.predict(trigram_train_dataset)\n",
    "train_bleu = 0\n",
    "for i in tqdm(range(train_pred.shape[0])):\n",
    "    output = idx_to_word(train_pred[i], trigram_index_to_word)\n",
    "    inp = trigram_train.output.values[i]\n",
    "    train_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "val_pred = pred_model.predict(trigram_val_dataset, verbose=1)\n",
    "val_bleu = 0\n",
    "for i in tqdm(range(val_pred.shape[0])):\n",
    "    output = idx_to_word(val_pred[i], trigram_index_to_word)\n",
    "    inp = trigram_val.output.values[i]\n",
    "    val_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "test_pred = pred_model.predict(trigram_test_dataset, verbose=1)\n",
    "test_bleu = 0\n",
    "for i in tqdm(range(test_pred.shape[0])):\n",
    "    output = idx_to_word(test_pred[i], trigram_index_to_word)\n",
    "    inp = trigram_test.output.values[i]\n",
    "    test_bleu += sentence_bleu([inp], output)\n",
    "    \n",
    "print('BLEU Score on train: ',train_bleu/train_pred.shape[0])\n",
    "print('BLEU Score on val: ',val_bleu/val_pred.shape[0])\n",
    "print('BLEU Score on test: ',test_bleu/test_pred.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
